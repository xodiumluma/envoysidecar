[1mdiff --git a/.bazelrc b/.bazelrc[m
[1mindex ce5e826be6..79da7e60c4 100644[m
[1m--- a/.bazelrc[m
[1m+++ b/.bazelrc[m
[36m@@ -159,7 +159,9 @@[m [mbuild:libc++ --action_env=BAZEL_LINKOPTS=-lm:-pthread[m
 build:libc++ --define force_libcpp=enabled[m
 [m
 # Optimize build for binary size reduction.[m
[31m-build:sizeopt -c opt --copt -Os --linkopt=-Wl,--gc-sections --linkopt=-Wl,--dead_strip[m
[32m+[m[32mbuild:sizeopt-sections -c opt --copt -Os --linkopt=-Wl,--gc-sections[m
[32m+[m[32mbuild:sizeopt -c opt --copt -Os[m
[32m+[m[32mbuild:sizeopt-strip -c opt --copt -Os --linkopt=-Wl,-dead_strip[m
 [m
 # Test options[m
 build --test_env=HEAPCHECK=normal --test_env=PPROF_PATH[m
[1mdiff --git a/.github/workflows/depsreview.yml b/.github/workflows/depsreview.yml[m
[1mindex f8aac61052..ca7fc7d313 100644[m
[1m--- a/.github/workflows/depsreview.yml[m
[1m+++ b/.github/workflows/depsreview.yml[m
[36m@@ -9,4 +9,4 @@[m [mjobs:[m
       - name: 'Checkout Repository'[m
         uses: actions/checkout@93ea575cb5d8a053eaa0ac8fa3b40d7e05a33cc8[m
       - name: 'Dependency Review'[m
[31m-        uses: actions/dependency-review-action@0efb1d1d84fc9633afcdaad14c485cbbc90ef46c[m
[32m+[m[32m        uses: actions/dependency-review-action@30d582111533d59ab793fd9f971817241654f3ec[m
[1mdiff --git a/CODEOWNERS b/CODEOWNERS[m
[1mindex 749a89644b..672dc4a717 100644[m
[1m--- a/CODEOWNERS[m
[1m+++ b/CODEOWNERS[m
[36m@@ -164,6 +164,8 @@[m [mextensions/filters/http/oauth2 @derekargueta @snowp[m
 /*/extensions/matching/input_matchers/consistent_hashing @snowp @donyu[m
 # environment generic input[m
 /*/extensions/matching/common_inputs/environment @snowp @donyu[m
[32m+[m[32m# format string matching[m
[32m+[m[32m/*/extensions/matching/actions/format_string @kyessenov @UNOWNED[m
 # user space socket pair, event, connection and listener[m
 /*/extensions/io_socket/user_space @kyessenov @antoniovicente[m
 /*/extensions/bootstrap/internal_listener @kyessenov @adisuissa[m
[1mdiff --git a/api/envoy/config/endpoint/v3/endpoint_components.proto b/api/envoy/config/endpoint/v3/endpoint_components.proto[m
[1mindex 59e699ae64..247f2a5019 100644[m
[1m--- a/api/envoy/config/endpoint/v3/endpoint_components.proto[m
[1m+++ b/api/envoy/config/endpoint/v3/endpoint_components.proto[m
[36m@@ -51,6 +51,10 @@[m [mmessage Endpoint {[m
     //[m
     //   The form of the health check host address is expected to be a direct IP address.[m
     core.v3.Address address = 3;[m
[32m+[m
[32m+[m[32m    // Optional flag to control if perform active health check for this endpoint.[m
[32m+[m[32m    // Active health check is enabled by default if there is a health checker.[m
[32m+[m[32m    bool disable_active_health_check = 4;[m
   }[m
 [m
   // The upstream host address.[m
[1mdiff --git a/api/envoy/config/rbac/v3/rbac.proto b/api/envoy/config/rbac/v3/rbac.proto[m
[1mindex e4299789e7..e8d9e9cf8b 100644[m
[1m--- a/api/envoy/config/rbac/v3/rbac.proto[m
[1m+++ b/api/envoy/config/rbac/v3/rbac.proto[m
[36m@@ -5,6 +5,7 @@[m [mpackage envoy.config.rbac.v3;[m
 import "envoy/config/core/v3/address.proto";[m
 import "envoy/config/core/v3/extension.proto";[m
 import "envoy/config/route/v3/route_components.proto";[m
[32m+[m[32mimport "envoy/type/matcher/v3/filter_state.proto";[m
 import "envoy/type/matcher/v3/metadata.proto";[m
 import "envoy/type/matcher/v3/path.proto";[m
 import "envoy/type/matcher/v3/string.proto";[m
[36m@@ -229,7 +230,7 @@[m [mmessage Permission {[m
 [m
 // Principal defines an identity or a group of identities for a downstream[m
 // subject.[m
[31m-// [#next-free-field: 12][m
[32m+[m[32m// [#next-free-field: 13][m
 message Principal {[m
   option (udpa.annotations.versioning).previous_message_type = "envoy.config.rbac.v2.Principal";[m
 [m
[36m@@ -304,6 +305,9 @@[m [mmessage Principal {[m
     // Metadata that describes additional information about the principal.[m
     type.matcher.v3.MetadataMatcher metadata = 7;[m
 [m
[32m+[m[32m    // Identifies the principal using a filter state object.[m
[32m+[m[32m    type.matcher.v3.FilterStateMatcher filter_state = 12;[m
[32m+[m
     // Negates matching the provided principal. For instance, if the value of[m
     // ``not_id`` would match, this principal would not match. Conversely, if the[m
     // value of ``not_id`` would not match, this principal would match.[m
[1mdiff --git a/api/envoy/extensions/filters/http/ext_proc/v3/ext_proc.proto b/api/envoy/extensions/filters/http/ext_proc/v3/ext_proc.proto[m
[1mindex 8288684fbf..132ce3a4eb 100644[m
[1m--- a/api/envoy/extensions/filters/http/ext_proc/v3/ext_proc.proto[m
[1m+++ b/api/envoy/extensions/filters/http/ext_proc/v3/ext_proc.proto[m
[36m@@ -157,6 +157,11 @@[m [mmessage ExternalProcessor {[m
   // with the header prefix set via[m
   // :ref:`header_prefix <envoy_v3_api_field_config.bootstrap.v3.Bootstrap.header_prefix>`[m
   // (which is usually "x-envoy").[m
[32m+[m[32m  // Note that changing headers such as "host" or ":authority" may not in itself[m
[32m+[m[32m  // change Envoy's routing decision, as routes can be cached. To also force the[m
[32m+[m[32m  // route to be recomputed, set the[m
[32m+[m[32m  // :ref:`clear_route_cache <envoy_v3_api_field_service.ext_proc.v3.CommonResponse.clear_route_cache>`[m
[32m+[m[32m  // field to true in the same response.[m
   config.common.mutation_rules.v3.HeaderMutationRules mutation_rules = 9;[m
 }[m
 [m
[1mdiff --git a/api/envoy/type/matcher/v3/filter_state.proto b/api/envoy/type/matcher/v3/filter_state.proto[m
[1mnew file mode 100644[m
[1mindex 0000000000..f813178ae0[m
[1m--- /dev/null[m
[1m+++ b/api/envoy/type/matcher/v3/filter_state.proto[m
[36m@@ -0,0 +1,29 @@[m
[32m+[m[32msyntax = "proto3";[m
[32m+[m
[32m+[m[32mpackage envoy.type.matcher.v3;[m
[32m+[m
[32m+[m[32mimport "envoy/type/matcher/v3/string.proto";[m
[32m+[m
[32m+[m[32mimport "udpa/annotations/status.proto";[m
[32m+[m[32mimport "validate/validate.proto";[m
[32m+[m
[32m+[m[32moption java_package = "io.envoyproxy.envoy.type.matcher.v3";[m
[32m+[m[32moption java_outer_classname = "FilterStateProto";[m
[32m+[m[32moption java_multiple_files = true;[m
[32m+[m[32moption go_package = "github.com/envoyproxy/go-control-plane/envoy/type/matcher/v3;matcherv3";[m
[32m+[m[32moption (udpa.annotations.file_status).package_version_status = ACTIVE;[m
[32m+[m
[32m+[m[32m// [#protodoc-title: Filter state matcher][m
[32m+[m
[32m+[m[32m// FilterStateMatcher provides a general interface for matching the filter state objects.[m
[32m+[m[32mmessage FilterStateMatcher {[m
[32m+[m[32m  // The filter state key to retrieve the object.[m
[32m+[m[32m  string key = 1 [(validate.rules).string = {min_len: 1}];[m
[32m+[m
[32m+[m[32m  oneof matcher {[m
[32m+[m[32m    option (validate.required) = true;[m
[32m+[m
[32m+[m[32m    // Matches the filter state object as a string value.[m
[32m+[m[32m    StringMatcher string_match = 2;[m
[32m+[m[32m  }[m
[32m+[m[32m}[m
[1mdiff --git a/bazel/envoy_build_system.bzl b/bazel/envoy_build_system.bzl[m
[1mindex 57f2cc7c0b..de733b15e5 100644[m
[1m--- a/bazel/envoy_build_system.bzl[m
[1m+++ b/bazel/envoy_build_system.bzl[m
[36m@@ -49,6 +49,7 @@[m [mload([m
     "@envoy_build_config//:extensions_build_config.bzl",[m
     "CONTRIB_EXTENSION_PACKAGE_VISIBILITY",[m
     "EXTENSION_PACKAGE_VISIBILITY",[m
[32m+[m[32m    "MOBILE_PACKAGE_VISIBILITY",[m
 )[m
 load("@bazel_skylib//rules:common_settings.bzl", "bool_flag")[m
 [m
[36m@@ -69,7 +70,9 @@[m [mdef envoy_extension_package(enabled_default = True, default_visibility = EXTENSI[m
     )[m
 [m
 def envoy_mobile_package():[m
[31m-    envoy_extension_package()[m
[32m+[m[32m    # Mobile packages should only be visible to other mobile packages, not any other[m
[32m+[m[32m    # parts of the Envoy codebase.[m
[32m+[m[32m    envoy_extension_package(default_visibility = MOBILE_PACKAGE_VISIBILITY)[m
 [m
 def envoy_contrib_package():[m
     envoy_extension_package(default_visibility = CONTRIB_EXTENSION_PACKAGE_VISIBILITY)[m
[1mdiff --git a/bazel/repository_locations.bzl b/bazel/repository_locations.bzl[m
[1mindex 60bb366f0c..6c6c0e0914 100644[m
[1m--- a/bazel/repository_locations.bzl[m
[1m+++ b/bazel/repository_locations.bzl[m
[36m@@ -1080,12 +1080,12 @@[m [mREPOSITORY_LOCATIONS_SPEC = dict([m
         project_name = "QUICHE",[m
         project_desc = "QUICHE (QUIC, HTTP/2, Etc) is Googleâ€˜s implementation of QUIC and related protocols",[m
         project_url = "https://github.com/google/quiche",[m
[31m-        version = "c6efbc4f790a274f1f4030cd8437683a321f23b8",[m
[31m-        sha256 = "7911438519437af82356c76a9b96a8a61fcb3ee5c11ca7e6f190e4bca53c3cb0",[m
[32m+[m[32m        version = "8b0d15bda8fdeb80a40ef53b7932b0897025dc11",[m
[32m+[m[32m        sha256 = "b03b0c8e1c7d261ebb1865f9c9c7a3e6e14bc382203526e5bf767c7fed0f9c16",[m
         urls = ["https://github.com/google/quiche/archive/{version}.tar.gz"],[m
         strip_prefix = "quiche-{version}",[m
         use_category = ["dataplane_core"],[m
[31m-        release_date = "2022-11-02",[m
[32m+[m[32m        release_date = "2022-11-10",[m
         cpe = "N/A",[m
         license = "BSD-3-Clause",[m
         license_url = "https://github.com/google/quiche/blob/{version}/LICENSE",[m
[1mdiff --git a/changelogs/current.yaml b/changelogs/current.yaml[m
[1mindex b11b2363c2..ee03fcf329 100644[m
[1m--- a/changelogs/current.yaml[m
[1m+++ b/changelogs/current.yaml[m
[36m@@ -113,9 +113,15 @@[m [mnew_features:[m
 - area: udp_proxy[m
   change: |[m
     added support for :ref:`proxy_access_log <envoy_v3_api_field_extensions.filters.udp.udp_proxy.v3.UdpProxyConfig.proxy_access_log>`.[m
[32m+[m[32m- area: health_check[m
[32m+[m[32m  change: |[m
[32m+[m[32m    added an optional bool flag :ref:`disable_active_health_check <envoy_v3_api_field_config.endpoint.v3.Endpoint.HealthCheckConfig.disable_active_health_check>` to disable the active health check for the endpoint.[m
 - area: mobile[m
   change: |[m
     started merging the Envoy mobile library into the main Envoy repo.[m
[32m+[m[32m- area: matching[m
[32m+[m[32m  change: |[m
[32m+[m[32m    support filter chain selection based on the dynamic metadata and the filter state using :ref:`formatter actions <extension_envoy.matching.actions.format_string>`.[m
 - area: redis[m
   change: |[m
     extended :ref:`cluster support <arch_overview_redis_cluster_support>` by adding a :ref:`dns_cache_config <envoy_v3_api_field_extensions.filters.network.redis_proxy.v3.RedisProxy.ConnPoolSettings.dns_cache_config>` option that can be used to resolve hostnames returned by MOVED/ASK responses.[m
[1mdiff --git a/ci/Dockerfile-envoy b/ci/Dockerfile-envoy[m
[1mindex 1b736accba..c80c0c5923 100644[m
[1m--- a/ci/Dockerfile-envoy[m
[1m+++ b/ci/Dockerfile-envoy[m
[36m@@ -42,7 +42,7 @@[m [mCMD ["envoy", "-c", "/etc/envoy/envoy.yaml"][m
 [m
 # STAGE: envoy-distroless[m
 # gcr.io/distroless/base-debian11:nonroot[m
[31m-FROM gcr.io/distroless/base-debian11@sha256:49d2923f35d66b8402487a7c01bc62a66d8279cd42e89c11b64cdce8d5826c03 AS envoy-distroless[m
[32m+[m[32mFROM gcr.io/distroless/base-debian11:nonroot@sha256:4b22ca3c68018333c56f8dddcf1f8b55f32889f2dd12d28ab60856eba1130d04 AS envoy-distroless[m
 [m
 COPY --from=binary /usr/local/bin/envoy* /usr/local/bin/[m
 COPY --from=binary /usr/local/bin/su-exec /usr/local/bin/[m
[1mdiff --git a/ci/osx-build-config/extensions_build_config.bzl b/ci/osx-build-config/extensions_build_config.bzl[m
[1mindex d0ff8fdfac..79a960e731 100644[m
[1m--- a/ci/osx-build-config/extensions_build_config.bzl[m
[1m+++ b/ci/osx-build-config/extensions_build_config.bzl[m
[36m@@ -17,6 +17,7 @@[m [mWINDOWS_EXTENSIONS = {}[m
 EXTENSION_CONFIG_VISIBILITY = ["//:extension_config"][m
 EXTENSION_PACKAGE_VISIBILITY = ["//:extension_library"][m
 CONTRIB_EXTENSION_PACKAGE_VISIBILITY = ["//:contrib_library"][m
[32m+[m[32mMOBILE_PACKAGE_VISIBILITY = ["//:mobile_library"][m
 [m
 # As part of (https://github.com/envoyproxy/envoy-mobile/issues/175) we turned down alwayslink for envoy libraries[m
 # This tracks libraries that should be registered as extensions.[m
[1mdiff --git a/docs/root/api-docs/diagrams/envoy-perf-script.svg b/docs/root/api-docs/diagrams/envoy-perf-script.svg[m
[1mdeleted file mode 100644[m
[1mindex 74759e1482..0000000000[m
Binary files a/docs/root/api-docs/diagrams/envoy-perf-script.svg and /dev/null differ
[1mdiff --git a/docs/root/api-v3/types/types.rst b/docs/root/api-v3/types/types.rst[m
[1mindex 081171a162..6ebcc0b551 100644[m
[1m--- a/docs/root/api-v3/types/types.rst[m
[1m+++ b/docs/root/api-v3/types/types.rst[m
[36m@@ -12,6 +12,7 @@[m [mTypes[m
   ../type/v3/http_status.proto[m
   ../type/http/v3/cookie.proto[m
   ../type/metadata/v3/metadata.proto[m
[32m+[m[32m  ../type/matcher/v3/filter_state.proto[m
   ../type/matcher/v3/metadata.proto[m
   ../type/matcher/v3/node.proto[m
   ../type/matcher/v3/number.proto[m
[1mdiff --git a/docs/root/intro/arch_overview/advanced/matching/matching_api.rst b/docs/root/intro/arch_overview/advanced/matching/matching_api.rst[m
[1mindex ec72c0253d..721a3a6a0e 100644[m
[1m--- a/docs/root/intro/arch_overview/advanced/matching/matching_api.rst[m
[1m+++ b/docs/root/intro/arch_overview/advanced/matching/matching_api.rst[m
[36m@@ -86,6 +86,28 @@[m [mare available in some contexts:[m
 [m
 * :ref:`Trie-based IP matcher <envoy_v3_api_msg_.xds.type.matcher.v3.IPMatcher>` applies to network inputs.[m
 [m
[32m+[m[32mMatching actions[m
[32m+[m[32m################[m
[32m+[m
[32m+[m[32mThe action in the matcher framework typically refers to the selected resource by name.[m
[32m+[m
[32m+[m[32mNetwork filter chain matching supports the following extensions:[m
[32m+[m
[32m+[m[32m.. _extension_envoy.matching.actions.format_string:[m
[32m+[m
[32m+[m[32m* :ref:`Format string action <envoy_v3_api_msg_config.core.v3.SubstitutionFormatString>` computes the filter chain name[m
[32m+[m[32m  from the connection dynamic metadata and its filter state. Example:[m
[32m+[m
[32m+[m[32m.. validated-code-block:: yaml[m
[32m+[m[32m  :type-name: envoy.config.common.matcher.v3.Matcher.OnMatch[m
[32m+[m
[32m+[m[32m  action:[m
[32m+[m[32m    name: foo[m
[32m+[m[32m    typed_config:[m
[32m+[m[32m      "@type": type.googleapis.com/envoy.config.core.v3.SubstitutionFormatString[m
[32m+[m[32m      text_format_source:[m
[32m+[m[32m        inline_string: "%DYNAMIC_METADATA(com.test_filter:test_key)%"[m
[32m+[m
 Filter Integration[m
 ##################[m
 [m
[1mdiff --git a/envoy/formatter/substitution_formatter.h b/envoy/formatter/substitution_formatter.h[m
[1mindex 99f00ecc38..c0d1502c19 100644[m
[1m--- a/envoy/formatter/substitution_formatter.h[m
[1m+++ b/envoy/formatter/substitution_formatter.h[m
[36m@@ -36,6 +36,7 @@[m [mpublic:[m
 };[m
 [m
 using FormatterPtr = std::unique_ptr<Formatter>;[m
[32m+[m[32musing FormatterConstSharedPtr = std::shared_ptr<const Formatter>;[m
 [m
 /**[m
  * Interface for substitution provider.[m
[1mdiff --git a/envoy/http/filter.h b/envoy/http/filter.h[m
[1mindex 10ba0ec44c..c625bfe925 100644[m
[1m--- a/envoy/http/filter.h[m
[1m+++ b/envoy/http/filter.h[m
[36m@@ -172,7 +172,7 @@[m [menum class FilterMetadataStatus {[m
  * Return codes for onLocalReply filter invocations.[m
  */[m
 enum class LocalErrorStatus {[m
[31m-  // Continue sending the local reply after onLocalError has been sent to all filters.[m
[32m+[m[32m  // Continue sending the local reply after onLocalReply has been sent to all filters.[m
   Continue,[m
 [m
   // Continue sending onLocalReply to all filters, but reset the stream once all filters have been[m
[36m@@ -781,7 +781,7 @@[m [mpublic:[m
    * from onLocalReply, as that has the potential for looping.[m
    *[m
    * @param data data associated with the sendLocalReply call.[m
[31m-   * @param LocalErrorStatus the action to take after onLocalError completes.[m
[32m+[m[32m   * @param LocalErrorStatus the action to take after onLocalReply completes.[m
    */[m
   virtual LocalErrorStatus onLocalReply(const LocalReplyData&) {[m
     return LocalErrorStatus::Continue;[m
[1mdiff --git a/envoy/upstream/cluster_manager.h b/envoy/upstream/cluster_manager.h[m
[1mindex bbdcaf2711..fbb50d1736 100644[m
[1m--- a/envoy/upstream/cluster_manager.h[m
[1m+++ b/envoy/upstream/cluster_manager.h[m
[36m@@ -378,7 +378,10 @@[m [mpublic:[m
    *[m
    * @return the stat names.[m
    */[m
[31m-  virtual const ClusterStatNames& clusterStatNames() const PURE;[m
[32m+[m[32m  virtual const ClusterTrafficStatNames& clusterStatNames() const PURE;[m
[32m+[m[32m  virtual const ClusterConfigUpdateStatNames& clusterConfigUpdateStatNames() const PURE;[m
[32m+[m[32m  virtual const ClusterLbStatNames& clusterLbStatNames() const PURE;[m
[32m+[m[32m  virtual const ClusterEndpointStatNames& clusterEndpointStatNames() const PURE;[m
   virtual const ClusterLoadReportStatNames& clusterLoadReportStatNames() const PURE;[m
   virtual const ClusterCircuitBreakersStatNames& clusterCircuitBreakersStatNames() const PURE;[m
   virtual const ClusterRequestResponseSizeStatNames&[m
[1mdiff --git a/envoy/upstream/upstream.h b/envoy/upstream/upstream.h[m
[1mindex 2633fc60f7..69b600b484 100644[m
[1m--- a/envoy/upstream/upstream.h[m
[1m+++ b/envoy/upstream/upstream.h[m
[36m@@ -242,6 +242,16 @@[m [mpublic:[m
    * connection pools no longer need this host.[m
    */[m
   virtual HostHandlePtr acquireHandle() const PURE;[m
[32m+[m
[32m+[m[32m  /**[m
[32m+[m[32m   * @return true if active health check is disabled.[m
[32m+[m[32m   */[m
[32m+[m[32m  virtual bool disableActiveHealthCheck() const PURE;[m
[32m+[m
[32m+[m[32m  /**[m
[32m+[m[32m   * Set true to disable active health check for the host.[m
[32m+[m[32m   */[m
[32m+[m[32m  virtual void setDisableActiveHealthCheck(bool disable_active_health_check) PURE;[m
 };[m
 [m
 using HostConstSharedPtr = std::shared_ptr<const Host>;[m
[36m@@ -565,12 +575,36 @@[m [mpublic:[m
 };[m
 [m
 /**[m
[31m- * All cluster stats. @see stats_macros.h[m
[32m+[m[32m * All cluster config update related stats.[m
[32m+[m[32m * See https://github.com/envoyproxy/envoy/issues/23575 for details. Stats from ClusterInfo::stats()[m
[32m+[m[32m * will be split into subgroups "config-update", "lb", "endpoint" and "the rest"(which are mainly[m
[32m+[m[32m * upstream related), roughly based on their semantics.[m
  */[m
[31m-#define ALL_CLUSTER_STATS(COUNTER, GAUGE, HISTOGRAM, TEXT_READOUT, STATNAME)                       \[m
[32m+[m[32m#define ALL_CLUSTER_CONFIG_UPDATE_STATS(COUNTER, GAUGE, HISTOGRAM, TEXT_READOUT, STATNAME)         \[m
   COUNTER(assignment_stale)                                                                        \[m
   COUNTER(assignment_timeout_received)                                                             \[m
[31m-  COUNTER(bind_errors)                                                                             \[m
[32m+[m[32m  COUNTER(update_attempt)                                                                          \[m
[32m+[m[32m  COUNTER(update_empty)                                                                            \[m
[32m+[m[32m  COUNTER(update_failure)                                                                          \[m
[32m+[m[32m  COUNTER(update_no_rebuild)                                                                       \[m
[32m+[m[32m  COUNTER(update_success)                                                                          \[m
[32m+[m[32m  GAUGE(version, NeverImport)[m
[32m+[m
[32m+[m[32m/**[m
[32m+[m[32m * All cluster endpoints related stats.[m
[32m+[m[32m */[m
[32m+[m[32m#define ALL_CLUSTER_ENDPOINT_STATS(COUNTER, GAUGE, HISTOGRAM, TEXT_READOUT, STATNAME)              \[m
[32m+[m[32m  GAUGE(max_host_weight, NeverImport)                                                              \[m
[32m+[m[32m  COUNTER(membership_change)                                                                       \[m
[32m+[m[32m  GAUGE(membership_degraded, NeverImport)                                                          \[m
[32m+[m[32m  GAUGE(membership_excluded, NeverImport)                                                          \[m
[32m+[m[32m  GAUGE(membership_healthy, NeverImport)                                                           \[m
[32m+[m[32m  GAUGE(membership_total, NeverImport)[m
[32m+[m
[32m+[m[32m/**[m
[32m+[m[32m * All cluster loadbalancing related stats.[m
[32m+[m[32m */[m
[32m+[m[32m#define ALL_CLUSTER_LB_STATS(COUNTER, GAUGE, HISTOGRAM, TEXT_READOUT, STATNAME)                    \[m
   COUNTER(lb_healthy_panic)                                                                        \[m
   COUNTER(lb_local_cluster_not_ok)                                                                 \[m
   COUNTER(lb_recalculate_zone_structures)                                                          \[m
[36m@@ -585,14 +619,15 @@[m [mpublic:[m
   COUNTER(lb_zone_routing_all_directly)                                                            \[m
   COUNTER(lb_zone_routing_cross_zone)                                                              \[m
   COUNTER(lb_zone_routing_sampled)                                                                 \[m
[31m-  COUNTER(membership_change)                                                                       \[m
[32m+[m[32m  GAUGE(lb_subsets_active, Accumulate)[m
[32m+[m
[32m+[m[32m/**[m
[32m+[m[32m * All cluster stats. @see stats_macros.h[m
[32m+[m[32m */[m
[32m+[m[32m#define ALL_CLUSTER_TRAFFIC_STATS(COUNTER, GAUGE, HISTOGRAM, TEXT_READOUT, STATNAME)               \[m
[32m+[m[32m  COUNTER(bind_errors)                                                                             \[m
   COUNTER(original_dst_host_invalid)                                                               \[m
   COUNTER(retry_or_shadow_abandoned)                                                               \[m
[31m-  COUNTER(update_attempt)                                                                          \[m
[31m-  COUNTER(update_empty)                                                                            \[m
[31m-  COUNTER(update_failure)                                                                          \[m
[31m-  COUNTER(update_no_rebuild)                                                                       \[m
[31m-  COUNTER(update_success)                                                                          \[m
   COUNTER(upstream_cx_close_notify)                                                                \[m
   COUNTER(upstream_cx_connect_attempts_exceeded)                                                   \[m
   COUNTER(upstream_cx_connect_fail)                                                                \[m
[36m@@ -644,18 +679,11 @@[m [mpublic:[m
   COUNTER(upstream_rq_total)                                                                       \[m
   COUNTER(upstream_rq_tx_reset)                                                                    \[m
   COUNTER(upstream_http3_broken)                                                                   \[m
[31m-  GAUGE(lb_subsets_active, Accumulate)                                                             \[m
[31m-  GAUGE(max_host_weight, NeverImport)                                                              \[m
[31m-  GAUGE(membership_degraded, NeverImport)                                                          \[m
[31m-  GAUGE(membership_excluded, NeverImport)                                                          \[m
[31m-  GAUGE(membership_healthy, NeverImport)                                                           \[m
[31m-  GAUGE(membership_total, NeverImport)                                                             \[m
   GAUGE(upstream_cx_active, Accumulate)                                                            \[m
   GAUGE(upstream_cx_rx_bytes_buffered, Accumulate)                                                 \[m
   GAUGE(upstream_cx_tx_bytes_buffered, Accumulate)                                                 \[m
   GAUGE(upstream_rq_active, Accumulate)                                                            \[m
   GAUGE(upstream_rq_pending_active, Accumulate)                                                    \[m
[31m-  GAUGE(version, NeverImport)                                                                      \[m
   HISTOGRAM(upstream_cx_connect_ms, Milliseconds)                                                  \[m
   HISTOGRAM(upstream_cx_length_ms, Milliseconds)[m
 [m
[36m@@ -708,10 +736,29 @@[m [mpublic:[m
   HISTOGRAM(upstream_rq_timeout_budget_per_try_percent_used, Unspecified)[m
 [m
 /**[m
[31m- * Struct definition for all cluster stats. @see stats_macros.h[m
[32m+[m[32m * Struct definition for cluster config update stats. @see stats_macros.h[m
[32m+[m[32m */[m
[32m+[m[32mMAKE_STAT_NAMES_STRUCT(ClusterConfigUpdateStatNames, ALL_CLUSTER_CONFIG_UPDATE_STATS);[m
[32m+[m[32mMAKE_STATS_STRUCT(ClusterConfigUpdateStats, ClusterConfigUpdateStatNames,[m
[32m+[m[32m                  ALL_CLUSTER_CONFIG_UPDATE_STATS);[m
[32m+[m
[32m+[m[32m/**[m
[32m+[m[32m * Struct definition for cluster endpoint related stats. @see stats_macros.h[m
[32m+[m[32m */[m
[32m+[m[32mMAKE_STAT_NAMES_STRUCT(ClusterEndpointStatNames, ALL_CLUSTER_ENDPOINT_STATS);[m
[32m+[m[32mMAKE_STATS_STRUCT(ClusterEndpointStats, ClusterEndpointStatNames, ALL_CLUSTER_ENDPOINT_STATS);[m
[32m+[m
[32m+[m[32m/**[m
[32m+[m[32m * Struct definition for cluster load balancing stats. @see stats_macros.h[m
  */[m
[31m-MAKE_STAT_NAMES_STRUCT(ClusterStatNames, ALL_CLUSTER_STATS);[m
[31m-MAKE_STATS_STRUCT(ClusterStats, ClusterStatNames, ALL_CLUSTER_STATS);[m
[32m+[m[32mMAKE_STAT_NAMES_STRUCT(ClusterLbStatNames, ALL_CLUSTER_LB_STATS);[m
[32m+[m[32mMAKE_STATS_STRUCT(ClusterLbStats, ClusterLbStatNames, ALL_CLUSTER_LB_STATS);[m
[32m+[m
[32m+[m[32m/**[m
[32m+[m[32m * Struct definition for all cluster traffic stats. @see stats_macros.h[m
[32m+[m[32m */[m
[32m+[m[32mMAKE_STAT_NAMES_STRUCT(ClusterTrafficStatNames, ALL_CLUSTER_TRAFFIC_STATS);[m
[32m+[m[32mMAKE_STATS_STRUCT(ClusterTrafficStats, ClusterTrafficStatNames, ALL_CLUSTER_TRAFFIC_STATS);[m
 [m
 MAKE_STAT_NAMES_STRUCT(ClusterLoadReportStatNames, ALL_CLUSTER_LOAD_REPORT_STATS);[m
 MAKE_STATS_STRUCT(ClusterLoadReportStats, ClusterLoadReportStatNames,[m
[36m@@ -992,9 +1039,24 @@[m [mpublic:[m
   virtual TransportSocketMatcher& transportSocketMatcher() const PURE;[m
 [m
   /**[m
[31m-   * @return ClusterStats& strongly named stats for this cluster.[m
[32m+[m[32m   * @return ClusterConfigUpdateStats& config update stats for this cluster.[m
[32m+[m[32m   */[m
[32m+[m[32m  virtual ClusterConfigUpdateStats& configUpdateStats() const PURE;[m
[32m+[m
[32m+[m[32m  /**[m
[32m+[m[32m   * @return ClusterLbStats& load-balancer-related stats for this cluster.[m
[32m+[m[32m   */[m
[32m+[m[32m  virtual ClusterLbStats& lbStats() const PURE;[m
[32m+[m
[32m+[m[32m  /**[m
[32m+[m[32m   * @return ClusterEndpointStats& endpoint related stats for this cluster.[m
[32m+[m[32m   */[m
[32m+[m[32m  virtual ClusterEndpointStats& endpointStats() const PURE;[m
[32m+[m
[32m+[m[32m  /**[m
[32m+[m[32m   * @return ClusterTrafficStats& all traffic related stats for this cluster.[m
    */[m
[31m-  virtual ClusterStats& stats() const PURE;[m
[32m+[m[32m  virtual ClusterTrafficStats& trafficStats() const PURE;[m
 [m
   /**[m
    * @return the stats scope that contains all cluster stats. This can be used to produce dynamic[m
[36m@@ -1003,7 +1065,7 @@[m [mpublic:[m
   virtual Stats::Scope& statsScope() const PURE;[m
 [m
   /**[m
[31m-   * @return ClusterLoadReportStats& strongly named load report stats for this cluster.[m
[32m+[m[32m   * @return ClusterLoadReportStats& load report stats for this cluster.[m
    */[m
   virtual ClusterLoadReportStats& loadReportStats() const PURE;[m
 [m
[1mdiff --git a/examples/ext_authz/auth/grpc-service/Dockerfile b/examples/ext_authz/auth/grpc-service/Dockerfile[m
[1mindex 99d1ac5926..778497f4c2 100644[m
[1m--- a/examples/ext_authz/auth/grpc-service/Dockerfile[m
[1m+++ b/examples/ext_authz/auth/grpc-service/Dockerfile[m
[36m@@ -1,4 +1,4 @@[m
[31m-FROM golang:alpine@sha256:8558ae624304387d18694b9ea065cc9813dd4f7f9bd5073edb237541f2d0561b AS builder[m
[32m+[m[32mFROM golang:alpine@sha256:dc4f4756a4fb91b6f496a958e11e00c0621130c8dfbb31ac0737b0229ad6ad9c AS builder[m
 [m
 RUN apk --no-cache add make[m
 COPY . /app[m
[1mdiff --git a/examples/ext_authz/auth/http-service/Dockerfile b/examples/ext_authz/auth/http-service/Dockerfile[m
[1mindex 28c511b2d1..805d3ed04e 100644[m
[1m--- a/examples/ext_authz/auth/http-service/Dockerfile[m
[1m+++ b/examples/ext_authz/auth/http-service/Dockerfile[m
[36m@@ -1,4 +1,4 @@[m
[31m-FROM node:alpine@sha256:00c5c0850a48bbbf0136f1c886bad52784f9816a8d314a99307d734598359ed4[m
[32m+[m[32mFROM node:alpine@sha256:083a23fe246cc82294f64e154f5d6bce8c90b9fc8f2dce54d3c58d41ddd8f8c8[m
 [m
 COPY . /app[m
 CMD ["node", "/app/http-service/server"][m
[1mdiff --git a/examples/shared/postgres/Dockerfile b/examples/shared/postgres/Dockerfile[m
[1mindex daa795e062..4129a88250 100644[m
[1m--- a/examples/shared/postgres/Dockerfile[m
[1m+++ b/examples/shared/postgres/Dockerfile[m
[36m@@ -1 +1 @@[m
[31m-FROM postgres:latest@sha256:bab8d7be6466e029f7fa1e69ff6aa0082704db330572638fd01f2791824774d8[m
[32m+[m[32mFROM postgres:latest@sha256:9eb2589e67e69daf321fa95ae40e7509ce08bb1ef90d5a27a0775aa88ee0c704[m
[1mdiff --git a/source/common/common/matchers.cc b/source/common/common/matchers.cc[m
[1mindex 021e5e9a64..c1355f35f2 100644[m
[1m--- a/source/common/common/matchers.cc[m
[1m+++ b/source/common/common/matchers.cc[m
[36m@@ -96,6 +96,33 @@[m [mMetadataMatcher::MetadataMatcher(const envoy::type::matcher::v3::MetadataMatcher[m
   value_matcher_ = ValueMatcher::create(v);[m
 }[m
 [m
[32m+[m[32mnamespace {[m
[32m+[m[32mStringMatcherPtr[m
[32m+[m[32mvalueMatcherFromProto(const envoy::type::matcher::v3::FilterStateMatcher& matcher) {[m
[32m+[m[32m  switch (matcher.matcher_case()) {[m
[32m+[m[32m  case envoy::type::matcher::v3::FilterStateMatcher::MatcherCase::kStringMatch:[m
[32m+[m[32m    return std::make_unique<const StringMatcherImpl<envoy::type::matcher::v3::StringMatcher>>([m
[32m+[m[32m        matcher.string_match());[m
[32m+[m[32m    break;[m
[32m+[m[32m  default:[m
[32m+[m[32m    PANIC_DUE_TO_PROTO_UNSET;[m
[32m+[m[32m  }[m
[32m+[m[32m}[m
[32m+[m
[32m+[m[32m} // namespace[m
[32m+[m
[32m+[m[32mFilterStateMatcher::FilterStateMatcher(const envoy::type::matcher::v3::FilterStateMatcher& matcher)[m
[32m+[m[32m    : key_(matcher.key()), value_matcher_(valueMatcherFromProto(matcher)) {}[m
[32m+[m
[32m+[m[32mbool FilterStateMatcher::match(const StreamInfo::FilterState& filter_state) const {[m
[32m+[m[32m  const auto* object = filter_state.getDataReadOnlyGeneric(key_);[m
[32m+[m[32m  if (object == nullptr) {[m
[32m+[m[32m    return false;[m
[32m+[m[32m  }[m
[32m+[m[32m  const auto string_value = object->serializeAsString();[m
[32m+[m[32m  return string_value && value_matcher_->match(*string_value);[m
[32m+[m[32m}[m
[32m+[m
 PathMatcherConstSharedPtr PathMatcher::createExact(const std::string& exact, bool ignore_case) {[m
   envoy::type::matcher::v3::StringMatcher matcher;[m
   matcher.set_exact(exact);[m
[1mdiff --git a/source/common/common/matchers.h b/source/common/common/matchers.h[m
[1mindex 34a8730dbd..214c628b64 100644[m
[1m--- a/source/common/common/matchers.h[m
[1m+++ b/source/common/common/matchers.h[m
[36m@@ -6,6 +6,7 @@[m
 #include "envoy/common/matchers.h"[m
 #include "envoy/common/regex.h"[m
 #include "envoy/config/core/v3/base.pb.h"[m
[32m+[m[32m#include "envoy/type/matcher/v3/filter_state.pb.h"[m
 #include "envoy/type/matcher/v3/metadata.pb.h"[m
 #include "envoy/type/matcher/v3/number.pb.h"[m
 #include "envoy/type/matcher/v3/path.pb.h"[m
[36m@@ -189,6 +190,22 @@[m [mprivate:[m
   ValueMatcherConstSharedPtr value_matcher_;[m
 };[m
 [m
[32m+[m[32mclass FilterStateMatcher {[m
[32m+[m[32mpublic:[m
[32m+[m[32m  FilterStateMatcher(const envoy::type::matcher::v3::FilterStateMatcher& matcher);[m
[32m+[m
[32m+[m[32m  /**[m
[32m+[m[32m   * Check whether the filter state object is matched to the matcher.[m
[32m+[m[32m   * @param filter state to check.[m
[32m+[m[32m   * @return true if it's matched otherwise false.[m
[32m+[m[32m   */[m
[32m+[m[32m  bool match(const StreamInfo::FilterState& filter_state) const;[m
[32m+[m
[32m+[m[32mprivate:[m
[32m+[m[32m  const std::string key_;[m
[32m+[m[32m  const StringMatcherPtr value_matcher_;[m
[32m+[m[32m};[m
[32m+[m
 class PathMatcher : public StringMatcher {[m
 public:[m
   PathMatcher(const envoy::type::matcher::v3::PathMatcher& path) : matcher_(path.path()) {}[m
[1mdiff --git a/source/common/conn_pool/conn_pool_base.cc b/source/common/conn_pool/conn_pool_base.cc[m
[1mindex 78d26a13f9..31ee3f3aff 100644[m
[1m--- a/source/common/conn_pool/conn_pool_base.cc[m
[1m+++ b/source/common/conn_pool/conn_pool_base.cc[m
[36m@@ -135,7 +135,7 @@[m [mConnPoolImplBase::tryCreateNewConnection(float global_preconnect_ratio) {[m
   const bool can_create_connection = host_->canCreateConnection(priority_);[m
 [m
   if (!can_create_connection) {[m
[31m-    host_->cluster().stats().upstream_cx_overflow_.inc();[m
[32m+[m[32m    host_->cluster().trafficStats().upstream_cx_overflow_.inc();[m
   }[m
   // If we are at the connection circuit-breaker limit due to other upstreams having[m
   // too many open connections, and this upstream has no connections, always create one, to[m
[36m@@ -168,14 +168,14 @@[m [mvoid ConnPoolImplBase::attachStreamToClient(Envoy::ConnectionPool::ActiveClient&[m
   ASSERT(client.readyForStream());[m
 [m
   if (client.state() == Envoy::ConnectionPool::ActiveClient::State::ReadyForEarlyData) {[m
[31m-    host_->cluster().stats().upstream_rq_0rtt_.inc();[m
[32m+[m[32m    host_->cluster().trafficStats().upstream_rq_0rtt_.inc();[m
   }[m
 [m
   if (enforceMaxRequests() && !host_->cluster().resourceManager(priority_).requests().canCreate()) {[m
     ENVOY_LOG(debug, "max streams overflow");[m
     onPoolFailure(client.real_host_description_, absl::string_view(),[m
                   ConnectionPool::PoolFailureReason::Overflow, context);[m
[31m-    host_->cluster().stats().upstream_rq_pending_overflow_.inc();[m
[32m+[m[32m    host_->cluster().trafficStats().upstream_rq_pending_overflow_.inc();[m
     return;[m
   }[m
   ENVOY_CONN_LOG(debug, "creating stream", client);[m
[36m@@ -185,7 +185,7 @@[m [mvoid ConnPoolImplBase::attachStreamToClient(Envoy::ConnectionPool::ActiveClient&[m
   client.remaining_streams_--;[m
   if (client.remaining_streams_ == 0) {[m
     ENVOY_CONN_LOG(debug, "maximum streams per connection, start draining", client);[m
[31m-    host_->cluster().stats().upstream_cx_max_requests_.inc();[m
[32m+[m[32m    host_->cluster().trafficStats().upstream_cx_max_requests_.inc();[m
     transitionActiveClientState(client, Envoy::ConnectionPool::ActiveClient::State::Draining);[m
   } else if (capacity == 1) {[m
     // As soon as the new stream is created, the client will be maxed out.[m
[36m@@ -202,8 +202,8 @@[m [mvoid ConnPoolImplBase::attachStreamToClient(Envoy::ConnectionPool::ActiveClient&[m
   num_active_streams_++;[m
   host_->stats().rq_total_.inc();[m
   host_->stats().rq_active_.inc();[m
[31m-  host_->cluster().stats().upstream_rq_total_.inc();[m
[31m-  host_->cluster().stats().upstream_rq_active_.inc();[m
[32m+[m[32m  host_->cluster().trafficStats().upstream_rq_total_.inc();[m
[32m+[m[32m  host_->cluster().trafficStats().upstream_rq_active_.inc();[m
   host_->cluster().resourceManager(priority_).requests().inc();[m
 [m
   onPoolReady(client, context);[m
[36m@@ -216,7 +216,7 @@[m [mvoid ConnPoolImplBase::onStreamClosed(Envoy::ConnectionPool::ActiveClient& clien[m
   state_.decrActiveStreams(1);[m
   num_active_streams_--;[m
   host_->stats().rq_active_.dec();[m
[31m-  host_->cluster().stats().upstream_rq_active_.dec();[m
[32m+[m[32m  host_->cluster().trafficStats().upstream_rq_active_.dec();[m
   host_->cluster().resourceManager(priority_).requests().dec();[m
   // We don't update the capacity for HTTP/3 as the stream count should only[m
   // increase when a MAX_STREAMS frame is received.[m
[36m@@ -282,7 +282,7 @@[m [mConnectionPool::Cancellable* ConnPoolImplBase::newStreamImpl(AttachContext& cont[m
     ENVOY_LOG(debug, "max pending streams overflow");[m
     onPoolFailure(nullptr, absl::string_view(), ConnectionPool::PoolFailureReason::Overflow,[m
                   context);[m
[31m-    host_->cluster().stats().upstream_rq_pending_overflow_.inc();[m
[32m+[m[32m    host_->cluster().trafficStats().upstream_rq_pending_overflow_.inc();[m
     return nullptr;[m
   }[m
 [m
[36m@@ -490,7 +490,7 @@[m [mvoid ConnPoolImplBase::onConnectionEvent(ActiveClient& client, absl::string_view[m
 [m
     if (!client.hasHandshakeCompleted()) {[m
       client.has_handshake_completed_ = true;[m
[31m-      host_->cluster().stats().upstream_cx_connect_fail_.inc();[m
[32m+[m[32m      host_->cluster().trafficStats().upstream_cx_connect_fail_.inc();[m
       host_->stats().cx_connect_fail_.inc();[m
 [m
       onConnectFailed(client);[m
[36m@@ -595,7 +595,7 @@[m [mvoid ConnPoolImplBase::onConnectionEvent(ActiveClient& client, absl::string_view[m
                    client.currentUnusedCapacity());[m
     // No need to update connecting capacity and connect_timer_ as the client is still connecting.[m
     ASSERT(client.state() == ActiveClient::State::Connecting);[m
[31m-    host()->cluster().stats().upstream_cx_connect_with_0_rtt_.inc();[m
[32m+[m[32m    host()->cluster().trafficStats().upstream_cx_connect_with_0_rtt_.inc();[m
     transitionActiveClientState(client, (client.currentUnusedCapacity() > 0[m
                                              ? ActiveClient::State::ReadyForEarlyData[m
                                              : ActiveClient::State::Busy));[m
[36m@@ -606,13 +606,13 @@[m [mvoid ConnPoolImplBase::onConnectionEvent(ActiveClient& client, absl::string_view[m
 [m
 PendingStream::PendingStream(ConnPoolImplBase& parent, bool can_send_early_data)[m
     : parent_(parent), can_send_early_data_(can_send_early_data) {[m
[31m-  parent_.host()->cluster().stats().upstream_rq_pending_total_.inc();[m
[31m-  parent_.host()->cluster().stats().upstream_rq_pending_active_.inc();[m
[32m+[m[32m  parent_.host()->cluster().trafficStats().upstream_rq_pending_total_.inc();[m
[32m+[m[32m  parent_.host()->cluster().trafficStats().upstream_rq_pending_active_.inc();[m
   parent_.host()->cluster().resourceManager(parent_.priority()).pendingRequests().inc();[m
 }[m
 [m
 PendingStream::~PendingStream() {[m
[31m-  parent_.host()->cluster().stats().upstream_rq_pending_active_.dec();[m
[32m+[m[32m  parent_.host()->cluster().trafficStats().upstream_rq_pending_active_.dec();[m
   parent_.host()->cluster().resourceManager(parent_.priority()).pendingRequests().dec();[m
 }[m
 [m
[36m@@ -630,7 +630,7 @@[m [mvoid ConnPoolImplBase::purgePendingStreams([m
   while (!pending_streams_to_purge_.empty()) {[m
     PendingStreamPtr stream =[m
         pending_streams_to_purge_.front()->removeFromList(pending_streams_to_purge_);[m
[31m-    host_->cluster().stats().upstream_rq_pending_failure_eject_.inc();[m
[32m+[m[32m    host_->cluster().trafficStats().upstream_rq_pending_failure_eject_.inc();[m
     onPoolFailure(host_description, failure_reason, reason, stream->context());[m
   }[m
 }[m
[36m@@ -683,7 +683,7 @@[m [mvoid ConnPoolImplBase::onPendingStreamCancel(PendingStream& stream,[m
     }[m
   }[m
 [m
[31m-  host_->cluster().stats().upstream_rq_cancelled_.inc();[m
[32m+[m[32m  host_->cluster().trafficStats().upstream_rq_cancelled_.inc();[m
   checkForIdleAndCloseIdleConnsIfDraining();[m
 }[m
 [m
[36m@@ -757,14 +757,16 @@[m [mActiveClient::ActiveClient(ConnPoolImplBase& parent, uint32_t lifetime_stream_li[m
       concurrent_stream_limit_(translateZeroToUnlimited(concurrent_stream_limit)),[m
       connect_timer_(parent_.dispatcher().createTimer([this]() { onConnectTimeout(); })) {[m
   conn_connect_ms_ = std::make_unique<Stats::HistogramCompletableTimespanImpl>([m
[31m-      parent_.host()->cluster().stats().upstream_cx_connect_ms_, parent_.dispatcher().timeSource());[m
[32m+[m[32m      parent_.host()->cluster().trafficStats().upstream_cx_connect_ms_,[m
[32m+[m[32m      parent_.dispatcher().timeSource());[m
   conn_length_ = std::make_unique<Stats::HistogramCompletableTimespanImpl>([m
[31m-      parent_.host()->cluster().stats().upstream_cx_length_ms_, parent_.dispatcher().timeSource());[m
[32m+[m[32m      parent_.host()->cluster().trafficStats().upstream_cx_length_ms_,[m
[32m+[m[32m      parent_.dispatcher().timeSource());[m
   connect_timer_->enableTimer(parent_.host()->cluster().connectTimeout());[m
   parent_.host()->stats().cx_total_.inc();[m
   parent_.host()->stats().cx_active_.inc();[m
[31m-  parent_.host()->cluster().stats().upstream_cx_total_.inc();[m
[31m-  parent_.host()->cluster().stats().upstream_cx_active_.inc();[m
[32m+[m[32m  parent_.host()->cluster().trafficStats().upstream_cx_total_.inc();[m
[32m+[m[32m  parent_.host()->cluster().trafficStats().upstream_cx_active_.inc();[m
   parent_.host()->cluster().resourceManager(parent_.priority()).connections().inc();[m
 }[m
 [m
[36m@@ -776,7 +778,7 @@[m [mvoid ActiveClient::releaseResourcesBase() {[m
 [m
     conn_length_->complete();[m
 [m
[31m-    parent_.host()->cluster().stats().upstream_cx_active_.dec();[m
[32m+[m[32m    parent_.host()->cluster().trafficStats().upstream_cx_active_.dec();[m
     parent_.host()->stats().cx_active_.dec();[m
     parent_.host()->cluster().resourceManager(parent_.priority()).connections().dec();[m
   }[m
[36m@@ -784,7 +786,7 @@[m [mvoid ActiveClient::releaseResourcesBase() {[m
 [m
 void ActiveClient::onConnectTimeout() {[m
   ENVOY_CONN_LOG(debug, "connect timeout", *this);[m
[31m-  parent_.host()->cluster().stats().upstream_cx_connect_timeout_.inc();[m
[32m+[m[32m  parent_.host()->cluster().trafficStats().upstream_cx_connect_timeout_.inc();[m
   timed_out_ = true;[m
   close();[m
 }[m
[36m@@ -809,7 +811,7 @@[m [mvoid ActiveClient::onConnectionDurationTimeout() {[m
   }[m
 [m
   ENVOY_CONN_LOG(debug, "max connection duration reached, start draining", *this);[m
[31m-  parent_.host()->cluster().stats().upstream_cx_max_duration_reached_.inc();[m
[32m+[m[32m  parent_.host()->cluster().trafficStats().upstream_cx_max_duration_reached_.inc();[m
   parent_.transitionActiveClientState(*this, Envoy::ConnectionPool::ActiveClient::State::Draining);[m
 [m
   // Close out the draining client if we no longer have active streams.[m
[1mdiff --git a/source/common/event/dispatcher_impl.cc b/source/common/event/dispatcher_impl.cc[m
[1mindex 104d72afc9..fead55e225 100644[m
[1m--- a/source/common/event/dispatcher_impl.cc[m
[1m+++ b/source/common/event/dispatcher_impl.cc[m
[36m@@ -151,8 +151,8 @@[m [mDispatcherImpl::createServerConnection(Network::ConnectionSocketPtr&& socket,[m
                                        Network::TransportSocketPtr&& transport_socket,[m
                                        StreamInfo::StreamInfo& stream_info) {[m
   ASSERT(isThreadSafe());[m
[31m-  return std::make_unique<Network::ServerConnectionImpl>([m
[31m-      *this, std::move(socket), std::move(transport_socket), stream_info, true);[m
[32m+[m[32m  return std::make_unique<Network::ServerConnectionImpl>(*this, std::move(socket),[m
[32m+[m[32m                                                         std::move(transport_socket), stream_info);[m
 }[m
 [m
 Network::ClientConnectionPtr DispatcherImpl::createClientConnection([m
[1mdiff --git a/source/common/http/codec_client.cc b/source/common/http/codec_client.cc[m
[1mindex e7e8008ef1..19c1110618 100644[m
[1m--- a/source/common/http/codec_client.cc[m
[1m+++ b/source/common/http/codec_client.cc[m
[36m@@ -171,7 +171,7 @@[m [mvoid CodecClient::onData(Buffer::Instance& data) {[m
     if (!isPrematureResponseError(status) ||[m
         (!active_requests_.empty() ||[m
          getPrematureResponseHttpCode(status) != Code::RequestTimeout)) {[m
[31m-      host_->cluster().stats().upstream_cx_protocol_error_.inc();[m
[32m+[m[32m      host_->cluster().trafficStats().upstream_cx_protocol_error_.inc();[m
       protocol_error_ = true;[m
     }[m
     close();[m
[1mdiff --git a/source/common/http/codec_client.h b/source/common/http/codec_client.h[m
[1mindex c53fbf5c34..6dee7081f4 100644[m
[1m--- a/source/common/http/codec_client.h[m
[1m+++ b/source/common/http/codec_client.h[m
[36m@@ -164,7 +164,7 @@[m [mprotected:[m
   }[m
 [m
   void onIdleTimeout() {[m
[31m-    host_->cluster().stats().upstream_cx_idle_timeout_.inc();[m
[32m+[m[32m    host_->cluster().trafficStats().upstream_cx_idle_timeout_.inc();[m
     close();[m
   }[m
 [m
[1mdiff --git a/source/common/http/conn_pool_base.cc b/source/common/http/conn_pool_base.cc[m
[1mindex 284e479a86..0ed8e5ef1c 100644[m
[1m--- a/source/common/http/conn_pool_base.cc[m
[1m+++ b/source/common/http/conn_pool_base.cc[m
[36m@@ -99,7 +99,7 @@[m [mstatic const uint64_t DEFAULT_MAX_STREAMS = (1 << 29);[m
 [m
 void MultiplexedActiveClientBase::onGoAway(Http::GoAwayErrorCode) {[m
   ENVOY_CONN_LOG(debug, "remote goaway", *codec_client_);[m
[31m-  parent_.host()->cluster().stats().upstream_cx_close_notify_.inc();[m
[32m+[m[32m  parent_.host()->cluster().trafficStats().upstream_cx_close_notify_.inc();[m
   if (state() != ActiveClient::State::Draining) {[m
     if (codec_client_->numActiveRequests() == 0) {[m
       codec_client_->close();[m
[36m@@ -160,16 +160,16 @@[m [mvoid MultiplexedActiveClientBase::onStreamReset(Http::StreamResetReason reason)[m
   switch (reason) {[m
   case StreamResetReason::ConnectionTermination:[m
   case StreamResetReason::ConnectionFailure:[m
[31m-    parent_.host()->cluster().stats().upstream_rq_pending_failure_eject_.inc();[m
[32m+[m[32m    parent_.host()->cluster().trafficStats().upstream_rq_pending_failure_eject_.inc();[m
     closed_with_active_rq_ = true;[m
     break;[m
   case StreamResetReason::LocalReset:[m
   case StreamResetReason::ProtocolError:[m
   case StreamResetReason::OverloadManager:[m
[31m-    parent_.host()->cluster().stats().upstream_rq_tx_reset_.inc();[m
[32m+[m[32m    parent_.host()->cluster().trafficStats().upstream_rq_tx_reset_.inc();[m
     break;[m
   case StreamResetReason::RemoteReset:[m
[31m-    parent_.host()->cluster().stats().upstream_rq_rx_reset_.inc();[m
[32m+[m[32m    parent_.host()->cluster().trafficStats().upstream_rq_rx_reset_.inc();[m
     break;[m
   case StreamResetReason::LocalRefusedStreamReset:[m
   case StreamResetReason::RemoteRefusedStreamReset:[m
[1mdiff --git a/source/common/http/conn_pool_base.h b/source/common/http/conn_pool_base.h[m
[1mindex 44451ab606..33e2faabf1 100644[m
[1m--- a/source/common/http/conn_pool_base.h[m
[1m+++ b/source/common/http/conn_pool_base.h[m
[36m@@ -129,11 +129,11 @@[m [mpublic:[m
     codec_client_ = parent.createCodecClient(data);[m
     codec_client_->addConnectionCallbacks(*this);[m
     codec_client_->setConnectionStats([m
[31m-        {parent_.host()->cluster().stats().upstream_cx_rx_bytes_total_,[m
[31m-         parent_.host()->cluster().stats().upstream_cx_rx_bytes_buffered_,[m
[31m-         parent_.host()->cluster().stats().upstream_cx_tx_bytes_total_,[m
[31m-         parent_.host()->cluster().stats().upstream_cx_tx_bytes_buffered_,[m
[31m-         &parent_.host()->cluster().stats().bind_errors_, nullptr});[m
[32m+[m[32m        {parent_.host()->cluster().trafficStats().upstream_cx_rx_bytes_total_,[m
[32m+[m[32m         parent_.host()->cluster().trafficStats().upstream_cx_rx_bytes_buffered_,[m
[32m+[m[32m         parent_.host()->cluster().trafficStats().upstream_cx_tx_bytes_total_,[m
[32m+[m[32m         parent_.host()->cluster().trafficStats().upstream_cx_tx_bytes_buffered_,[m
[32m+[m[32m         &parent_.host()->cluster().trafficStats().bind_errors_, nullptr});[m
   }[m
 [m
   absl::optional<Http::Protocol> protocol() const override { return codec_client_->protocol(); }[m
[1mdiff --git a/source/common/http/conn_pool_grid.cc b/source/common/http/conn_pool_grid.cc[m
[1mindex 6c45e48eca..15db9536b9 100644[m
[1m--- a/source/common/http/conn_pool_grid.cc[m
[1m+++ b/source/common/http/conn_pool_grid.cc[m
[36m@@ -382,7 +382,7 @@[m [mHttpServerPropertiesCache::Http3StatusTracker& ConnectivityGrid::getHttp3StatusT[m
 bool ConnectivityGrid::isHttp3Broken() const { return getHttp3StatusTracker().isHttp3Broken(); }[m
 [m
 void ConnectivityGrid::markHttp3Broken() {[m
[31m-  host_->cluster().stats().upstream_http3_broken_.inc();[m
[32m+[m[32m  host_->cluster().trafficStats().upstream_http3_broken_.inc();[m
   getHttp3StatusTracker().markHttp3Broken();[m
 }[m
 [m
[1mdiff --git a/source/common/http/http1/balsa_parser.cc b/source/common/http/http1/balsa_parser.cc[m
[1mindex 4fe04ab5a3..4cba847421 100644[m
[1m--- a/source/common/http/http1/balsa_parser.cc[m
[1m+++ b/source/common/http/http1/balsa_parser.cc[m
[36m@@ -143,7 +143,12 @@[m [mParserStatus BalsaParser::getStatus() const { return status_; }[m
 uint16_t BalsaParser::statusCode() const { return headers_.parsed_response_code(); }[m
 [m
 bool BalsaParser::isHttp11() const {[m
[31m-  return absl::EndsWith(headers_.first_line(), Http::Headers::get().ProtocolStrings.Http11String);[m
[32m+[m[32m  if (framer_.is_request()) {[m
[32m+[m[32m    return absl::EndsWith(headers_.first_line(), Http::Headers::get().ProtocolStrings.Http11String);[m
[32m+[m[32m  } else {[m
[32m+[m[32m    return absl::StartsWith(headers_.first_line(),[m
[32m+[m[32m                            Http::Headers::get().ProtocolStrings.Http11String);[m
[32m+[m[32m  }[m
 }[m
 [m
 absl::optional<uint64_t> BalsaParser::contentLength() const {[m
[1mdiff --git a/source/common/http/http1/conn_pool.cc b/source/common/http/http1/conn_pool.cc[m
[1mindex ab7165f5d3..77cf7ebcdd 100644[m
[1m--- a/source/common/http/http1/conn_pool.cc[m
[1m+++ b/source/common/http/http1/conn_pool.cc[m
[36m@@ -42,7 +42,7 @@[m [mvoid ActiveClient::StreamWrapper::decodeHeaders(ResponseHeaderMapPtr&& headers,[m
   close_connection_ =[m
       HeaderUtility::shouldCloseConnection(parent_.codec_client_->protocol(), *headers);[m
   if (close_connection_) {[m
[31m-    parent_.parent().host()->cluster().stats().upstream_cx_close_notify_.inc();[m
[32m+[m[32m    parent_.parent().host()->cluster().trafficStats().upstream_cx_close_notify_.inc();[m
   }[m
   ResponseDecoderWrapper::decodeHeaders(std::move(headers), end_stream);[m
 }[m
[36m@@ -76,7 +76,7 @@[m [mActiveClient::ActiveClient(HttpConnPoolImplBase& parent,[m
     : Envoy::Http::ActiveClient(parent, parent.host()->cluster().maxRequestsPerConnection(),[m
                                 /* effective_concurrent_stream_limit */ 1,[m
                                 /* configured_concurrent_stream_limit */ 1, data) {[m
[31m-  parent.host()->cluster().stats().upstream_cx_http1_total_.inc();[m
[32m+[m[32m  parent.host()->cluster().trafficStats().upstream_cx_http1_total_.inc();[m
 }[m
 [m
 ActiveClient::~ActiveClient() { ASSERT(!stream_wrapper_.get()); }[m
[1mdiff --git a/source/common/http/http2/conn_pool.cc b/source/common/http/http2/conn_pool.cc[m
[1mindex 8879d75282..67761b3889 100644[m
[1m--- a/source/common/http/http2/conn_pool.cc[m
[1m+++ b/source/common/http/http2/conn_pool.cc[m
[36m@@ -45,7 +45,7 @@[m [mActiveClient::ActiveClient(HttpConnPoolImplBase& parent,[m
     : MultiplexedActiveClientBase([m
           parent, calculateInitialStreamsLimit(parent.cache(), parent.origin(), parent.host()),[m
           parent.host()->cluster().http2Options().max_concurrent_streams().value(),[m
[31m-          parent.host()->cluster().stats().upstream_cx_http2_total_, data) {}[m
[32m+[m[32m          parent.host()->cluster().trafficStats().upstream_cx_http2_total_, data) {}[m
 [m
 ConnectionPool::InstancePtr[m
 allocateConnPool(Event::Dispatcher& dispatcher, Random::RandomGenerator& random_generator,[m
[1mdiff --git a/source/common/http/http3/conn_pool.cc b/source/common/http/http3/conn_pool.cc[m
[1mindex 25d3cc5bc5..061f71b364 100644[m
[1m--- a/source/common/http/http3/conn_pool.cc[m
[1m+++ b/source/common/http/http3/conn_pool.cc[m
[36m@@ -39,9 +39,9 @@[m [mstd::string sni(const Network::TransportSocketOptionsConstSharedPtr& options,[m
 [m
 ActiveClient::ActiveClient(Envoy::Http::HttpConnPoolImplBase& parent,[m
                            Upstream::Host::CreateConnectionData& data)[m
[31m-    : MultiplexedActiveClientBase(parent, getMaxStreams(parent.host()->cluster()),[m
[31m-                                  getMaxStreams(parent.host()->cluster()),[m
[31m-                                  parent.host()->cluster().stats().upstream_cx_http3_total_, data),[m
[32m+[m[32m    : MultiplexedActiveClientBase([m
[32m+[m[32m          parent, getMaxStreams(parent.host()->cluster()), getMaxStreams(parent.host()->cluster()),[m
[32m+[m[32m          parent.host()->cluster().trafficStats().upstream_cx_http3_total_, data),[m
       async_connect_callback_(parent_.dispatcher().createSchedulableCallback([this]() {[m
         if (state() != Envoy::ConnectionPool::ActiveClient::State::Connecting) {[m
           return;[m
[1mdiff --git a/source/common/http/utility.cc b/source/common/http/utility.cc[m
[1mindex e5d8b851f8..b20bbeb086 100644[m
[1m--- a/source/common/http/utility.cc[m
[1m+++ b/source/common/http/utility.cc[m
[36m@@ -551,25 +551,6 @@[m [mbool Utility::isWebSocketUpgradeRequest(const RequestHeaderMap& headers) {[m
                                  Http::Headers::get().UpgradeValues.WebSocket));[m
 }[m
 [m
[31m-void Utility::sendLocalReply(const bool& is_reset, StreamDecoderFilterCallbacks& callbacks,[m
[31m-                             const LocalReplyData& local_reply_data) {[m
[31m-  absl::string_view details;[m
[31m-  if (callbacks.streamInfo().responseCodeDetails().has_value()) {[m
[31m-    details = callbacks.streamInfo().responseCodeDetails().value();[m
[31m-  };[m
[31m-[m
[31m-  sendLocalReply([m
[31m-      is_reset,[m
[31m-      Utility::EncodeFunctions{nullptr, nullptr,[m
[31m-                               [&](ResponseHeaderMapPtr&& headers, bool end_stream) -> void {[m
[31m-                                 callbacks.encodeHeaders(std::move(headers), end_stream, details);[m
[31m-                               },[m
[31m-                               [&](Buffer::Instance& data, bool end_stream) -> void {[m
[31m-                                 callbacks.encodeData(data, end_stream);[m
[31m-                               }},[m
[31m-      local_reply_data);[m
[31m-}[m
[31m-[m
 void Utility::sendLocalReply(const bool& is_reset, const EncodeFunctions& encode_functions,[m
                              const LocalReplyData& local_reply_data) {[m
   // encode_headers() may reset the stream, so the stream must not be reset before calling it.[m
[1mdiff --git a/source/common/http/utility.h b/source/common/http/utility.h[m
[1mindex 641bf2d764..90ae1fbb2f 100644[m
[1m--- a/source/common/http/utility.h[m
[1m+++ b/source/common/http/utility.h[m
[36m@@ -379,17 +379,6 @@[m [mstruct LocalReplyData {[m
   bool is_head_request_ = false;[m
 };[m
 [m
[31m-/**[m
[31m- * Create a locally generated response using filter callbacks.[m
[31m- * @param is_reset boolean reference that indicates whether a stream has been reset. It is the[m
[31m- *        responsibility of the caller to ensure that this is set to false if onDestroy()[m
[31m- *        is invoked in the context of sendLocalReply().[m
[31m- * @param callbacks supplies the filter callbacks to use.[m
[31m- * @param local_reply_data struct which keeps data related to generate reply.[m
[31m- */[m
[31m-void sendLocalReply(const bool& is_reset, StreamDecoderFilterCallbacks& callbacks,[m
[31m-                    const LocalReplyData& local_reply_data);[m
[31m-[m
 /**[m
  * Create a locally generated response using the provided lambdas.[m
 [m
[1mdiff --git a/source/common/matcher/matcher.h b/source/common/matcher/matcher.h[m
[1mindex 265182fc40..ff83ecfbdb 100644[m
[1m--- a/source/common/matcher/matcher.h[m
[1m+++ b/source/common/matcher/matcher.h[m
[36m@@ -24,8 +24,10 @@[m
 namespace Envoy {[m
 namespace Matcher {[m
 [m
[31m-template <class ProtoType> class ActionBase : public Action {[m
[32m+[m[32mtemplate <class ProtoType, class Base = Action> class ActionBase : public Base {[m
 public:[m
[32m+[m[32m  template <typename... Args> ActionBase(Args... args) : Base(args...) {}[m
[32m+[m
   absl::string_view typeUrl() const override { return staticTypeUrl(); }[m
 [m
   static absl::string_view staticTypeUrl() {[m
[1mdiff --git a/source/common/network/connection_impl.cc b/source/common/network/connection_impl.cc[m
[1mindex 796fa59922..0c3da97b93 100644[m
[1m--- a/source/common/network/connection_impl.cc[m
[1m+++ b/source/common/network/connection_impl.cc[m
[36m@@ -811,9 +811,9 @@[m [mvoid ConnectionImpl::dumpState(std::ostream& os, int indent_level) const {[m
 ServerConnectionImpl::ServerConnectionImpl(Event::Dispatcher& dispatcher,[m
                                            ConnectionSocketPtr&& socket,[m
                                            TransportSocketPtr&& transport_socket,[m
[31m-                                           StreamInfo::StreamInfo& stream_info, bool connected)[m
[32m+[m[32m                                           StreamInfo::StreamInfo& stream_info)[m
     : ConnectionImpl(dispatcher, std::move(socket), std::move(transport_socket), stream_info,[m
[31m-                     connected) {}[m
[32m+[m[32m                     true) {}[m
 [m
 void ServerConnectionImpl::setTransportSocketConnectTimeout(std::chrono::milliseconds timeout,[m
                                                             Stats::Counter& timeout_stat) {[m
[1mdiff --git a/source/common/network/connection_impl.h b/source/common/network/connection_impl.h[m
[1mindex bf9933f180..433112eedc 100644[m
[1m--- a/source/common/network/connection_impl.h[m
[1m+++ b/source/common/network/connection_impl.h[m
[36m@@ -238,8 +238,7 @@[m [mprivate:[m
 class ServerConnectionImpl : public ConnectionImpl, virtual public ServerConnection {[m
 public:[m
   ServerConnectionImpl(Event::Dispatcher& dispatcher, ConnectionSocketPtr&& socket,[m
[31m-                       TransportSocketPtr&& transport_socket, StreamInfo::StreamInfo& stream_info,[m
[31m-                       bool connected);[m
[32m+[m[32m                       TransportSocketPtr&& transport_socket, StreamInfo::StreamInfo& stream_info);[m
 [m
   // ServerConnection impl[m
   void setTransportSocketConnectTimeout(std::chrono::milliseconds timeout,[m
[1mdiff --git a/source/common/router/config_impl.cc b/source/common/router/config_impl.cc[m
[1mindex c920c9adfb..decd948fc9 100644[m
[1m--- a/source/common/router/config_impl.cc[m
[1m+++ b/source/common/router/config_impl.cc[m
[36m@@ -1880,9 +1880,7 @@[m [mRouteConstSharedPtr VirtualHostImpl::getRouteFromEntries(const RouteCallback& cb[m
       // The only possible action that can be used within the route matching context[m
       // is the RouteMatchAction, so this must be true.[m
       const auto result = match.result_();[m
[31m-      ASSERT(result->typeUrl() == RouteMatchAction::staticTypeUrl());[m
[31m-      ASSERT(dynamic_cast<RouteMatchAction*>(result.get()));[m
[31m-      const RouteMatchAction& route_action = static_cast<const RouteMatchAction&>(*result);[m
[32m+[m[32m      const RouteMatchAction& route_action = result->getTyped<RouteMatchAction>();[m
 [m
       if (route_action.route()->matches(headers, stream_info, random_value)) {[m
         return route_action.route();[m
[1mdiff --git a/source/common/router/retry_state_impl.cc b/source/common/router/retry_state_impl.cc[m
[1mindex 419e398909..f4d3a868c7 100644[m
[1m--- a/source/common/router/retry_state_impl.cc[m
[1m+++ b/source/common/router/retry_state_impl.cc[m
[36m@@ -185,13 +185,13 @@[m [mvoid RetryStateImpl::enableBackoffTimer() {[m
     // be reused.[m
     ratelimited_backoff_strategy_.reset();[m
 [m
[31m-    cluster_.stats().upstream_rq_retry_backoff_ratelimited_.inc();[m
[32m+[m[32m    cluster_.trafficStats().upstream_rq_retry_backoff_ratelimited_.inc();[m
 [m
   } else {[m
     // Otherwise we use a fully jittered exponential backoff algorithm.[m
     retry_timer_->enableTimer(std::chrono::milliseconds(backoff_strategy_->nextBackOffMs()));[m
 [m
[31m-    cluster_.stats().upstream_rq_retry_backoff_exponential_.inc();[m
[32m+[m[32m    cluster_.trafficStats().upstream_rq_retry_backoff_exponential_.inc();[m
   }[m
 }[m
 [m
[36m@@ -277,7 +277,7 @@[m [mRetryStatus RetryStateImpl::shouldRetry(RetryDecision would_retry, DoRetryCallba[m
   // retry this particular request, we can infer that we did a retry earlier[m
   // and it was successful.[m
   if ((backoff_callback_ || next_loop_callback_) && would_retry == RetryDecision::NoRetry) {[m
[31m-    cluster_.stats().upstream_rq_retry_success_.inc();[m
[32m+[m[32m    cluster_.trafficStats().upstream_rq_retry_success_.inc();[m
     if (vcluster_) {[m
       vcluster_->stats().upstream_rq_retry_success_.inc();[m
     }[m
[36m@@ -295,7 +295,7 @@[m [mRetryStatus RetryStateImpl::shouldRetry(RetryDecision would_retry, DoRetryCallba[m
   // The request has exhausted the number of retries allotted to it by the retry policy configured[m
   // (or the x-envoy-max-retries header).[m
   if (retries_remaining_ == 0) {[m
[31m-    cluster_.stats().upstream_rq_retry_limit_exceeded_.inc();[m
[32m+[m[32m    cluster_.trafficStats().upstream_rq_retry_limit_exceeded_.inc();[m
     if (vcluster_) {[m
       vcluster_->stats().upstream_rq_retry_limit_exceeded_.inc();[m
     }[m
[36m@@ -308,7 +308,7 @@[m [mRetryStatus RetryStateImpl::shouldRetry(RetryDecision would_retry, DoRetryCallba[m
   retries_remaining_--;[m
 [m
   if (!cluster_.resourceManager(priority_).retries().canCreate()) {[m
[31m-    cluster_.stats().upstream_rq_retry_overflow_.inc();[m
[32m+[m[32m    cluster_.trafficStats().upstream_rq_retry_overflow_.inc();[m
     if (vcluster_) {[m
       vcluster_->stats().upstream_rq_retry_overflow_.inc();[m
     }[m
[36m@@ -324,7 +324,7 @@[m [mRetryStatus RetryStateImpl::shouldRetry(RetryDecision would_retry, DoRetryCallba[m
 [m
   ASSERT(!backoff_callback_ && !next_loop_callback_);[m
   cluster_.resourceManager(priority_).retries().inc();[m
[31m-  cluster_.stats().upstream_rq_retry_.inc();[m
[32m+[m[32m  cluster_.trafficStats().upstream_rq_retry_.inc();[m
   if (vcluster_) {[m
     vcluster_->stats().upstream_rq_retry_.inc();[m
   }[m
[1mdiff --git a/source/common/router/router.cc b/source/common/router/router.cc[m
[1mindex acd45f3147..40a377c7e6 100644[m
[1m--- a/source/common/router/router.cc[m
[1m+++ b/source/common/router/router.cc[m
[36m@@ -509,7 +509,7 @@[m [mHttp::FilterHeadersStatus Filter::decodeHeaders(Http::RequestHeaderMap& headers,[m
           modify_headers(headers);[m
         },[m
         absl::nullopt, StreamInfo::ResponseCodeDetails::get().MaintenanceMode);[m
[31m-    cluster_->stats().upstream_rq_maintenance_mode_.inc();[m
[32m+[m[32m    cluster_->trafficStats().upstream_rq_maintenance_mode_.inc();[m
     return Http::FilterHeadersStatus::StopIteration;[m
   }[m
 [m
[36m@@ -754,7 +754,7 @@[m [mHttp::FilterDataStatus Filter::decodeData(Buffer::Instance& data, bool end_strea[m
               "The request payload has at least {} bytes data which exceeds buffer limit {}. Give "[m
               "up on the retry/shadow.",[m
               getLength(callbacks_->decodingBuffer()) + data.length(), retry_shadow_buffer_limit_);[m
[31m-    cluster_->stats().retry_or_shadow_abandoned_.inc();[m
[32m+[m[32m    cluster_->trafficStats().retry_or_shadow_abandoned_.inc();[m
     retry_state_.reset();[m
     buffering = false;[m
     active_shadow_policies_.clear();[m
[36m@@ -956,7 +956,7 @@[m [mvoid Filter::onResponseTimeout() {[m
     if (Runtime::runtimeFeatureEnabled([m
             "envoy.reloadable_features.do_not_await_headers_on_upstream_timeout_to_emit_stats") ||[m
         upstream_request->awaitingHeaders()) {[m
[31m-      cluster_->stats().upstream_rq_timeout_.inc();[m
[32m+[m[32m      cluster_->trafficStats().upstream_rq_timeout_.inc();[m
       if (request_vcluster_) {[m
         request_vcluster_->stats().upstream_rq_timeout_.inc();[m
       }[m
[36m@@ -1030,12 +1030,13 @@[m [mvoid Filter::onSoftPerTryTimeout(UpstreamRequest& upstream_request) {[m
 }[m
 [m
 void Filter::onPerTryIdleTimeout(UpstreamRequest& upstream_request) {[m
[31m-  onPerTryTimeoutCommon(upstream_request, cluster_->stats().upstream_rq_per_try_idle_timeout_,[m
[32m+[m[32m  onPerTryTimeoutCommon(upstream_request,[m
[32m+[m[32m                        cluster_->trafficStats().upstream_rq_per_try_idle_timeout_,[m
                         StreamInfo::ResponseCodeDetails::get().UpstreamPerTryIdleTimeout);[m
 }[m
 [m
 void Filter::onPerTryTimeout(UpstreamRequest& upstream_request) {[m
[31m-  onPerTryTimeoutCommon(upstream_request, cluster_->stats().upstream_rq_per_try_timeout_,[m
[32m+[m[32m  onPerTryTimeoutCommon(upstream_request, cluster_->trafficStats().upstream_rq_per_try_timeout_,[m
                         StreamInfo::ResponseCodeDetails::get().UpstreamPerTryTimeout);[m
 }[m
 [m
[36m@@ -1624,7 +1625,7 @@[m [mbool Filter::setupRedirect(const Http::ResponseHeaderMap& headers) {[m
       convertRequestHeadersForInternalRedirect(*downstream_headers_, *location, status_code) &&[m
       callbacks_->recreateStream(&headers)) {[m
     ENVOY_STREAM_LOG(debug, "Internal redirect succeeded", *callbacks_);[m
[31m-    cluster_->stats().upstream_internal_redirect_succeeded_total_.inc();[m
[32m+[m[32m    cluster_->trafficStats().upstream_internal_redirect_succeeded_total_.inc();[m
     return true;[m
   }[m
   // convertRequestHeadersForInternalRedirect logs failure reasons but log[m
[36m@@ -1637,7 +1638,7 @@[m [mbool Filter::setupRedirect(const Http::ResponseHeaderMap& headers) {[m
     ENVOY_STREAM_LOG(trace, "Internal redirect failed: missing location header", *callbacks_);[m
   }[m
 [m
[31m-  cluster_->stats().upstream_internal_redirect_failed_total_.inc();[m
[32m+[m[32m  cluster_->trafficStats().upstream_internal_redirect_failed_total_.inc();[m
   return false;[m
 }[m
 [m
[1mdiff --git a/source/common/router/upstream_codec_filter.cc b/source/common/router/upstream_codec_filter.cc[m
[1mindex 07e43f4998..33994cbdce 100644[m
[1m--- a/source/common/router/upstream_codec_filter.cc[m
[1m+++ b/source/common/router/upstream_codec_filter.cc[m
[36m@@ -28,12 +28,12 @@[m [mnamespace Envoy {[m
 namespace Router {[m
 [m
 void UpstreamCodecFilter::onBelowWriteBufferLowWatermark() {[m
[31m-  callbacks_->clusterInfo()->stats().upstream_flow_control_resumed_reading_total_.inc();[m
[32m+[m[32m  callbacks_->clusterInfo()->trafficStats().upstream_flow_control_resumed_reading_total_.inc();[m
   callbacks_->upstreamCallbacks()->upstream()->readDisable(false);[m
 }[m
 [m
 void UpstreamCodecFilter::onAboveWriteBufferHighWatermark() {[m
[31m-  callbacks_->clusterInfo()->stats().upstream_flow_control_paused_reading_total_.inc();[m
[32m+[m[32m  callbacks_->clusterInfo()->trafficStats().upstream_flow_control_paused_reading_total_.inc();[m
   callbacks_->upstreamCallbacks()->upstream()->readDisable(true);[m
 }[m
 [m
[1mdiff --git a/source/common/router/upstream_request.cc b/source/common/router/upstream_request.cc[m
[1mindex 5407c2ce1d..cb59213804 100644[m
[1m--- a/source/common/router/upstream_request.cc[m
[1m+++ b/source/common/router/upstream_request.cc[m
[36m@@ -197,7 +197,7 @@[m [mvoid UpstreamRequest::cleanUp() {[m
 [m
   while (downstream_data_disabled_ != 0) {[m
     parent_.callbacks()->onDecoderFilterBelowWriteBufferLowWatermark();[m
[31m-    parent_.cluster()->stats().upstream_flow_control_drained_total_.inc();[m
[32m+[m[32m    parent_.cluster()->trafficStats().upstream_flow_control_drained_total_.inc();[m
     --downstream_data_disabled_;[m
   }[m
   if (allow_upstream_filters_) {[m
[36m@@ -753,7 +753,7 @@[m [mUpstreamToDownstream& UpstreamRequest::upstreamToDownstream() {[m
 }[m
 [m
 void UpstreamRequest::onStreamMaxDurationReached() {[m
[31m-  upstream_host_->cluster().stats().upstream_rq_max_duration_reached_.inc();[m
[32m+[m[32m  upstream_host_->cluster().trafficStats().upstream_rq_max_duration_reached_.inc();[m
 [m
   // The upstream had closed then try to retry along with retry policy.[m
   parent_.onStreamMaxDurationReached(*this);[m
[36m@@ -780,7 +780,7 @@[m [mvoid UpstreamRequest::DownstreamWatermarkManager::onAboveWriteBufferHighWatermar[m
   // The downstream connection is overrun. Pause reads from upstream.[m
   // If there are multiple calls to readDisable either the codec (H2) or the underlying[m
   // Network::Connection (H1) will handle reference counting.[m
[31m-  parent_.parent_.cluster()->stats().upstream_flow_control_paused_reading_total_.inc();[m
[32m+[m[32m  parent_.parent_.cluster()->trafficStats().upstream_flow_control_paused_reading_total_.inc();[m
   parent_.upstream_->readDisable(true);[m
 }[m
 [m
[36m@@ -789,7 +789,7 @@[m [mvoid UpstreamRequest::DownstreamWatermarkManager::onBelowWriteBufferLowWatermark[m
 [m
   // One source of connection blockage has buffer available. Pass this on to the stream, which[m
   // will resume reads if this was the last remaining high watermark.[m
[31m-  parent_.parent_.cluster()->stats().upstream_flow_control_resumed_reading_total_.inc();[m
[32m+[m[32m  parent_.parent_.cluster()->trafficStats().upstream_flow_control_resumed_reading_total_.inc();[m
   parent_.upstream_->readDisable(false);[m
 }[m
 [m
[36m@@ -804,7 +804,7 @@[m [mvoid UpstreamRequest::disableDataFromDownstreamForFlowControl() {[m
   // the per try timeout timer is started only after downstream_end_stream_[m
   // is true.[m
   ASSERT(parent_.upstreamRequests().size() == 1 || parent_.downstreamEndStream());[m
[31m-  parent_.cluster()->stats().upstream_flow_control_backed_up_total_.inc();[m
[32m+[m[32m  parent_.cluster()->trafficStats().upstream_flow_control_backed_up_total_.inc();[m
   parent_.callbacks()->onDecoderFilterAboveWriteBufferHighWatermark();[m
   ++downstream_data_disabled_;[m
 }[m
[36m@@ -820,7 +820,7 @@[m [mvoid UpstreamRequest::enableDataFromDownstreamForFlowControl() {[m
   // the per try timeout timer is started only after downstream_end_stream_[m
   // is true.[m
   ASSERT(parent_.upstreamRequests().size() == 1 || parent_.downstreamEndStream());[m
[31m-  parent_.cluster()->stats().upstream_flow_control_drained_total_.inc();[m
[32m+[m[32m  parent_.cluster()->trafficStats().upstream_flow_control_drained_total_.inc();[m
   parent_.callbacks()->onDecoderFilterBelowWriteBufferLowWatermark();[m
   ASSERT(downstream_data_disabled_ != 0);[m
   if (downstream_data_disabled_ > 0) {[m
[1mdiff --git a/source/common/tcp/conn_pool.cc b/source/common/tcp/conn_pool.cc[m
[1mindex 0ece4a46b1..036315cc02 100644[m
[1m--- a/source/common/tcp/conn_pool.cc[m
[1m+++ b/source/common/tcp/conn_pool.cc[m
[36m@@ -25,11 +25,11 @@[m [mActiveTcpClient::ActiveTcpClient(Envoy::ConnectionPool::ConnPoolImplBase& parent[m
   connection_->addConnectionCallbacks(*this);[m
   read_filter_handle_ = std::make_shared<ConnReadFilter>(*this);[m
   connection_->addReadFilter(read_filter_handle_);[m
[31m-  connection_->setConnectionStats({host->cluster().stats().upstream_cx_rx_bytes_total_,[m
[31m-                                   host->cluster().stats().upstream_cx_rx_bytes_buffered_,[m
[31m-                                   host->cluster().stats().upstream_cx_tx_bytes_total_,[m
[31m-                                   host->cluster().stats().upstream_cx_tx_bytes_buffered_,[m
[31m-                                   &host->cluster().stats().bind_errors_, nullptr});[m
[32m+[m[32m  connection_->setConnectionStats({host->cluster().trafficStats().upstream_cx_rx_bytes_total_,[m
[32m+[m[32m                                   host->cluster().trafficStats().upstream_cx_rx_bytes_buffered_,[m
[32m+[m[32m                                   host->cluster().trafficStats().upstream_cx_tx_bytes_total_,[m
[32m+[m[32m                                   host->cluster().trafficStats().upstream_cx_tx_bytes_buffered_,[m
[32m+[m[32m                                   &host->cluster().trafficStats().bind_errors_, nullptr});[m
   connection_->noDelay(true);[m
   connection_->connect();[m
 }[m
[1mdiff --git a/source/common/tcp_proxy/tcp_proxy.cc b/source/common/tcp_proxy/tcp_proxy.cc[m
[1mindex aed1680c88..473270115e 100644[m
[1m--- a/source/common/tcp_proxy/tcp_proxy.cc[m
[1m+++ b/source/common/tcp_proxy/tcp_proxy.cc[m
[36m@@ -234,12 +234,12 @@[m [mvoid Filter::readDisableUpstream(bool disable) {[m
   if (disable) {[m
     read_callbacks_->upstreamHost()[m
         ->cluster()[m
[31m-        .stats()[m
[32m+[m[32m        .trafficStats()[m
         .upstream_flow_control_paused_reading_total_.inc();[m
   } else {[m
     read_callbacks_->upstreamHost()[m
         ->cluster()[m
[31m-        .stats()[m
[32m+[m[32m        .trafficStats()[m
         .upstream_flow_control_resumed_reading_total_.inc();[m
   }[m
 }[m
[36m@@ -377,7 +377,7 @@[m [mNetwork::FilterStatus Filter::establishUpstreamConnection() {[m
   // will never be released.[m
   if (!cluster->resourceManager(Upstream::ResourcePriority::Default).connections().canCreate()) {[m
     getStreamInfo().setResponseFlag(StreamInfo::ResponseFlag::UpstreamOverflow);[m
[31m-    cluster->stats().upstream_cx_overflow_.inc();[m
[32m+[m[32m    cluster->trafficStats().upstream_cx_overflow_.inc();[m
     onInitFailure(UpstreamFailureReason::ResourceLimitExceeded);[m
     return Network::FilterStatus::StopIteration;[m
   }[m
[36m@@ -385,7 +385,7 @@[m [mNetwork::FilterStatus Filter::establishUpstreamConnection() {[m
   const uint32_t max_connect_attempts = config_->maxConnectAttempts();[m
   if (connect_attempts_ >= max_connect_attempts) {[m
     getStreamInfo().setResponseFlag(StreamInfo::ResponseFlag::UpstreamRetryLimitExceeded);[m
[31m-    cluster->stats().upstream_cx_connect_attempts_exceeded_.inc();[m
[32m+[m[32m    cluster->trafficStats().upstream_cx_connect_attempts_exceeded_.inc();[m
     onInitFailure(UpstreamFailureReason::ConnectFailed);[m
     return Network::FilterStatus::StopIteration;[m
   }[m
[36m@@ -417,7 +417,7 @@[m [mNetwork::FilterStatus Filter::establishUpstreamConnection() {[m
 [m
   if (!maybeTunnel(*thread_local_cluster)) {[m
     // Either cluster is unknown or there are no healthy hosts. tcpConnPool() increments[m
[31m-    // cluster->stats().upstream_cx_none_healthy in the latter case.[m
[32m+[m[32m    // cluster->trafficStats().upstream_cx_none_healthy in the latter case.[m
     getStreamInfo().setResponseFlag(StreamInfo::ResponseFlag::NoHealthyUpstream);[m
     onInitFailure(UpstreamFailureReason::NoHealthyUpstream);[m
   }[m
[1mdiff --git a/source/common/upstream/cluster_manager_impl.cc b/source/common/upstream/cluster_manager_impl.cc[m
[1mindex 6aeef2866c..b591852843 100644[m
[1m--- a/source/common/upstream/cluster_manager_impl.cc[m
[1m+++ b/source/common/upstream/cluster_manager_impl.cc[m
[36m@@ -295,6 +295,9 @@[m [mClusterManagerImpl::ClusterManagerImpl([m
       time_source_(main_thread_dispatcher.timeSource()), dispatcher_(main_thread_dispatcher),[m
       http_context_(http_context), router_context_(router_context),[m
       cluster_stat_names_(stats.symbolTable()),[m
[32m+[m[32m      cluster_config_update_stat_names_(stats.symbolTable()),[m
[32m+[m[32m      cluster_lb_stat_names_(stats.symbolTable()),[m
[32m+[m[32m      cluster_endpoint_stat_names_(stats.symbolTable()),[m
       cluster_load_report_stat_names_(stats.symbolTable()),[m
       cluster_circuit_breakers_stat_names_(stats.symbolTable()),[m
       cluster_request_response_size_stat_names_(stats.symbolTable()),[m
[36m@@ -885,14 +888,14 @@[m [mClusterManagerImpl::loadCluster(const envoy::config::cluster::v3::Cluster& clust[m
   if (cluster_reference.info()->lbType() == LoadBalancerType::RingHash) {[m
     if (!cluster_reference.info()->lbSubsetInfo().isEnabled()) {[m
       cluster_entry_it->second->thread_aware_lb_ = std::make_unique<RingHashLoadBalancer>([m
[31m-          cluster_reference.prioritySet(), cluster_reference.info()->stats(),[m
[32m+[m[32m          cluster_reference.prioritySet(), cluster_reference.info()->lbStats(),[m
           cluster_reference.info()->statsScope(), runtime_, random_,[m
           cluster_reference.info()->lbRingHashConfig(), cluster_reference.info()->lbConfig());[m
     }[m
   } else if (cluster_reference.info()->lbType() == LoadBalancerType::Maglev) {[m
     if (!cluster_reference.info()->lbSubsetInfo().isEnabled()) {[m
       cluster_entry_it->second->thread_aware_lb_ = std::make_unique<MaglevLoadBalancer>([m
[31m-          cluster_reference.prioritySet(), cluster_reference.info()->stats(),[m
[32m+[m[32m          cluster_reference.prioritySet(), cluster_reference.info()->lbStats(),[m
           cluster_reference.info()->statsScope(), runtime_, random_,[m
           cluster_reference.info()->lbMaglevConfig(), cluster_reference.info()->lbConfig());[m
     }[m
[36m@@ -1151,7 +1154,7 @@[m [mHost::CreateConnectionData ClusterManagerImpl::ThreadLocalClusterManagerImpl::Cl[m
     }[m
     return conn_info;[m
   } else {[m
[31m-    cluster_info_->stats().upstream_cx_none_healthy_.inc();[m
[32m+[m[32m    cluster_info_->trafficStats().upstream_cx_none_healthy_.inc();[m
     return {nullptr, nullptr};[m
   }[m
 }[m
[36m@@ -1516,7 +1519,7 @@[m [mClusterManagerImpl::ThreadLocalClusterManagerImpl::ClusterEntry::ClusterEntry([m
   // benefit given the healthy panic, locality, and priority calculations that take place.[m
   if (cluster->lbSubsetInfo().isEnabled()) {[m
     lb_ = std::make_unique<SubsetLoadBalancer>([m
[31m-        cluster->lbType(), priority_set_, parent_.local_priority_set_, cluster->stats(),[m
[32m+[m[32m        cluster->lbType(), priority_set_, parent_.local_priority_set_, cluster->lbStats(),[m
         cluster->statsScope(), parent.parent_.runtime_, parent.parent_.random_,[m
         cluster->lbSubsetInfo(), cluster->lbRingHashConfig(), cluster->lbMaglevConfig(),[m
         cluster->lbRoundRobinConfig(), cluster->lbLeastRequestConfig(), cluster->lbConfig(),[m
[36m@@ -1526,7 +1529,7 @@[m [mClusterManagerImpl::ThreadLocalClusterManagerImpl::ClusterEntry::ClusterEntry([m
     case LoadBalancerType::LeastRequest: {[m
       ASSERT(lb_factory_ == nullptr);[m
       lb_ = std::make_unique<LeastRequestLoadBalancer>([m
[31m-          priority_set_, parent_.local_priority_set_, cluster->stats(), parent.parent_.runtime_,[m
[32m+[m[32m          priority_set_, parent_.local_priority_set_, cluster->lbStats(), parent.parent_.runtime_,[m
           parent.parent_.random_, cluster->lbConfig(), cluster->lbLeastRequestConfig(),[m
           parent.thread_local_dispatcher_.timeSource());[m
       break;[m
[36m@@ -1534,14 +1537,14 @@[m [mClusterManagerImpl::ThreadLocalClusterManagerImpl::ClusterEntry::ClusterEntry([m
     case LoadBalancerType::Random: {[m
       ASSERT(lb_factory_ == nullptr);[m
       lb_ = std::make_unique<RandomLoadBalancer>(priority_set_, parent_.local_priority_set_,[m
[31m-                                                 cluster->stats(), parent.parent_.runtime_,[m
[32m+[m[32m                                                 cluster->lbStats(), parent.parent_.runtime_,[m
                                                  parent.parent_.random_, cluster->lbConfig());[m
       break;[m
     }[m
     case LoadBalancerType::RoundRobin: {[m
       ASSERT(lb_factory_ == nullptr);[m
       lb_ = std::make_unique<RoundRobinLoadBalancer>([m
[31m-          priority_set_, parent_.local_priority_set_, cluster->stats(), parent.parent_.runtime_,[m
[32m+[m[32m          priority_set_, parent_.local_priority_set_, cluster->lbStats(), parent.parent_.runtime_,[m
           parent.parent_.random_, cluster->lbConfig(), cluster->lbRoundRobinConfig(),[m
           parent.thread_local_dispatcher_.timeSource());[m
       break;[m
[36m@@ -1623,7 +1626,7 @@[m [mClusterManagerImpl::ThreadLocalClusterManagerImpl::ClusterEntry::httpConnPoolImp[m
   if (!host) {[m
     if (!peek) {[m
       ENVOY_LOG(debug, "no healthy host for HTTP connection pool");[m
[31m-      cluster_info_->stats().upstream_cx_none_healthy_.inc();[m
[32m+[m[32m      cluster_info_->trafficStats().upstream_cx_none_healthy_.inc();[m
     }[m
     return nullptr;[m
   }[m
[36m@@ -1753,7 +1756,7 @@[m [mClusterManagerImpl::ThreadLocalClusterManagerImpl::ClusterEntry::tcpConnPoolImpl[m
   if (!host) {[m
     if (!peek) {[m
       ENVOY_LOG(debug, "no healthy host for TCP connection pool");[m
[31m-      cluster_info_->stats().upstream_cx_none_healthy_.inc();[m
[32m+[m[32m      cluster_info_->trafficStats().upstream_cx_none_healthy_.inc();[m
     }[m
     return nullptr;[m
   }[m
[1mdiff --git a/source/common/upstream/cluster_manager_impl.h b/source/common/upstream/cluster_manager_impl.h[m
[1mindex 77ccfa324b..b28c92f306 100644[m
[1m--- a/source/common/upstream/cluster_manager_impl.h[m
[1m+++ b/source/common/upstream/cluster_manager_impl.h[m
[36m@@ -328,7 +328,14 @@[m [mpublic:[m
   void[m
   initializeSecondaryClusters(const envoy::config::bootstrap::v3::Bootstrap& bootstrap) override;[m
 [m
[31m-  const ClusterStatNames& clusterStatNames() const override { return cluster_stat_names_; }[m
[32m+[m[32m  const ClusterTrafficStatNames& clusterStatNames() const override { return cluster_stat_names_; }[m
[32m+[m[32m  const ClusterConfigUpdateStatNames& clusterConfigUpdateStatNames() const override {[m
[32m+[m[32m    return cluster_config_update_stat_names_;[m
[32m+[m[32m  }[m
[32m+[m[32m  const ClusterLbStatNames& clusterLbStatNames() const override { return cluster_lb_stat_names_; }[m
[32m+[m[32m  const ClusterEndpointStatNames& clusterEndpointStatNames() const override {[m
[32m+[m[32m    return cluster_endpoint_stat_names_;[m
[32m+[m[32m  }[m
   const ClusterLoadReportStatNames& clusterLoadReportStatNames() const override {[m
     return cluster_load_report_stat_names_;[m
   }[m
[36m@@ -781,7 +788,10 @@[m [mprivate:[m
   Event::Dispatcher& dispatcher_;[m
   Http::Context& http_context_;[m
   Router::Context& router_context_;[m
[31m-  ClusterStatNames cluster_stat_names_;[m
[32m+[m[32m  ClusterTrafficStatNames cluster_stat_names_;[m
[32m+[m[32m  ClusterConfigUpdateStatNames cluster_config_update_stat_names_;[m
[32m+[m[32m  ClusterLbStatNames cluster_lb_stat_names_;[m
[32m+[m[32m  ClusterEndpointStatNames cluster_endpoint_stat_names_;[m
   ClusterLoadReportStatNames cluster_load_report_stat_names_;[m
   ClusterCircuitBreakersStatNames cluster_circuit_breakers_stat_names_;[m
   ClusterRequestResponseSizeStatNames cluster_request_response_size_stat_names_;[m
[1mdiff --git a/source/common/upstream/conn_pool_map_impl.h b/source/common/upstream/conn_pool_map_impl.h[m
[1mindex 1eebd6ec82..e30a73bfd2 100644[m
[1m--- a/source/common/upstream/conn_pool_map_impl.h[m
[1m+++ b/source/common/upstream/conn_pool_map_impl.h[m
[36m@@ -35,7 +35,7 @@[m [mConnPoolMap<KEY_TYPE, POOL_TYPE>::getPool(const KEY_TYPE& key, const PoolFactory[m
   if (!connPoolResource.canCreate()) {[m
     // We're full. Try to free up a pool. If we can't, bail out.[m
     if (!freeOnePool()) {[m
[31m-      host_->cluster().stats().upstream_cx_pool_overflow_.inc();[m
[32m+[m[32m      host_->cluster().trafficStats().upstream_cx_pool_overflow_.inc();[m
       return absl::nullopt;[m
     }[m
 [m
[1mdiff --git a/source/common/upstream/eds.cc b/source/common/upstream/eds.cc[m
[1mindex dda70e1f37..b8dd5ec5b2 100644[m
[1m--- a/source/common/upstream/eds.cc[m
[1m+++ b/source/common/upstream/eds.cc[m
[36m@@ -120,7 +120,7 @@[m [mvoid EdsClusterImpl::BatchUpdateHelper::batchUpdate(PrioritySet::HostUpdateCb& h[m
   }[m
 [m
   if (!cluster_rebuilt) {[m
[31m-    parent_.info_->stats().update_no_rebuild_.inc();[m
[32m+[m[32m    parent_.info_->configUpdateStats().update_no_rebuild_.inc();[m
   }[m
 [m
   // If we didn't setup to initialize when our first round of health checking is complete, just[m
[36m@@ -178,7 +178,7 @@[m [mvoid EdsClusterImpl::onConfigUpdate(const std::vector<Config::DecodedResourceRef[m
       PROTOBUF_GET_MS_OR_DEFAULT(cluster_load_assignment.policy(), endpoint_stale_after, 0);[m
   if (stale_after_ms > 0) {[m
     // Stat to track how often we receive valid assignment_timeout in response.[m
[31m-    info_->stats().assignment_timeout_received_.inc();[m
[32m+[m[32m    info_->configUpdateStats().assignment_timeout_received_.inc();[m
     assignment_timeout_->enableTimer(std::chrono::milliseconds(stale_after_ms));[m
   }[m
 [m
[36m@@ -260,7 +260,7 @@[m [mvoid EdsClusterImpl::onConfigUpdate(const std::vector<Config::DecodedResourceRef[m
 bool EdsClusterImpl::validateUpdateSize(int num_resources) {[m
   if (num_resources == 0) {[m
     ENVOY_LOG(debug, "Missing ClusterLoadAssignment for {} in onConfigUpdate()", cluster_name_);[m
[31m-    info_->stats().update_empty_.inc();[m
[32m+[m[32m    info_->configUpdateStats().update_empty_.inc();[m
     onPreInitComplete();[m
     return false;[m
   }[m
[36m@@ -286,7 +286,7 @@[m [mvoid EdsClusterImpl::onAssignmentTimeout() {[m
   std::vector<Config::DecodedResourceRef> resource_refs = {*decoded_resource};[m
   onConfigUpdate(resource_refs, "");[m
   // Stat to track how often we end up with stale assignments.[m
[31m-  info_->stats().assignment_stale_.inc();[m
[32m+[m[32m  info_->configUpdateStats().assignment_stale_.inc();[m
 }[m
 [m
 void EdsClusterImpl::reloadHealthyHostsHelper(const HostSharedPtr& host) {[m
[1mdiff --git a/source/common/upstream/health_checker_base_impl.cc b/source/common/upstream/health_checker_base_impl.cc[m
[1mindex da5963e194..4d73d67f9c 100644[m
[1m--- a/source/common/upstream/health_checker_base_impl.cc[m
[1m+++ b/source/common/upstream/health_checker_base_impl.cc[m
[36m@@ -105,7 +105,7 @@[m [mstd::chrono::milliseconds HealthCheckerImplBase::interval(HealthState state,[m
   // If a connection has been established, we choose an interval based on the host's health. Please[m
   // refer to the HealthCheck API documentation for more details.[m
   uint64_t base_time_ms;[m
[31m-  if (cluster_.info()->stats().upstream_cx_total_.used()) {[m
[32m+[m[32m  if (cluster_.info()->trafficStats().upstream_cx_total_.used()) {[m
     // When healthy/unhealthy threshold is configured the health transition of a host will be[m
     // delayed. In this situation Envoy should use the edge interval settings between health checks.[m
     //[m
[36m@@ -160,6 +160,9 @@[m [mHealthCheckerImplBase::intervalWithJitter(uint64_t base_time_ms,[m
 [m
 void HealthCheckerImplBase::addHosts(const HostVector& hosts) {[m
   for (const HostSharedPtr& host : hosts) {[m
[32m+[m[32m    if (host->disableActiveHealthCheck()) {[m
[32m+[m[32m      continue;[m
[32m+[m[32m    }[m
     active_sessions_[host] = makeSession(host);[m
     host->setHealthChecker([m
         HealthCheckHostMonitorPtr{new HealthCheckHostMonitorImpl(shared_from_this(), host)});[m
[36m@@ -171,6 +174,9 @@[m [mvoid HealthCheckerImplBase::onClusterMemberUpdate(const HostVector& hosts_added,[m
                                                   const HostVector& hosts_removed) {[m
   addHosts(hosts_added);[m
   for (const HostSharedPtr& host : hosts_removed) {[m
[32m+[m[32m    if (host->disableActiveHealthCheck()) {[m
[32m+[m[32m      continue;[m
[32m+[m[32m    }[m
     auto session_iter = active_sessions_.find(host);[m
     ASSERT(active_sessions_.end() != session_iter);[m
     // This deletion can happen inline in response to a host failure, so we deferred delete.[m
[1mdiff --git a/source/common/upstream/health_discovery_service.cc b/source/common/upstream/health_discovery_service.cc[m
[1mindex 101631cc23..818126c78c 100644[m
[1m--- a/source/common/upstream/health_discovery_service.cc[m
[1m+++ b/source/common/upstream/health_discovery_service.cc[m
[36m@@ -167,6 +167,12 @@[m [menvoy::config::cluster::v3::Cluster HdsDelegate::createClusterConfig([m
 [m
     // add all endpoints for this locality group to the config[m
     for (const auto& endpoint : locality_endpoints.endpoints()) {[m
[32m+[m[32m      if (endpoint.has_health_check_config() &&[m
[32m+[m[32m          endpoint.health_check_config().disable_active_health_check()) {[m
[32m+[m[32m        ENVOY_LOG(debug, "Skip adding the endpoint {} with optional disabled health check for HDS.",[m
[32m+[m[32m                  endpoint.DebugString());[m
[32m+[m[32m        continue;[m
[32m+[m[32m      }[m
       auto* new_endpoint = endpoints->add_lb_endpoints()->mutable_endpoint();[m
       new_endpoint->mutable_address()->MergeFrom(endpoint.address());[m
       new_endpoint->mutable_health_check_config()->MergeFrom(endpoint.health_check_config());[m
[1mdiff --git a/source/common/upstream/load_balancer_impl.cc b/source/common/upstream/load_balancer_impl.cc[m
[1mindex 3cd2987a9e..2ed3728267 100644[m
[1m--- a/source/common/upstream/load_balancer_impl.cc[m
[1m+++ b/source/common/upstream/load_balancer_impl.cc[m
[36m@@ -109,7 +109,7 @@[m [mLoadBalancerBase::choosePriority(uint64_t hash, const HealthyLoad& healthy_per_p[m
 }[m
 [m
 LoadBalancerBase::LoadBalancerBase([m
[31m-    const PrioritySet& priority_set, ClusterStats& stats, Runtime::Loader& runtime,[m
[32m+[m[32m    const PrioritySet& priority_set, ClusterLbStats& stats, Runtime::Loader& runtime,[m
     Random::RandomGenerator& random,[m
     const envoy::config::cluster::v3::Cluster::CommonLbConfig& common_config)[m
     : stats_(stats), runtime_(runtime), random_(random),[m
[36m@@ -351,7 +351,7 @@[m [mLoadBalancerBase::chooseHostSet(LoadBalancerContext* context, uint64_t hash) con[m
 }[m
 [m
 ZoneAwareLoadBalancerBase::ZoneAwareLoadBalancerBase([m
[31m-    const PrioritySet& priority_set, const PrioritySet* local_priority_set, ClusterStats& stats,[m
[32m+[m[32m    const PrioritySet& priority_set, const PrioritySet* local_priority_set, ClusterLbStats& stats,[m
     Runtime::Loader& runtime, Random::RandomGenerator& random,[m
     const envoy::config::cluster::v3::Cluster::CommonLbConfig& common_config)[m
     : LoadBalancerBase(priority_set, stats, runtime, random, common_config),[m
[36m@@ -717,7 +717,7 @@[m [mconst HostVector& ZoneAwareLoadBalancerBase::hostSourceToHosts(HostsSource hosts[m
 }[m
 [m
 EdfLoadBalancerBase::EdfLoadBalancerBase([m
[31m-    const PrioritySet& priority_set, const PrioritySet* local_priority_set, ClusterStats& stats,[m
[32m+[m[32m    const PrioritySet& priority_set, const PrioritySet* local_priority_set, ClusterLbStats& stats,[m
     Runtime::Loader& runtime, Random::RandomGenerator& random,[m
     const envoy::config::cluster::v3::Cluster::CommonLbConfig& common_config,[m
     const absl::optional<envoy::config::cluster::v3::Cluster::SlowStartConfig> slow_start_config,[m
[1mdiff --git a/source/common/upstream/load_balancer_impl.h b/source/common/upstream/load_balancer_impl.h[m
[1mindex c513ccefae..a8483408ec 100644[m
[1m--- a/source/common/upstream/load_balancer_impl.h[m
[1m+++ b/source/common/upstream/load_balancer_impl.h[m
[36m@@ -68,7 +68,7 @@[m [mprotected:[m
    */[m
   void recalculateLoadInTotalPanic();[m
 [m
[31m-  LoadBalancerBase(const PrioritySet& priority_set, ClusterStats& stats, Runtime::Loader& runtime,[m
[32m+[m[32m  LoadBalancerBase(const PrioritySet& priority_set, ClusterLbStats& stats, Runtime::Loader& runtime,[m
                    Random::RandomGenerator& random,[m
                    const envoy::config::cluster::v3::Cluster::CommonLbConfig& common_config);[m
 [m
[36m@@ -103,7 +103,7 @@[m [mprotected:[m
     }[m
   }[m
 [m
[31m-  ClusterStats& stats_;[m
[32m+[m[32m  ClusterLbStats& stats_;[m
   Runtime::Loader& runtime_;[m
   std::deque<uint64_t> stashed_random_;[m
   Random::RandomGenerator& random_;[m
[36m@@ -199,7 +199,7 @@[m [mpublic:[m
 protected:[m
   // Both priority_set and local_priority_set if non-null must have at least one host set.[m
   ZoneAwareLoadBalancerBase([m
[31m-      const PrioritySet& priority_set, const PrioritySet* local_priority_set, ClusterStats& stats,[m
[32m+[m[32m      const PrioritySet& priority_set, const PrioritySet* local_priority_set, ClusterLbStats& stats,[m
       Runtime::Loader& runtime, Random::RandomGenerator& random,[m
       const envoy::config::cluster::v3::Cluster::CommonLbConfig& common_config);[m
 [m
[36m@@ -401,7 +401,7 @@[m [mclass EdfLoadBalancerBase : public ZoneAwareLoadBalancerBase,[m
                             Logger::Loggable<Logger::Id::upstream> {[m
 public:[m
   EdfLoadBalancerBase([m
[31m-      const PrioritySet& priority_set, const PrioritySet* local_priority_set, ClusterStats& stats,[m
[32m+[m[32m      const PrioritySet& priority_set, const PrioritySet* local_priority_set, ClusterLbStats& stats,[m
       Runtime::Loader& runtime, Random::RandomGenerator& random,[m
       const envoy::config::cluster::v3::Cluster::CommonLbConfig& common_config,[m
       const absl::optional<envoy::config::cluster::v3::Cluster::SlowStartConfig> slow_start_cofig,[m
[36m@@ -469,7 +469,7 @@[m [mprotected:[m
 class RoundRobinLoadBalancer : public EdfLoadBalancerBase {[m
 public:[m
   RoundRobinLoadBalancer([m
[31m-      const PrioritySet& priority_set, const PrioritySet* local_priority_set, ClusterStats& stats,[m
[32m+[m[32m      const PrioritySet& priority_set, const PrioritySet* local_priority_set, ClusterLbStats& stats,[m
       Runtime::Loader& runtime, Random::RandomGenerator& random,[m
       const envoy::config::cluster::v3::Cluster::CommonLbConfig& common_config,[m
       const absl::optional<envoy::config::cluster::v3::Cluster::RoundRobinLbConfig>[m
[36m@@ -547,7 +547,7 @@[m [mprivate:[m
 class LeastRequestLoadBalancer : public EdfLoadBalancerBase {[m
 public:[m
   LeastRequestLoadBalancer([m
[31m-      const PrioritySet& priority_set, const PrioritySet* local_priority_set, ClusterStats& stats,[m
[32m+[m[32m      const PrioritySet& priority_set, const PrioritySet* local_priority_set, ClusterLbStats& stats,[m
       Runtime::Loader& runtime, Random::RandomGenerator& random,[m
       const envoy::config::cluster::v3::Cluster::CommonLbConfig& common_config,[m
       const absl::optional<envoy::config::cluster::v3::Cluster::LeastRequestLbConfig>[m
[36m@@ -647,7 +647,8 @@[m [mclass RandomLoadBalancer : public ZoneAwareLoadBalancerBase,[m
                            Logger::Loggable<Logger::Id::upstream> {[m
 public:[m
   RandomLoadBalancer(const PrioritySet& priority_set, const PrioritySet* local_priority_set,[m
[31m-                     ClusterStats& stats, Runtime::Loader& runtime, Random::RandomGenerator& random,[m
[32m+[m[32m                     ClusterLbStats& stats, Runtime::Loader& runtime,[m
[32m+[m[32m                     Random::RandomGenerator& random,[m
                      const envoy::config::cluster::v3::Cluster::CommonLbConfig& common_config)[m
       : ZoneAwareLoadBalancerBase(priority_set, local_priority_set, stats, runtime, random,[m
                                   common_config) {}[m
[1mdiff --git a/source/common/upstream/maglev_lb.cc b/source/common/upstream/maglev_lb.cc[m
[1mindex 7617853725..1f33f094a2 100644[m
[1m--- a/source/common/upstream/maglev_lb.cc[m
[1m+++ b/source/common/upstream/maglev_lb.cc[m
[36m@@ -93,7 +93,7 @@[m [muint64_t MaglevTable::permutation(const TableBuildEntry& entry) {[m
 }[m
 [m
 MaglevLoadBalancer::MaglevLoadBalancer([m
[31m-    const PrioritySet& priority_set, ClusterStats& stats, Stats::Scope& scope,[m
[32m+[m[32m    const PrioritySet& priority_set, ClusterLbStats& stats, Stats::Scope& scope,[m
     Runtime::Loader& runtime, Random::RandomGenerator& random,[m
     const absl::optional<envoy::config::cluster::v3::Cluster::MaglevLbConfig>& config,[m
     const envoy::config::cluster::v3::Cluster::CommonLbConfig& common_config)[m
[1mdiff --git a/source/common/upstream/maglev_lb.h b/source/common/upstream/maglev_lb.h[m
[1mindex 0d1c6fc5f7..3f0d4d5b55 100644[m
[1m--- a/source/common/upstream/maglev_lb.h[m
[1m+++ b/source/common/upstream/maglev_lb.h[m
[36m@@ -72,7 +72,7 @@[m [mclass MaglevLoadBalancer : public ThreadAwareLoadBalancerBase,[m
                            Logger::Loggable<Logger::Id::upstream> {[m
 public:[m
   MaglevLoadBalancer([m
[31m-      const PrioritySet& priority_set, ClusterStats& stats, Stats::Scope& scope,[m
[32m+[m[32m      const PrioritySet& priority_set, ClusterLbStats& stats, Stats::Scope& scope,[m
       Runtime::Loader& runtime, Random::RandomGenerator& random,[m
       const absl::optional<envoy::config::cluster::v3::Cluster::MaglevLbConfig>& config,[m
       const envoy::config::cluster::v3::Cluster::CommonLbConfig& common_config);[m
[1mdiff --git a/source/common/upstream/ring_hash_lb.cc b/source/common/upstream/ring_hash_lb.cc[m
[1mindex f20ebd676b..7548e0d5f1 100644[m
[1m--- a/source/common/upstream/ring_hash_lb.cc[m
[1m+++ b/source/common/upstream/ring_hash_lb.cc[m
[36m@@ -17,7 +17,7 @@[m [mnamespace Envoy {[m
 namespace Upstream {[m
 [m
 RingHashLoadBalancer::RingHashLoadBalancer([m
[31m-    const PrioritySet& priority_set, ClusterStats& stats, Stats::Scope& scope,[m
[32m+[m[32m    const PrioritySet& priority_set, ClusterLbStats& stats, Stats::Scope& scope,[m
     Runtime::Loader& runtime, Random::RandomGenerator& random,[m
     const absl::optional<envoy::config::cluster::v3::Cluster::RingHashLbConfig>& config,[m
     const envoy::config::cluster::v3::Cluster::CommonLbConfig& common_config)[m
[1mdiff --git a/source/common/upstream/ring_hash_lb.h b/source/common/upstream/ring_hash_lb.h[m
[1mindex aa861eb9fb..2ad0fe263d 100644[m
[1m--- a/source/common/upstream/ring_hash_lb.h[m
[1m+++ b/source/common/upstream/ring_hash_lb.h[m
[36m@@ -41,7 +41,7 @@[m [mclass RingHashLoadBalancer : public ThreadAwareLoadBalancerBase,[m
                              Logger::Loggable<Logger::Id::upstream> {[m
 public:[m
   RingHashLoadBalancer([m
[31m-      const PrioritySet& priority_set, ClusterStats& stats, Stats::Scope& scope,[m
[32m+[m[32m      const PrioritySet& priority_set, ClusterLbStats& stats, Stats::Scope& scope,[m
       Runtime::Loader& runtime, Random::RandomGenerator& random,[m
       const absl::optional<envoy::config::cluster::v3::Cluster::RingHashLbConfig>& config,[m
       const envoy::config::cluster::v3::Cluster::CommonLbConfig& common_config);[m
[1mdiff --git a/source/common/upstream/subset_lb.cc b/source/common/upstream/subset_lb.cc[m
[1mindex 4474be84c0..bd7eaa8802 100644[m
[1m--- a/source/common/upstream/subset_lb.cc[m
[1m+++ b/source/common/upstream/subset_lb.cc[m
[36m@@ -23,7 +23,7 @@[m [musing HostPredicate = std::function<bool(const Host&)>;[m
 [m
 SubsetLoadBalancer::SubsetLoadBalancer([m
     LoadBalancerType lb_type, PrioritySet& priority_set, const PrioritySet* local_priority_set,[m
[31m-    ClusterStats& stats, Stats::Scope& scope, Runtime::Loader& runtime,[m
[32m+[m[32m    ClusterLbStats& stats, Stats::Scope& scope, Runtime::Loader& runtime,[m
     Random::RandomGenerator& random, const LoadBalancerSubsetInfo& subsets,[m
     const absl::optional<envoy::config::cluster::v3::Cluster::RingHashLbConfig>&[m
         lb_ring_hash_config,[m
[36m@@ -466,9 +466,9 @@[m [mvoid SubsetLoadBalancer::processSubsets(uint32_t priority, const HostVector& all[m
     }[m
   }[m
 [m
[31m-  // This stat isn't added to `ClusterStats` because it wouldn't be used for nearly all clusters,[m
[31m-  // and is only set during configuration updates, not in the data path, so performance of looking[m
[31m-  // up the stat isn't critical.[m
[32m+[m[32m  // This stat isn't added to `ClusterTrafficStats` because it wouldn't be used for nearly all[m
[32m+[m[32m  // clusters, and is only set during configuration updates, not in the data path, so performance of[m
[32m+[m[32m  // looking up the stat isn't critical.[m
   if (single_duplicate_stat_ == nullptr) {[m
     Stats::StatNameManagedStorage name_storage("lb_subsets_single_host_per_subset_duplicate",[m
                                                scope_.symbolTable());[m
[1mdiff --git a/source/common/upstream/subset_lb.h b/source/common/upstream/subset_lb.h[m
[1mindex 091c15718d..bf02b1ee64 100644[m
[1m--- a/source/common/upstream/subset_lb.h[m
[1m+++ b/source/common/upstream/subset_lb.h[m
[36m@@ -30,7 +30,7 @@[m [mclass SubsetLoadBalancer : public LoadBalancer, Logger::Loggable<Logger::Id::ups[m
 public:[m
   SubsetLoadBalancer([m
       LoadBalancerType lb_type, PrioritySet& priority_set, const PrioritySet* local_priority_set,[m
[31m-      ClusterStats& stats, Stats::Scope& scope, Runtime::Loader& runtime,[m
[32m+[m[32m      ClusterLbStats& stats, Stats::Scope& scope, Runtime::Loader& runtime,[m
       Random::RandomGenerator& random, const LoadBalancerSubsetInfo& subsets,[m
       const absl::optional<envoy::config::cluster::v3::Cluster::RingHashLbConfig>&[m
           lb_ring_hash_config,[m
[36m@@ -365,7 +365,7 @@[m [mprivate:[m
   const absl::optional<envoy::config::cluster::v3::Cluster::LeastRequestLbConfig>[m
       least_request_config_;[m
   const envoy::config::cluster::v3::Cluster::CommonLbConfig common_config_;[m
[31m-  ClusterStats& stats_;[m
[32m+[m[32m  ClusterLbStats& stats_;[m
   Stats::Scope& scope_;[m
   Runtime::Loader& runtime_;[m
   Random::RandomGenerator& random_;[m
[1mdiff --git a/source/common/upstream/thread_aware_lb_impl.cc b/source/common/upstream/thread_aware_lb_impl.cc[m
[1mindex 047ee00518..17c83d315b 100644[m
[1m--- a/source/common/upstream/thread_aware_lb_impl.cc[m
[1m+++ b/source/common/upstream/thread_aware_lb_impl.cc[m
[36m@@ -190,7 +190,7 @@[m [mdouble ThreadAwareLoadBalancerBase::BoundedLoadHashingLoadBalancer::hostOverload[m
   // TODO(scheler): This will not work if rq_active cluster stat is disabled, need to detect[m
   // and alert the user if that's the case.[m
 [m
[31m-  const uint32_t overall_active = host.cluster().stats().upstream_rq_active_.value();[m
[32m+[m[32m  const uint32_t overall_active = host.cluster().trafficStats().upstream_rq_active_.value();[m
   const uint32_t host_active = host.stats().rq_active_.value();[m
 [m
   const uint32_t total_slots = ((overall_active + 1) * hash_balance_factor_ + 99) / 100;[m
[1mdiff --git a/source/common/upstream/thread_aware_lb_impl.h b/source/common/upstream/thread_aware_lb_impl.h[m
[1mindex 81e9d5212d..2cd2b78a17 100644[m
[1m--- a/source/common/upstream/thread_aware_lb_impl.h[m
[1m+++ b/source/common/upstream/thread_aware_lb_impl.h[m
[36m@@ -107,7 +107,7 @@[m [mpublic:[m
 [m
 protected:[m
   ThreadAwareLoadBalancerBase([m
[31m-      const PrioritySet& priority_set, ClusterStats& stats, Runtime::Loader& runtime,[m
[32m+[m[32m      const PrioritySet& priority_set, ClusterLbStats& stats, Runtime::Loader& runtime,[m
       Random::RandomGenerator& random,[m
       const envoy::config::cluster::v3::Cluster::CommonLbConfig& common_config)[m
       : LoadBalancerBase(priority_set, stats, runtime, random, common_config),[m
[36m@@ -121,7 +121,7 @@[m [mprivate:[m
   using PerPriorityStatePtr = std::unique_ptr<PerPriorityState>;[m
 [m
   struct LoadBalancerImpl : public LoadBalancer {[m
[31m-    LoadBalancerImpl(ClusterStats& stats, Random::RandomGenerator& random)[m
[32m+[m[32m    LoadBalancerImpl(ClusterLbStats& stats, Random::RandomGenerator& random)[m
         : stats_(stats), random_(random) {}[m
 [m
     // Upstream::LoadBalancer[m
[36m@@ -138,7 +138,7 @@[m [mprivate:[m
       return {};[m
     }[m
 [m
[31m-    ClusterStats& stats_;[m
[32m+[m[32m    ClusterLbStats& stats_;[m
     Random::RandomGenerator& random_;[m
     std::shared_ptr<std::vector<PerPriorityStatePtr>> per_priority_state_;[m
     std::shared_ptr<HealthyLoad> healthy_per_priority_load_;[m
[36m@@ -146,7 +146,7 @@[m [mprivate:[m
   };[m
 [m
   struct LoadBalancerFactoryImpl : public LoadBalancerFactory {[m
[31m-    LoadBalancerFactoryImpl(ClusterStats& stats, Random::RandomGenerator& random)[m
[32m+[m[32m    LoadBalancerFactoryImpl(ClusterLbStats& stats, Random::RandomGenerator& random)[m
         : stats_(stats), random_(random) {}[m
 [m
     // Upstream::LoadBalancerFactory[m
[36m@@ -154,7 +154,7 @@[m [mprivate:[m
     // Ignore the params for the thread-aware LB.[m
     LoadBalancerPtr create(LoadBalancerParams) override { return create(); }[m
 [m
[31m-    ClusterStats& stats_;[m
[32m+[m[32m    ClusterLbStats& stats_;[m
     Random::RandomGenerator& random_;[m
     absl::Mutex mutex_;[m
     std::shared_ptr<std::vector<PerPriorityStatePtr>> per_priority_state_ ABSL_GUARDED_BY(mutex_);[m
[1mdiff --git a/source/common/upstream/upstream_impl.cc b/source/common/upstream/upstream_impl.cc[m
[1mindex f8f06cdf59..63934a5453 100644[m
[1m--- a/source/common/upstream/upstream_impl.cc[m
[1m+++ b/source/common/upstream/upstream_impl.cc[m
[36m@@ -832,9 +832,9 @@[m [mvoid MainPrioritySetImpl::updateCrossPriorityHostMap(const HostVector& hosts_add[m
   }[m
 }[m
 [m
[31m-ClusterStats ClusterInfoImpl::generateStats(Stats::Scope& scope,[m
[31m-                                            const ClusterStatNames& stat_names) {[m
[31m-  return ClusterStats(stat_names, scope);[m
[32m+[m[32mClusterTrafficStats ClusterInfoImpl::generateStats(Stats::Scope& scope,[m
[32m+[m[32m                                                   const ClusterTrafficStatNames& stat_names) {[m
[32m+[m[32m  return {stat_names, scope};[m
 }[m
 [m
 ClusterRequestResponseSizeStats ClusterInfoImpl::generateRequestResponseSizeStats([m
[36m@@ -845,13 +845,13 @@[m [mClusterRequestResponseSizeStats ClusterInfoImpl::generateRequestResponseSizeStat[m
 ClusterLoadReportStats[m
 ClusterInfoImpl::generateLoadReportStats(Stats::Scope& scope,[m
                                          const ClusterLoadReportStatNames& stat_names) {[m
[31m-  return ClusterLoadReportStats(stat_names, scope);[m
[32m+[m[32m  return {stat_names, scope};[m
 }[m
 [m
 ClusterTimeoutBudgetStats[m
 ClusterInfoImpl::generateTimeoutBudgetStats(Stats::Scope& scope,[m
                                             const ClusterTimeoutBudgetStatNames& stat_names) {[m
[31m-  return ClusterTimeoutBudgetStats(stat_names, scope);[m
[32m+[m[32m  return {stat_names, scope};[m
 }[m
 [m
 // Implements the FactoryContext interface required by network filters.[m
[36m@@ -978,6 +978,10 @@[m [mClusterInfoImpl::ClusterInfoImpl([m
           PROTOBUF_GET_WRAPPED_OR_DEFAULT(config, per_connection_buffer_limit_bytes, 1024 * 1024)),[m
       socket_matcher_(std::move(socket_matcher)), stats_scope_(std::move(stats_scope)),[m
       stats_(generateStats(*stats_scope_, factory_context.clusterManager().clusterStatNames())),[m
[32m+[m[32m      config_update_stats_(factory_context.clusterManager().clusterConfigUpdateStatNames(),[m
[32m+[m[32m                           *stats_scope_),[m
[32m+[m[32m      lb_stats_(factory_context.clusterManager().clusterLbStatNames(), *stats_scope_),[m
[32m+[m[32m      endpoint_stats_(factory_context.clusterManager().clusterEndpointStatNames(), *stats_scope_),[m
       load_report_stats_store_(stats_scope_->symbolTable()),[m
       load_report_stats_(generateLoadReportStats([m
           load_report_stats_store_, factory_context.clusterManager().clusterLoadReportStatNames())),[m
[36m@@ -1322,7 +1326,7 @@[m [mClusterImplBase::ClusterImplBase([m
   priority_update_cb_ = priority_set_.addPriorityUpdateCb([m
       [this](uint32_t, const HostVector& hosts_added, const HostVector& hosts_removed) {[m
         if (!hosts_added.empty() || !hosts_removed.empty()) {[m
[31m-          info_->stats().membership_change_.inc();[m
[32m+[m[32m          info_->endpointStats().membership_change_.inc();[m
         }[m
 [m
         uint32_t healthy_hosts = 0;[m
[36m@@ -1335,10 +1339,10 @@[m [mClusterImplBase::ClusterImplBase([m
           degraded_hosts += host_set->degradedHosts().size();[m
           excluded_hosts += host_set->excludedHosts().size();[m
         }[m
[31m-        info_->stats().membership_total_.set(hosts);[m
[31m-        info_->stats().membership_healthy_.set(healthy_hosts);[m
[31m-        info_->stats().membership_degraded_.set(degraded_hosts);[m
[31m-        info_->stats().membership_excluded_.set(excluded_hosts);[m
[32m+[m[32m        info_->endpointStats().membership_total_.set(hosts);[m
[32m+[m[32m        info_->endpointStats().membership_healthy_.set(healthy_hosts);[m
[32m+[m[32m        info_->endpointStats().membership_degraded_.set(degraded_hosts);[m
[32m+[m[32m        info_->endpointStats().membership_excluded_.set(excluded_hosts);[m
       });[m
 }[m
 [m
[36m@@ -1417,8 +1421,15 @@[m [mvoid ClusterImplBase::onPreInitComplete() {[m
 void ClusterImplBase::onInitDone() {[m
   if (health_checker_ && pending_initialize_health_checks_ == 0) {[m
     for (auto& host_set : prioritySet().hostSetsPerPriority()) {[m
[31m-      pending_initialize_health_checks_ += host_set->hosts().size();[m
[32m+[m[32m      for (auto& host : host_set->hosts()) {[m
[32m+[m[32m        if (host->disableActiveHealthCheck()) {[m
[32m+[m[32m          continue;[m
[32m+[m[32m        }[m
[32m+[m[32m        ++pending_initialize_health_checks_;[m
[32m+[m[32m      }[m
     }[m
[32m+[m[32m    ENVOY_LOG(debug, "Cluster onInitDone pending initialize health check count {}",[m
[32m+[m[32m              pending_initialize_health_checks_);[m
 [m
     // TODO(mattklein123): Remove this callback when done.[m
     health_checker_->addHostCheckCompleteCb([this](HostSharedPtr, HealthTransition) -> void {[m
[36m@@ -1769,8 +1780,9 @@[m [mvoid PriorityStateManager::updateClusterPrioritySet([m
   for (const HostSharedPtr& host : *hosts) {[m
     // Take into consideration when a non-EDS cluster has active health checking, i.e. to mark all[m
     // the hosts unhealthy (host->healthFlagSet(Host::HealthFlag::FAILED_ACTIVE_HC)) and then fire[m
[31m-    // update callbacks to start the health checking process.[m
[31m-    if (health_checker_flag.has_value()) {[m
[32m+[m[32m    // update callbacks to start the health checking process. The endpoint with disabled active[m
[32m+[m[32m    // health check should not be set FAILED_ACTIVE_HC here.[m
[32m+[m[32m    if (health_checker_flag.has_value() && !host->disableActiveHealthCheck()) {[m
       host->healthFlagSet(health_checker_flag.value());[m
     }[m
     hosts_per_locality[host->locality()].push_back(host);[m
[36m@@ -1851,6 +1863,9 @@[m [mbool BaseDynamicClusterImpl::updateDynamicHostList([m
   // Keep track of hosts for which locality is changed.[m
   absl::flat_hash_set<std::string> hosts_with_updated_locality_for_current_priority([m
       current_priority_hosts.size());[m
[32m+[m[32m  // Keep track of hosts for which active health check flag is changed.[m
[32m+[m[32m  absl::flat_hash_set<std::string> hosts_with_active_health_check_flag_changed([m
[32m+[m[32m      current_priority_hosts.size());[m
   HostVector final_hosts;[m
   for (const HostSharedPtr& host : new_hosts) {[m
     // To match a new host with an existing host means comparing their addresses.[m
[36m@@ -1877,7 +1892,14 @@[m [mbool BaseDynamicClusterImpl::updateDynamicHostList([m
       hosts_with_updated_locality_for_current_priority.emplace(existing_host->first);[m
     }[m
 [m
[31m-    const bool skip_inplace_host_update = health_check_address_changed || locality_changed;[m
[32m+[m[32m    const bool active_health_check_flag_changed =[m
[32m+[m[32m        (health_checker_ != nullptr && existing_host_found &&[m
[32m+[m[32m         existing_host->second->disableActiveHealthCheck() != host->disableActiveHealthCheck());[m
[32m+[m[32m    if (active_health_check_flag_changed) {[m
[32m+[m[32m      hosts_with_active_health_check_flag_changed.emplace(existing_host->first);[m
[32m+[m[32m    }[m
[32m+[m[32m    const bool skip_inplace_host_update =[m
[32m+[m[32m        health_check_address_changed || locality_changed || active_health_check_flag_changed;[m
 [m
     // When there is a match and we decided to do in-place update, we potentially update the[m
     // host's health check flag and metadata. Afterwards, the host is pushed back into the[m
[36m@@ -1940,7 +1962,7 @@[m [mbool BaseDynamicClusterImpl::updateDynamicHostList([m
       }[m
 [m
       // If we are depending on a health checker, we initialize to unhealthy.[m
[31m-      if (health_checker_ != nullptr) {[m
[32m+[m[32m      if (health_checker_ != nullptr && !host->disableActiveHealthCheck()) {[m
         host->healthFlagSet(Host::HealthFlag::FAILED_ACTIVE_HC);[m
 [m
         // If we want to exclude hosts until they have been health checked, mark them with[m
[36m@@ -1992,7 +2014,8 @@[m [mbool BaseDynamicClusterImpl::updateDynamicHostList([m
     erase_from = std::remove_if([m
         current_priority_hosts.begin(), current_priority_hosts.end(),[m
         [&all_new_hosts, &new_hosts_for_current_priority,[m
[31m-         &hosts_with_updated_locality_for_current_priority, &final_hosts,[m
[32m+[m[32m         &hosts_with_updated_locality_for_current_priority,[m
[32m+[m[32m         &hosts_with_active_health_check_flag_changed, &final_hosts,[m
          &max_host_weight](const HostSharedPtr& p) {[m
           // This host has already been added as a new host in the[m
           // new_hosts_for_current_priority. Return false here to make sure that host[m
[36m@@ -2001,6 +2024,10 @@[m [mbool BaseDynamicClusterImpl::updateDynamicHostList([m
             return false;[m
           }[m
 [m
[32m+[m[32m          if (hosts_with_active_health_check_flag_changed.contains(p->address()->asString())) {[m
[32m+[m[32m            return false;[m
[32m+[m[32m          }[m
[32m+[m
           if (all_new_hosts.contains(p->address()->asString()) &&[m
               !new_hosts_for_current_priority.contains(p->address()->asString())) {[m
             // If the address is being completely deleted from this priority, but is[m
[36m@@ -2012,8 +2039,11 @@[m [mbool BaseDynamicClusterImpl::updateDynamicHostList([m
             return false;[m
           }[m
 [m
[31m-          if (!(p->healthFlagGet(Host::HealthFlag::FAILED_ACTIVE_HC) ||[m
[31m-                p->healthFlagGet(Host::HealthFlag::FAILED_EDS_HEALTH))) {[m
[32m+[m[32m          // PENDING_DYNAMIC_REMOVAL doesn't apply for the host with disabled active[m
[32m+[m[32m          // health check, the host is removed immediately from this priority.[m
[32m+[m[32m          if ((!(p->healthFlagGet(Host::HealthFlag::FAILED_ACTIVE_HC) ||[m
[32m+[m[32m                 p->healthFlagGet(Host::HealthFlag::FAILED_EDS_HEALTH))) &&[m
[32m+[m[32m              !p->disableActiveHealthCheck()) {[m
             if (p->weight() > max_host_weight) {[m
               max_host_weight = p->weight();[m
             }[m
[36m@@ -2029,7 +2059,7 @@[m [mbool BaseDynamicClusterImpl::updateDynamicHostList([m
 [m
   // At this point we've accounted for all the new hosts as well the hosts that previously[m
   // existed in this priority.[m
[31m-  info_->stats().max_host_weight_.set(max_host_weight);[m
[32m+[m[32m  info_->endpointStats().max_host_weight_.set(max_host_weight);[m
 [m
   // Whatever remains in current_priority_hosts should be removed.[m
   if (!hosts_added_to_current_priority.empty() || !current_priority_hosts.empty()) {[m
[36m@@ -2054,21 +2084,21 @@[m [mgetDnsLookupFamilyFromCluster(const envoy::config::cluster::v3::Cluster& cluster[m
 [m
 void reportUpstreamCxDestroy(const Upstream::HostDescriptionConstSharedPtr& host,[m
                              Network::ConnectionEvent event) {[m
[31m-  host->cluster().stats().upstream_cx_destroy_.inc();[m
[32m+[m[32m  host->cluster().trafficStats().upstream_cx_destroy_.inc();[m
   if (event == Network::ConnectionEvent::RemoteClose) {[m
[31m-    host->cluster().stats().upstream_cx_destroy_remote_.inc();[m
[32m+[m[32m    host->cluster().trafficStats().upstream_cx_destroy_remote_.inc();[m
   } else {[m
[31m-    host->cluster().stats().upstream_cx_destroy_local_.inc();[m
[32m+[m[32m    host->cluster().trafficStats().upstream_cx_destroy_local_.inc();[m
   }[m
 }[m
 [m
 void reportUpstreamCxDestroyActiveRequest(const Upstream::HostDescriptionConstSharedPtr& host,[m
                                           Network::ConnectionEvent event) {[m
[31m-  host->cluster().stats().upstream_cx_destroy_with_active_rq_.inc();[m
[32m+[m[32m  host->cluster().trafficStats().upstream_cx_destroy_with_active_rq_.inc();[m
   if (event == Network::ConnectionEvent::RemoteClose) {[m
[31m-    host->cluster().stats().upstream_cx_destroy_remote_with_active_rq_.inc();[m
[32m+[m[32m    host->cluster().trafficStats().upstream_cx_destroy_remote_with_active_rq_.inc();[m
   } else {[m
[31m-    host->cluster().stats().upstream_cx_destroy_local_with_active_rq_.inc();[m
[32m+[m[32m    host->cluster().trafficStats().upstream_cx_destroy_local_with_active_rq_.inc();[m
   }[m
 }[m
 [m
[1mdiff --git a/source/common/upstream/upstream_impl.h b/source/common/upstream/upstream_impl.h[m
[1mindex 3807d86961..e44289d419 100644[m
[1m--- a/source/common/upstream/upstream_impl.h[m
[1m+++ b/source/common/upstream/upstream_impl.h[m
[36m@@ -266,6 +266,7 @@[m [mpublic:[m
            TimeSource& time_source)[m
       : HostDescriptionImpl(cluster, hostname, address, metadata, locality, health_check_config,[m
                             priority, time_source),[m
[32m+[m[32m        disable_active_health_check_(health_check_config.disable_active_health_check()),[m
         health_status_(health_status) {[m
     // This EDS flags setting is still necessary for stats, configuration dump, canonical[m
     // coarseHealth() etc.[m
[36m@@ -273,6 +274,11 @@[m [mpublic:[m
     HostImpl::weight(initial_weight);[m
   }[m
 [m
[32m+[m[32m  bool disableActiveHealthCheck() const override { return disable_active_health_check_; }[m
[32m+[m[32m  void setDisableActiveHealthCheck(bool disable_active_health_check) override {[m
[32m+[m[32m    disable_active_health_check_ = disable_active_health_check;[m
[32m+[m[32m  }[m
[32m+[m
   // Upstream::Host[m
   std::vector<std::pair<absl::string_view, Stats::PrimitiveCounterReference>>[m
   counters() const override {[m
[36m@@ -359,6 +365,7 @@[m [mprivate:[m
 [m
   std::atomic<uint32_t> health_flags_{};[m
   std::atomic<uint32_t> weight_;[m
[32m+[m[32m  bool disable_active_health_check_;[m
   // TODO(wbpcode): should we store the EDS health status to health_flags_ to get unified status or[m
   // flag access? May be we could refactor HealthFlag to contain all these statuses and flags in the[m
   // future.[m
[36m@@ -703,8 +710,8 @@[m [mpublic:[m
                   TransportSocketMatcherPtr&& socket_matcher, Stats::ScopeSharedPtr&& stats_scope,[m
                   bool added_via_api, Server::Configuration::TransportSocketFactoryContext&);[m
 [m
[31m-  static ClusterStats generateStats(Stats::Scope& scope,[m
[31m-                                    const ClusterStatNames& cluster_stat_names);[m
[32m+[m[32m  static ClusterTrafficStats generateStats(Stats::Scope& scope,[m
[32m+[m[32m                                           const ClusterTrafficStatNames& cluster_stat_names);[m
   static ClusterLoadReportStats[m
   generateLoadReportStats(Stats::Scope& scope, const ClusterLoadReportStatNames& stat_names);[m
   static ClusterCircuitBreakersStats[m
[36m@@ -791,7 +798,10 @@[m [mpublic:[m
   const std::string& observabilityName() const override { return observability_name_; }[m
   ResourceManager& resourceManager(ResourcePriority priority) const override;[m
   TransportSocketMatcher& transportSocketMatcher() const override { return *socket_matcher_; }[m
[31m-  ClusterStats& stats() const override { return stats_; }[m
[32m+[m[32m  ClusterTrafficStats& trafficStats() const override { return stats_; }[m
[32m+[m[32m  ClusterConfigUpdateStats& configUpdateStats() const override { return config_update_stats_; }[m
[32m+[m[32m  ClusterLbStats& lbStats() const override { return lb_stats_; }[m
[32m+[m[32m  ClusterEndpointStats& endpointStats() const override { return endpoint_stats_; }[m
   Stats::Scope& statsScope() const override { return *stats_scope_; }[m
 [m
   ClusterRequestResponseSizeStatsOptRef requestResponseSizeStats() const override {[m
[36m@@ -905,7 +915,10 @@[m [mprivate:[m
   const uint32_t per_connection_buffer_limit_bytes_;[m
   TransportSocketMatcherPtr socket_matcher_;[m
   Stats::ScopeSharedPtr stats_scope_;[m
[31m-  mutable ClusterStats stats_;[m
[32m+[m[32m  mutable ClusterTrafficStats stats_;[m
[32m+[m[32m  mutable ClusterConfigUpdateStats config_update_stats_;[m
[32m+[m[32m  mutable ClusterLbStats lb_stats_;[m
[32m+[m[32m  mutable ClusterEndpointStats endpoint_stats_;[m
   Stats::IsolatedStoreImpl load_report_stats_store_;[m
   mutable ClusterLoadReportStats load_report_stats_;[m
   const std::unique_ptr<OptionalClusterStats> optional_cluster_stats_;[m
[1mdiff --git a/source/extensions/clusters/aggregate/cluster.cc b/source/extensions/clusters/aggregate/cluster.cc[m
[1mindex 78caa5e8a1..bebac8ac11 100644[m
[1m--- a/source/extensions/clusters/aggregate/cluster.cc[m
[1m+++ b/source/extensions/clusters/aggregate/cluster.cc[m
[36m@@ -110,7 +110,7 @@[m [mvoid AggregateClusterLoadBalancer::refresh(OptRef<const std::string> excluded_cl[m
   PriorityContextPtr priority_context = linearizePrioritySet(excluded_cluster);[m
   if (!priority_context->priority_set_.hostSetsPerPriority().empty()) {[m
     load_balancer_ = std::make_unique<LoadBalancerImpl>([m
[31m-        *priority_context, parent_info_->stats(), runtime_, random_, parent_info_->lbConfig());[m
[32m+[m[32m        *priority_context, parent_info_->lbStats(), runtime_, random_, parent_info_->lbConfig());[m
   } else {[m
     load_balancer_ = nullptr;[m
   }[m
[1mdiff --git a/source/extensions/clusters/aggregate/cluster.h b/source/extensions/clusters/aggregate/cluster.h[m
[1mindex 90a267a606..7d0a215d6e 100644[m
[1m--- a/source/extensions/clusters/aggregate/cluster.h[m
[1m+++ b/source/extensions/clusters/aggregate/cluster.h[m
[36m@@ -91,10 +91,10 @@[m [mprivate:[m
   // priority set could be empty, we cannot initialize LoadBalancerBase when priority set is empty.[m
   class LoadBalancerImpl : public Upstream::LoadBalancerBase {[m
   public:[m
[31m-    LoadBalancerImpl(const PriorityContext& priority_context, Upstream::ClusterStats& stats,[m
[32m+[m[32m    LoadBalancerImpl(const PriorityContext& priority_context, Upstream::ClusterLbStats& lb_stats,[m
                      Runtime::Loader& runtime, Random::RandomGenerator& random,[m
                      const envoy::config::cluster::v3::Cluster::CommonLbConfig& common_config)[m
[31m-        : Upstream::LoadBalancerBase(priority_context.priority_set_, stats, runtime, random,[m
[32m+[m[32m        : Upstream::LoadBalancerBase(priority_context.priority_set_, lb_stats, runtime, random,[m
                                      common_config),[m
           priority_context_(priority_context) {}[m
 [m
[1mdiff --git a/source/extensions/clusters/logical_dns/logical_dns_cluster.cc b/source/extensions/clusters/logical_dns/logical_dns_cluster.cc[m
[1mindex ace5cd9e64..fe7107fc87 100644[m
[1m--- a/source/extensions/clusters/logical_dns/logical_dns_cluster.cc[m
[1m+++ b/source/extensions/clusters/logical_dns/logical_dns_cluster.cc[m
[36m@@ -106,7 +106,7 @@[m [mLogicalDnsCluster::~LogicalDnsCluster() {[m
 [m
 void LogicalDnsCluster::startResolve() {[m
   ENVOY_LOG(debug, "starting async DNS resolution for {}", dns_address_);[m
[31m-  info_->stats().update_attempt_.inc();[m
[32m+[m[32m  info_->configUpdateStats().update_attempt_.inc();[m
 [m
   active_dns_query_ = dns_resolver_->resolve([m
       dns_address_, dns_lookup_family_,[m
[36m@@ -121,7 +121,7 @@[m [mvoid LogicalDnsCluster::startResolve() {[m
         // cluster does not update. This ensures that a potentially previously resolved address does[m
         // not stabilize back to 0 hosts.[m
         if (status == Network::DnsResolver::ResolutionStatus::Success && !response.empty()) {[m
[31m-          info_->stats().update_success_.inc();[m
[32m+[m[32m          info_->configUpdateStats().update_success_.inc();[m
           const auto addrinfo = response.front().addrInfo();[m
           // TODO(mattklein123): Move port handling into the DNS interface.[m
           ASSERT(addrinfo.address_ != nullptr);[m
[36m@@ -166,7 +166,7 @@[m [mvoid LogicalDnsCluster::startResolve() {[m
           ENVOY_LOG(debug, "DNS refresh rate reset for {}, refresh rate {} ms", dns_address_,[m
                     final_refresh_rate.count());[m
         } else {[m
[31m-          info_->stats().update_failure_.inc();[m
[32m+[m[32m          info_->configUpdateStats().update_failure_.inc();[m
           final_refresh_rate =[m
               std::chrono::milliseconds(failure_backoff_strategy_->nextBackOffMs());[m
           ENVOY_LOG(debug, "DNS refresh rate reset for {}, (failure) refresh rate {} ms",[m
[1mdiff --git a/source/extensions/clusters/original_dst/original_dst_cluster.cc b/source/extensions/clusters/original_dst/original_dst_cluster.cc[m
[1mindex 32dc599774..5f9f636df1 100644[m
[1m--- a/source/extensions/clusters/original_dst/original_dst_cluster.cc[m
[1m+++ b/source/extensions/clusters/original_dst/original_dst_cluster.cc[m
[36m@@ -124,7 +124,7 @@[m [mOriginalDstCluster::LoadBalancer::requestOverrideHost(LoadBalancerContext* conte[m
   if (request_host == nullptr) {[m
     ENVOY_LOG(debug, "original_dst_load_balancer: invalid override header value. {}",[m
               request_override_host);[m
[31m-    parent_->info()->stats().original_dst_host_invalid_.inc();[m
[32m+[m[32m    parent_->info()->trafficStats().original_dst_host_invalid_.inc();[m
     return nullptr;[m
   }[m
   ENVOY_LOG(debug, "Using request override host {}.", request_override_host);[m
[1mdiff --git a/source/extensions/clusters/redis/redis_cluster.cc b/source/extensions/clusters/redis/redis_cluster.cc[m
[1mindex cb98b84f5c..db0ce184bf 100644[m
[1m--- a/source/extensions/clusters/redis/redis_cluster.cc[m
[1m+++ b/source/extensions/clusters/redis/redis_cluster.cc[m
[36m@@ -142,7 +142,7 @@[m [mvoid RedisCluster::onClusterSlotUpdate(ClusterSlotsSharedPtr&& slots) {[m
     }));[m
     updateAllHosts(hosts_added, hosts_removed, localityLbEndpoint().priority());[m
   } else {[m
[31m-    info_->stats().update_no_rebuild_.inc();[m
[32m+[m[32m    info_->configUpdateStats().update_no_rebuild_.inc();[m
   }[m
 [m
   // TODO(hyang): If there is an initialize callback, fire it now. Note that if the[m
[36m@@ -190,9 +190,9 @@[m [mvoid RedisCluster::DnsDiscoveryResolveTarget::startResolveDns() {[m
         ENVOY_LOG(trace, "async DNS resolution complete for {}", dns_address_);[m
         if (status == Network::DnsResolver::ResolutionStatus::Failure || response.empty()) {[m
           if (status == Network::DnsResolver::ResolutionStatus::Failure) {[m
[31m-            parent_.info_->stats().update_failure_.inc();[m
[32m+[m[32m            parent_.info_->configUpdateStats().update_failure_.inc();[m
           } else {[m
[31m-            parent_.info_->stats().update_empty_.inc();[m
[32m+[m[32m            parent_.info_->configUpdateStats().update_empty_.inc();[m
           }[m
 [m
           if (!resolve_timer_) {[m
[36m@@ -273,7 +273,7 @@[m [mvoid RedisCluster::RedisDiscoverySession::registerDiscoveryAddress([m
 }[m
 [m
 void RedisCluster::RedisDiscoverySession::startResolveRedis() {[m
[31m-  parent_.info_->stats().update_attempt_.inc();[m
[32m+[m[32m  parent_.info_->configUpdateStats().update_attempt_.inc();[m
   // If a resolution is currently in progress, skip it.[m
   if (current_request_) {[m
     ENVOY_LOG(debug, "redis cluster slot request is already in progress for '{}'",[m
[36m@@ -311,9 +311,9 @@[m [mvoid RedisCluster::RedisDiscoverySession::startResolveRedis() {[m
 void RedisCluster::RedisDiscoverySession::updateDnsStats([m
     Network::DnsResolver::ResolutionStatus status, bool empty_response) {[m
   if (status == Network::DnsResolver::ResolutionStatus::Failure) {[m
[31m-    parent_.info_->stats().update_failure_.inc();[m
[32m+[m[32m    parent_.info_->configUpdateStats().update_failure_.inc();[m
   } else if (empty_response) {[m
[31m-    parent_.info_->stats().update_empty_.inc();[m
[32m+[m[32m    parent_.info_->configUpdateStats().update_empty_.inc();[m
   }[m
 }[m
 [m
[36m@@ -558,7 +558,7 @@[m [mbool RedisCluster::RedisDiscoverySession::validateCluster([m
 void RedisCluster::RedisDiscoverySession::onUnexpectedResponse([m
     const NetworkFilters::Common::Redis::RespValuePtr& value) {[m
   ENVOY_LOG(warn, "Unexpected response to cluster slot command: {}", value->toString());[m
[31m-  this->parent_.info_->stats().update_failure_.inc();[m
[32m+[m[32m  this->parent_.info_->configUpdateStats().update_failure_.inc();[m
   resolve_timer_->enableTimer(parent_.cluster_refresh_rate_);[m
 }[m
 [m
[36m@@ -569,7 +569,7 @@[m [mvoid RedisCluster::RedisDiscoverySession::onFailure() {[m
     auto client_to_delete = client_map_.find(current_host_address_);[m
     client_to_delete->second->client_->close();[m
   }[m
[31m-  parent_.info()->stats().update_failure_.inc();[m
[32m+[m[32m  parent_.info()->configUpdateStats().update_failure_.inc();[m
   resolve_timer_->enableTimer(parent_.cluster_refresh_rate_);[m
 }[m
 [m
[1mdiff --git a/source/extensions/clusters/strict_dns/strict_dns_cluster.cc b/source/extensions/clusters/strict_dns/strict_dns_cluster.cc[m
[1mindex ed091974f4..5ec87de45a 100644[m
[1m--- a/source/extensions/clusters/strict_dns/strict_dns_cluster.cc[m
[1m+++ b/source/extensions/clusters/strict_dns/strict_dns_cluster.cc[m
[36m@@ -105,7 +105,7 @@[m [mStrictDnsClusterImpl::ResolveTarget::~ResolveTarget() {[m
 [m
 void StrictDnsClusterImpl::ResolveTarget::startResolve() {[m
   ENVOY_LOG(trace, "starting async DNS resolution for {}", dns_address_);[m
[31m-  parent_.info_->stats().update_attempt_.inc();[m
[32m+[m[32m  parent_.info_->configUpdateStats().update_attempt_.inc();[m
 [m
   active_query_ = parent_.dns_resolver_->resolve([m
       dns_address_, parent_.dns_lookup_family_,[m
[36m@@ -117,7 +117,7 @@[m [mvoid StrictDnsClusterImpl::ResolveTarget::startResolve() {[m
         std::chrono::milliseconds final_refresh_rate = parent_.dns_refresh_rate_ms_;[m
 [m
         if (status == Network::DnsResolver::ResolutionStatus::Success) {[m
[31m-          parent_.info_->stats().update_success_.inc();[m
[32m+[m[32m          parent_.info_->configUpdateStats().update_success_.inc();[m
 [m
           HostVector new_hosts;[m
           std::chrono::seconds ttl_refresh_rate = std::chrono::seconds::max();[m
[36m@@ -164,7 +164,7 @@[m [mvoid StrictDnsClusterImpl::ResolveTarget::startResolve() {[m
 [m
             parent_.updateAllHosts(hosts_added, hosts_removed, locality_lb_endpoints_.priority());[m
           } else {[m
[31m-            parent_.info_->stats().update_no_rebuild_.inc();[m
[32m+[m[32m            parent_.info_->configUpdateStats().update_no_rebuild_.inc();[m
           }[m
 [m
           // reset failure backoff strategy because there was a success.[m
[36m@@ -179,7 +179,7 @@[m [mvoid StrictDnsClusterImpl::ResolveTarget::startResolve() {[m
           ENVOY_LOG(debug, "DNS refresh rate reset for {}, refresh rate {} ms", dns_address_,[m
                     final_refresh_rate.count());[m
         } else {[m
[31m-          parent_.info_->stats().update_failure_.inc();[m
[32m+[m[32m          parent_.info_->configUpdateStats().update_failure_.inc();[m
 [m
           final_refresh_rate =[m
               std::chrono::milliseconds(parent_.failure_backoff_strategy_->nextBackOffMs());[m
[1mdiff --git a/source/extensions/extensions_build_config.bzl b/source/extensions/extensions_build_config.bzl[m
[1mindex 56519a62d0..9f4467964b 100644[m
[1m--- a/source/extensions/extensions_build_config.bzl[m
[1m+++ b/source/extensions/extensions_build_config.bzl[m
[36m@@ -75,6 +75,12 @@[m [mEXTENSIONS = {[m
 [m
     "envoy.matching.common_inputs.environment_variable":       "//source/extensions/matching/common_inputs/environment_variable:config",[m
 [m
[32m+[m[32m    #[m
[32m+[m[32m    # Matching actions[m
[32m+[m[32m    #[m
[32m+[m
[32m+[m[32m    "envoy.matching.actions.format_string":             "//source/extensions/matching/actions/format_string:config",[m
[32m+[m
     #[m
     # HTTP filters[m
     #[m
[36m@@ -381,6 +387,7 @@[m [mEXTENSIONS = {[m
 EXTENSION_CONFIG_VISIBILITY = ["//:extension_config", "//:contrib_library", "//:examples_library", "//:mobile_library"][m
 EXTENSION_PACKAGE_VISIBILITY = ["//:extension_library", "//:contrib_library", "//:examples_library", "//:mobile_library"][m
 CONTRIB_EXTENSION_PACKAGE_VISIBILITY = ["//:contrib_library"][m
[32m+[m[32mMOBILE_PACKAGE_VISIBILITY = ["//:mobile_library"][m
 [m
 # Set this variable to true to disable alwayslink for envoy_cc_library.[m
 # TODO(alyssawilk) audit uses of this in source/ and migrate all libraries to extensions.[m
[1mdiff --git a/source/extensions/extensions_metadata.yaml b/source/extensions/extensions_metadata.yaml[m
[1mindex c4c4fc9f52..c8238def04 100644[m
[1m--- a/source/extensions/extensions_metadata.yaml[m
[1m+++ b/source/extensions/extensions_metadata.yaml[m
[36m@@ -1321,3 +1321,10 @@[m [menvoy.matching.custom_matchers.trie_matcher:[m
   status: alpha[m
   type_urls:[m
   - xds.type.matcher.v3.IPMatcher[m
[32m+[m[32menvoy.matching.actions.format_string:[m
[32m+[m[32m  categories:[m
[32m+[m[32m  - envoy.matching.action[m
[32m+[m[32m  security_posture: unknown[m
[32m+[m[32m  status: alpha[m
[32m+[m[32m  type_urls:[m
[32m+[m[32m  - envoy.config.core.v3.SubstitutionFormatString[m
[1mdiff --git a/source/extensions/filters/common/rbac/matchers.cc b/source/extensions/filters/common/rbac/matchers.cc[m
[1mindex 74a7e34bd1..10ca0b60ed 100644[m
[1m--- a/source/extensions/filters/common/rbac/matchers.cc[m
[1m+++ b/source/extensions/filters/common/rbac/matchers.cc[m
[36m@@ -76,6 +76,8 @@[m [mMatcherConstSharedPtr Matcher::create(const envoy::config::rbac::v3::Principal&[m
     return std::make_shared<const NotMatcher>(principal.not_id());[m
   case envoy::config::rbac::v3::Principal::IdentifierCase::kUrlPath:[m
     return std::make_shared<const PathMatcher>(principal.url_path());[m
[32m+[m[32m  case envoy::config::rbac::v3::Principal::IdentifierCase::kFilterState:[m
[32m+[m[32m    return std::make_shared<const FilterStateMatcher>(principal.filter_state());[m
   case envoy::config::rbac::v3::Principal::IdentifierCase::IDENTIFIER_NOT_SET:[m
     break; // Fall through to PANIC.[m
   }[m
[36m@@ -233,6 +235,11 @@[m [mbool MetadataMatcher::matches(const Network::Connection&, const Envoy::Http::Req[m
   return matcher_.match(info.dynamicMetadata());[m
 }[m
 [m
[32m+[m[32mbool FilterStateMatcher::matches(const Network::Connection&, const Envoy::Http::RequestHeaderMap&,[m
[32m+[m[32m                                 const StreamInfo::StreamInfo& info) const {[m
[32m+[m[32m  return matcher_.match(info.filterState());[m
[32m+[m[32m}[m
[32m+[m
 bool PolicyMatcher::matches(const Network::Connection& connection,[m
                             const Envoy::Http::RequestHeaderMap& headers,[m
                             const StreamInfo::StreamInfo& info) const {[m
[1mdiff --git a/source/extensions/filters/common/rbac/matchers.h b/source/extensions/filters/common/rbac/matchers.h[m
[1mindex a43dbce5c7..2a2b41ea7f 100644[m
[1m--- a/source/extensions/filters/common/rbac/matchers.h[m
[1m+++ b/source/extensions/filters/common/rbac/matchers.h[m
[36m@@ -239,6 +239,18 @@[m [mprivate:[m
   const Envoy::Matchers::MetadataMatcher matcher_;[m
 };[m
 [m
[32m+[m[32mclass FilterStateMatcher : public Matcher {[m
[32m+[m[32mpublic:[m
[32m+[m[32m  FilterStateMatcher(const envoy::type::matcher::v3::FilterStateMatcher& matcher)[m
[32m+[m[32m      : matcher_(matcher) {}[m
[32m+[m
[32m+[m[32m  bool matches(const Network::Connection&, const Envoy::Http::RequestHeaderMap&,[m
[32m+[m[32m               const StreamInfo::StreamInfo& info) const override;[m
[32m+[m
[32m+[m[32mprivate:[m
[32m+[m[32m  const Envoy::Matchers::FilterStateMatcher matcher_;[m
[32m+[m[32m};[m
[32m+[m
 /**[m
  * Perform a match against the request server from the client's connection[m
  * request. This is typically TLS SNI.[m
[1mdiff --git a/source/extensions/filters/http/cache/cache_entry_utils.cc b/source/extensions/filters/http/cache/cache_entry_utils.cc[m
[1mindex 116f450a67..dd5eb27120 100644[m
[1m--- a/source/extensions/filters/http/cache/cache_entry_utils.cc[m
[1m+++ b/source/extensions/filters/http/cache/cache_entry_utils.cc[m
[36m@@ -28,6 +28,58 @@[m [mstd::ostream& operator<<(std::ostream& os, CacheEntryStatus status) {[m
   return os << cacheEntryStatusString(status);[m
 }[m
 [m
[32m+[m[32mnamespace {[m
[32m+[m[32mconst absl::flat_hash_set<Http::LowerCaseString> headersNotToUpdate() {[m
[32m+[m[32m  CONSTRUCT_ON_FIRST_USE([m
[32m+[m[32m      absl::flat_hash_set<Http::LowerCaseString>,[m
[32m+[m[32m      // Content range should not be changed upon validation[m
[32m+[m[32m      Http::Headers::get().ContentRange,[m
[32m+[m
[32m+[m[32m      // Headers that describe the body content should never be updated.[m
[32m+[m[32m      Http::Headers::get().ContentLength,[m
[32m+[m
[32m+[m[32m      // It does not make sense for this level of the code to be updating the ETag, when[m
[32m+[m[32m      // presumably the cached_response_headers reflect this specific ETag.[m
[32m+[m[32m      Http::CustomHeaders::get().Etag,[m
[32m+[m
[32m+[m[32m      // We don't update the cached response on a Vary; we just delete it[m
[32m+[m[32m      // entirely. So don't bother copying over the Vary header.[m
[32m+[m[32m      Http::CustomHeaders::get().Vary);[m
[32m+[m[32m}[m
[32m+[m[32m} // namespace[m
[32m+[m
[32m+[m[32mvoid applyHeaderUpdate(const Http::ResponseHeaderMap& new_headers,[m
[32m+[m[32m                       Http::ResponseHeaderMap& headers_to_update) {[m
[32m+[m[32m  // Assumptions:[m
[32m+[m[32m  // 1. The internet is fast, i.e. we get the result as soon as the server sends it.[m
[32m+[m[32m  //    Race conditions would not be possible because we are always processing up-to-date data.[m
[32m+[m[32m  // 2. No key collision for etag. Therefore, if etag matches it's the same resource.[m
[32m+[m[32m  // 3. Backend is correct. etag is being used as a unique identifier to the resource[m
[32m+[m
[32m+[m[32m  // use other header fields provided in the new response to replace all instances[m
[32m+[m[32m  // of the corresponding header fields in the stored response[m
[32m+[m
[32m+[m[32m  // `updatedHeaderFields` makes sure each field is only removed when we update the header[m
[32m+[m[32m  // field for the first time to handle the case where incoming headers have repeated values[m
[32m+[m[32m  absl::flat_hash_set<Http::LowerCaseString> updatedHeaderFields;[m
[32m+[m[32m  new_headers.iterate([m
[32m+[m[32m      [&headers_to_update, &updatedHeaderFields]([m
[32m+[m[32m          const Http::HeaderEntry& incoming_response_header) -> Http::HeaderMap::Iterate {[m
[32m+[m[32m        Http::LowerCaseString lower_case_key{incoming_response_header.key().getStringView()};[m
[32m+[m[32m        absl::string_view incoming_value{incoming_response_header.value().getStringView()};[m
[32m+[m[32m        if (headersNotToUpdate().contains(lower_case_key)) {[m
[32m+[m[32m          return Http::HeaderMap::Iterate::Continue;[m
[32m+[m[32m        }[m
[32m+[m[32m        if (!updatedHeaderFields.contains(lower_case_key)) {[m
[32m+[m[32m          headers_to_update.setCopy(lower_case_key, incoming_value);[m
[32m+[m[32m          updatedHeaderFields.insert(lower_case_key);[m
[32m+[m[32m        } else {[m
[32m+[m[32m          headers_to_update.addCopy(lower_case_key, incoming_value);[m
[32m+[m[32m        }[m
[32m+[m[32m        return Http::HeaderMap::Iterate::Continue;[m
[32m+[m[32m      });[m
[32m+[m[32m}[m
[32m+[m
 } // namespace Cache[m
 } // namespace HttpFilters[m
 } // namespace Extensions[m
[1mdiff --git a/source/extensions/filters/http/cache/cache_entry_utils.h b/source/extensions/filters/http/cache/cache_entry_utils.h[m
[1mindex d49f76990e..d43c4f6a41 100644[m
[1m--- a/source/extensions/filters/http/cache/cache_entry_utils.h[m
[1m+++ b/source/extensions/filters/http/cache/cache_entry_utils.h[m
[36m@@ -44,6 +44,17 @@[m [menum class CacheEntryStatus {[m
 absl::string_view cacheEntryStatusString(CacheEntryStatus s);[m
 std::ostream& operator<<(std::ostream& os, CacheEntryStatus status);[m
 [m
[32m+[m[32m// For an updateHeaders operation, new headers must be merged into existing headers[m
[32m+[m[32m// for the cache entry. This helper function performs that merge correctly, i.e.[m
[32m+[m[32m// - if a header appears in new_headers, prior values for that header are erased[m
[32m+[m[32m//   from headers_to_update.[m
[32m+[m[32m// - if a header appears more than once in new_headers, all new values are added[m
[32m+[m[32m//   to headers_to_update.[m
[32m+[m[32m// - headers that are not supposed to be updated during updateHeaders operations[m
[32m+[m[32m//   (etag, content-length, content-range, vary) are ignored.[m
[32m+[m[32mvoid applyHeaderUpdate(const Http::ResponseHeaderMap& new_headers,[m
[32m+[m[32m                       Http::ResponseHeaderMap& headers_to_update);[m
[32m+[m
 } // namespace Cache[m
 } // namespace HttpFilters[m
 } // namespace Extensions[m
[1mdiff --git a/source/extensions/filters/http/cache/simple_http_cache/simple_http_cache.cc b/source/extensions/filters/http/cache/simple_http_cache/simple_http_cache.cc[m
[1mindex 944d340866..53d8b26403 100644[m
[1m--- a/source/extensions/filters/http/cache/simple_http_cache/simple_http_cache.cc[m
[1m+++ b/source/extensions/filters/http/cache/simple_http_cache/simple_http_cache.cc[m
[36m@@ -142,24 +142,6 @@[m [mLookupContextPtr SimpleHttpCache::makeLookupContext(LookupRequest&& request,[m
   return std::make_unique<SimpleLookupContext>(*this, std::move(request));[m
 }[m
 [m
[31m-const absl::flat_hash_set<Http::LowerCaseString> SimpleHttpCache::headersNotToUpdate() {[m
[31m-  CONSTRUCT_ON_FIRST_USE([m
[31m-      absl::flat_hash_set<Http::LowerCaseString>,[m
[31m-      // Content range should not be changed upon validation[m
[31m-      Http::Headers::get().ContentRange,[m
[31m-[m
[31m-      // Headers that describe the body content should never be updated.[m
[31m-      Http::Headers::get().ContentLength,[m
[31m-[m
[31m-      // It does not make sense for this level of the code to be updating the ETag, when[m
[31m-      // presumably the cached_response_headers reflect this specific ETag.[m
[31m-      Http::CustomHeaders::get().Etag,[m
[31m-[m
[31m-      // We don't update the cached response on a Vary; we just delete it[m
[31m-      // entirely. So don't bother copying over the Vary header.[m
[31m-      Http::CustomHeaders::get().Vary);[m
[31m-}[m
[31m-[m
 void SimpleHttpCache::updateHeaders(const LookupContext& lookup_context,[m
                                     const Http::ResponseHeaderMap& response_headers,[m
                                     const ResponseMetadata& metadata,[m
[36m@@ -187,34 +169,7 @@[m [mvoid SimpleHttpCache::updateHeaders(const LookupContext& lookup_context,[m
   }[m
   Entry& entry = iter->second;[m
 [m
[31m-  // Assumptions:[m
[31m-  // 1. The internet is fast, i.e. we get the result as soon as the server sends it.[m
[31m-  //    Race conditions would not be possible because we are always processing up-to-date data.[m
[31m-  // 2. No key collision for etag. Therefore, if etag matches it's the same resource.[m
[31m-  // 3. Backend is correct. etag is being used as a unique identifier to the resource[m
[31m-[m
[31m-  // use other header fields provided in the new response to replace all instances[m
[31m-  // of the corresponding header fields in the stored response[m
[31m-[m
[31m-  // `updatedHeaderFields` makes sure each field is only removed when we update the header[m
[31m-  // field for the first time to handle the case where incoming headers have repeated values[m
[31m-  absl::flat_hash_set<Http::LowerCaseString> updatedHeaderFields;[m
[31m-  response_headers.iterate([m
[31m-      [&entry, &updatedHeaderFields]([m
[31m-          const Http::HeaderEntry& incoming_response_header) -> Http::HeaderMap::Iterate {[m
[31m-        Http::LowerCaseString lower_case_key{incoming_response_header.key().getStringView()};[m
[31m-        absl::string_view incoming_value{incoming_response_header.value().getStringView()};[m
[31m-        if (headersNotToUpdate().contains(lower_case_key)) {[m
[31m-          return Http::HeaderMap::Iterate::Continue;[m
[31m-        }[m
[31m-        if (!updatedHeaderFields.contains(lower_case_key)) {[m
[31m-          entry.response_headers_->setCopy(lower_case_key, incoming_value);[m
[31m-          updatedHeaderFields.insert(lower_case_key);[m
[31m-        } else {[m
[31m-          entry.response_headers_->addCopy(lower_case_key, incoming_value);[m
[31m-        }[m
[31m-        return Http::HeaderMap::Iterate::Continue;[m
[31m-      });[m
[32m+[m[32m  applyHeaderUpdate(response_headers, *entry.response_headers_);[m
   entry.metadata_ = metadata;[m
   on_complete(true);[m
 }[m
[1mdiff --git a/source/extensions/filters/http/health_check/health_check.cc b/source/extensions/filters/http/health_check/health_check.cc[m
[1mindex 08b3cb8b29..8a6bb39fdd 100644[m
[1m--- a/source/extensions/filters/http/health_check/health_check.cc[m
[1m+++ b/source/extensions/filters/http/health_check/health_check.cc[m
[36m@@ -145,8 +145,8 @@[m [mvoid HealthCheckFilter::onComplete() {[m
 [m
           break;[m
         }[m
[31m-        const auto& stats = cluster->info()->stats();[m
[31m-        const uint64_t membership_total = stats.membership_total_.value();[m
[32m+[m[32m        const auto& endpoint_stats = cluster->info()->endpointStats();[m
[32m+[m[32m        const uint64_t membership_total = endpoint_stats.membership_total_.value();[m
         if (membership_total == 0) {[m
           // If the cluster exists but is empty, consider the service unhealthy unless[m
           // the specified minimum percent healthy for the cluster happens to be zero.[m
[36m@@ -160,7 +160,8 @@[m [mvoid HealthCheckFilter::onComplete() {[m
         }[m
         // In the general case, consider the service unhealthy if fewer than the[m
         // specified percentage of the servers in the cluster are available (healthy + degraded).[m
[31m-        if ((100UL * (stats.membership_healthy_.value() + stats.membership_degraded_.value())) <[m
[32m+[m[32m        if ((100UL * (endpoint_stats.membership_healthy_.value() +[m
[32m+[m[32m                      endpoint_stats.membership_degraded_.value())) <[m
             membership_total * min_healthy_percentage) {[m
           final_status = Http::Code::ServiceUnavailable;[m
           details = &RcDetails::get().HealthCheckClusterUnhealthy;[m
[1mdiff --git a/source/extensions/filters/network/common/redis/client_impl.cc b/source/extensions/filters/network/common/redis/client_impl.cc[m
[1mindex bb3add9ab8..c2958cbfbb 100644[m
[1m--- a/source/extensions/filters/network/common/redis/client_impl.cc[m
[1m+++ b/source/extensions/filters/network/common/redis/client_impl.cc[m
[36m@@ -77,9 +77,9 @@[m [mClientImpl::ClientImpl(Upstream::HostConstSharedPtr host, Event::Dispatcher& dis[m
       flush_timer_(dispatcher.createTimer([this]() { flushBufferAndResetTimer(); })),[m
       time_source_(dispatcher.timeSource()), redis_command_stats_(redis_command_stats),[m
       scope_(scope), is_transaction_client_(is_transaction_client) {[m
[31m-  host->cluster().stats().upstream_cx_total_.inc();[m
[32m+[m[32m  host->cluster().trafficStats().upstream_cx_total_.inc();[m
   host->stats().cx_total_.inc();[m
[31m-  host->cluster().stats().upstream_cx_active_.inc();[m
[32m+[m[32m  host->cluster().trafficStats().upstream_cx_active_.inc();[m
   host->stats().cx_active_.inc();[m
   connect_or_op_timer_->enableTimer(host->cluster().connectTimeout());[m
 }[m
[36m@@ -87,7 +87,7 @@[m [mClientImpl::ClientImpl(Upstream::HostConstSharedPtr host, Event::Dispatcher& dis[m
 ClientImpl::~ClientImpl() {[m
   ASSERT(pending_requests_.empty());[m
   ASSERT(connection_->state() == Network::Connection::State::Closed);[m
[31m-  host_->cluster().stats().upstream_cx_active_.dec();[m
[32m+[m[32m  host_->cluster().trafficStats().upstream_cx_active_.dec();[m
   host_->stats().cx_active_.dec();[m
 }[m
 [m
[36m@@ -141,10 +141,10 @@[m [mPoolRequest* ClientImpl::makeRequest(const RespValue& request, ClientCallbacks&[m
 void ClientImpl::onConnectOrOpTimeout() {[m
   putOutlierEvent(Upstream::Outlier::Result::LocalOriginTimeout);[m
   if (connected_) {[m
[31m-    host_->cluster().stats().upstream_rq_timeout_.inc();[m
[32m+[m[32m    host_->cluster().trafficStats().upstream_rq_timeout_.inc();[m
     host_->stats().rq_timeout_.inc();[m
   } else {[m
[31m-    host_->cluster().stats().upstream_cx_connect_timeout_.inc();[m
[32m+[m[32m    host_->cluster().trafficStats().upstream_cx_connect_timeout_.inc();[m
     host_->stats().cx_connect_fail_.inc();[m
   }[m
 [m
[36m@@ -156,7 +156,7 @@[m [mvoid ClientImpl::onData(Buffer::Instance& data) {[m
     decoder_->decode(data);[m
   } catch (ProtocolError&) {[m
     putOutlierEvent(Upstream::Outlier::Result::ExtOriginRequestFailed);[m
[31m-    host_->cluster().stats().upstream_cx_protocol_error_.inc();[m
[32m+[m[32m    host_->cluster().trafficStats().upstream_cx_protocol_error_.inc();[m
     host_->stats().rq_error_.inc();[m
     connection_->close(Network::ConnectionCloseType::NoFlush);[m
   }[m
[36m@@ -185,7 +185,7 @@[m [mvoid ClientImpl::onEvent(Network::ConnectionEvent event) {[m
       if (!request.canceled_) {[m
         request.callbacks_.onFailure();[m
       } else {[m
[31m-        host_->cluster().stats().upstream_rq_cancelled_.inc();[m
[32m+[m[32m        host_->cluster().trafficStats().upstream_rq_cancelled_.inc();[m
       }[m
       pending_requests_.pop_front();[m
     }[m
[36m@@ -198,7 +198,7 @@[m [mvoid ClientImpl::onEvent(Network::ConnectionEvent event) {[m
   }[m
 [m
   if (event == Network::ConnectionEvent::RemoteClose && !connected_) {[m
[31m-    host_->cluster().stats().upstream_cx_connect_fail_.inc();[m
[32m+[m[32m    host_->cluster().trafficStats().upstream_cx_connect_fail_.inc();[m
     host_->stats().cx_connect_fail_.inc();[m
   }[m
 }[m
[36m@@ -221,7 +221,7 @@[m [mvoid ClientImpl::onRespValue(RespValuePtr&& value) {[m
   // result in closing the connection.[m
   pending_requests_.pop_front();[m
   if (canceled) {[m
[31m-    host_->cluster().stats().upstream_rq_cancelled_.inc();[m
[32m+[m[32m    host_->cluster().trafficStats().upstream_rq_cancelled_.inc();[m
   } else if (config_.enableRedirection() && !is_transaction_client_ &&[m
              (value->type() == Common::Redis::RespType::Error)) {[m
     std::vector<absl::string_view> err = StringUtil::splitToken(value->asString(), " ", false);[m
[36m@@ -263,14 +263,14 @@[m [mClientImpl::PendingRequest::PendingRequest(ClientImpl& parent, ClientCallbacks&[m
     command_request_timer_ = parent_.redis_command_stats_->createCommandTimer([m
         parent_.scope_, command_, parent_.time_source_);[m
   }[m
[31m-  parent.host_->cluster().stats().upstream_rq_total_.inc();[m
[32m+[m[32m  parent.host_->cluster().trafficStats().upstream_rq_total_.inc();[m
   parent.host_->stats().rq_total_.inc();[m
[31m-  parent.host_->cluster().stats().upstream_rq_active_.inc();[m
[32m+[m[32m  parent.host_->cluster().trafficStats().upstream_rq_active_.inc();[m
   parent.host_->stats().rq_active_.inc();[m
 }[m
 [m
 ClientImpl::PendingRequest::~PendingRequest() {[m
[31m-  parent_.host_->cluster().stats().upstream_rq_active_.dec();[m
[32m+[m[32m  parent_.host_->cluster().trafficStats().upstream_rq_active_.dec();[m
   parent_.host_->stats().rq_active_.dec();[m
 }[m
 [m
[1mdiff --git a/source/extensions/filters/network/redis_proxy/conn_pool_impl.cc b/source/extensions/filters/network/redis_proxy/conn_pool_impl.cc[m
[1mindex 4619fc2d51..4795ab4fec 100644[m
[1m--- a/source/extensions/filters/network/redis_proxy/conn_pool_impl.cc[m
[1m+++ b/source/extensions/filters/network/redis_proxy/conn_pool_impl.cc[m
[36m@@ -454,7 +454,7 @@[m [mvoid InstanceImpl::PendingRequest::onRedirection(Common::Redis::RespValuePtr&& v[m
                 host_address);[m
       auto host = host_;[m
       onResponse(std::move(resp_value_));[m
[31m-      host->cluster().stats().upstream_internal_redirect_failed_total_.inc();[m
[32m+[m[32m      host->cluster().trafficStats().upstream_internal_redirect_failed_total_.inc();[m
     } else {[m
       doRedirection(std::move(resp_value_),[m
                     formatAddress(*result.host_info_.value()->address()->ip()), ask_redirection_);[m
[36m@@ -470,7 +470,7 @@[m [mvoid InstanceImpl::PendingRequest::onRedirection(Common::Redis::RespValuePtr&& v[m
               host_address);[m
     auto host = host_;[m
     onResponse(std::move(resp_value_));[m
[31m-    host->cluster().stats().upstream_internal_redirect_failed_total_.inc();[m
[32m+[m[32m    host->cluster().trafficStats().upstream_internal_redirect_failed_total_.inc();[m
     return;[m
   }[m
   PANIC_DUE_TO_CORRUPT_ENUM;[m
[36m@@ -487,7 +487,7 @@[m [mvoid InstanceImpl::PendingRequest::onLoadDnsCacheComplete([m
     ENVOY_LOG(debug, "DNS lookup failed");[m
     auto host = host_;[m
     onResponse(std::move(resp_value_));[m
[31m-    host->cluster().stats().upstream_internal_redirect_failed_total_.inc();[m
[32m+[m[32m    host->cluster().trafficStats().upstream_internal_redirect_failed_total_.inc();[m
   } else {[m
     doRedirection(std::move(resp_value_), formatAddress(*host_info->address()->ip()),[m
                   ask_redirection_);[m
[36m@@ -509,16 +509,16 @@[m [mvoid InstanceImpl::PendingRequest::doRedirection(Common::Redis::RespValuePtr&& v[m
       !parent_.makeRequestToHost(host_address, Common::Redis::Utility::AskingRequest::instance(),[m
                                  null_client_callbacks)) {[m
     onResponse(std::move(value));[m
[31m-    host->cluster().stats().upstream_internal_redirect_failed_total_.inc();[m
[32m+[m[32m    host->cluster().trafficStats().upstream_internal_redirect_failed_total_.inc();[m
   } else {[m
     request_handler_ =[m
         parent_.makeRequestToHost(host_address, getRequest(incoming_request_), *this);[m
     if (!request_handler_) {[m
       onResponse(std::move(value));[m
[31m-      host->cluster().stats().upstream_internal_redirect_failed_total_.inc();[m
[32m+[m[32m      host->cluster().trafficStats().upstream_internal_redirect_failed_total_.inc();[m
     } else {[m
       parent_.refresh_manager_->onRedirection(parent_.cluster_name_);[m
[31m-      host->cluster().stats().upstream_internal_redirect_succeeded_total_.inc();[m
[32m+[m[32m      host->cluster().trafficStats().upstream_internal_redirect_succeeded_total_.inc();[m
     }[m
   }[m
 }[m
[1mdiff --git a/source/extensions/filters/udp/udp_proxy/udp_proxy_filter.cc b/source/extensions/filters/udp/udp_proxy/udp_proxy_filter.cc[m
[1mindex 118d206bc7..19ab2d997b 100644[m
[1m--- a/source/extensions/filters/udp/udp_proxy/udp_proxy_filter.cc[m
[1m+++ b/source/extensions/filters/udp/udp_proxy/udp_proxy_filter.cc[m
[36m@@ -128,7 +128,7 @@[m [mUdpProxyFilter::ClusterInfo::createSession(Network::UdpRecvData::LocalPeerAddres[m
            .connections()[m
            .canCreate()) {[m
     ENVOY_LOG(debug, "cannot create new connection.");[m
[31m-    cluster_.info()->stats().upstream_cx_overflow_.inc();[m
[32m+[m[32m    cluster_.info()->trafficStats().upstream_cx_overflow_.inc();[m
     return nullptr;[m
   }[m
 [m
[36m@@ -139,7 +139,7 @@[m [mUdpProxyFilter::ClusterInfo::createSession(Network::UdpRecvData::LocalPeerAddres[m
   auto host = chooseHost(addresses.peer_);[m
   if (host == nullptr) {[m
     ENVOY_LOG(debug, "cannot find any valid host.");[m
[31m-    cluster_.info()->stats().upstream_cx_none_healthy_.inc();[m
[32m+[m[32m    cluster_.info()->trafficStats().upstream_cx_none_healthy_.inc();[m
     return nullptr;[m
   }[m
   return createSessionWithHost(std::move(addresses), host);[m
[36m@@ -212,7 +212,7 @@[m [mUdpProxyFilter::PerPacketLoadBalancingClusterInfo::onData(Network::UdpRecvData&[m
   auto host = chooseHost(data.addresses_.peer_);[m
   if (host == nullptr) {[m
     ENVOY_LOG(debug, "cannot find any valid host.");[m
[31m-    cluster_.info()->stats().upstream_cx_none_healthy_.inc();[m
[32m+[m[32m    cluster_.info()->trafficStats().upstream_cx_none_healthy_.inc();[m
     return Network::FilterStatus::StopIteration;[m
   }[m
 [m
[36m@@ -407,7 +407,7 @@[m [mvoid UdpProxyFilter::ActiveSession::write(const Buffer::Instance& buffer) {[m
     cluster_.cluster_stats_.sess_tx_errors_.inc();[m
   } else {[m
     cluster_.cluster_stats_.sess_tx_datagrams_.inc();[m
[31m-    cluster_.cluster_.info()->stats().upstream_cx_tx_bytes_total_.add(buffer_length);[m
[32m+[m[32m    cluster_.cluster_.info()->trafficStats().upstream_cx_tx_bytes_total_.add(buffer_length);[m
   }[m
 }[m
 [m
[36m@@ -420,7 +420,7 @@[m [mvoid UdpProxyFilter::ActiveSession::processPacket(Network::Address::InstanceCons[m
   const uint64_t buffer_length = buffer->length();[m
 [m
   cluster_.cluster_stats_.sess_rx_datagrams_.inc();[m
[31m-  cluster_.cluster_.info()->stats().upstream_cx_rx_bytes_total_.add(buffer_length);[m
[32m+[m[32m  cluster_.cluster_.info()->trafficStats().upstream_cx_rx_bytes_total_.add(buffer_length);[m
 [m
   Network::UdpSendData data{addresses_.local_->ip(), *addresses_.peer_, *buffer};[m
   const Api::IoCallUint64Result rc = cluster_.filter_.read_callbacks_->udpListener().send(data);[m
[1mdiff --git a/source/extensions/matching/actions/format_string/BUILD b/source/extensions/matching/actions/format_string/BUILD[m
[1mnew file mode 100644[m
[1mindex 0000000000..0861595683[m
[1m--- /dev/null[m
[1m+++ b/source/extensions/matching/actions/format_string/BUILD[m
[36m@@ -0,0 +1,26 @@[m
[32m+[m[32mload([m
[32m+[m[32m    "//bazel:envoy_build_system.bzl",[m
[32m+[m[32m    "envoy_cc_extension",[m
[32m+[m[32m    "envoy_extension_package",[m
[32m+[m[32m)[m
[32m+[m
[32m+[m[32mlicenses(["notice"])  # Apache 2[m
[32m+[m
[32m+[m[32menvoy_extension_package()[m
[32m+[m
[32m+[m[32menvoy_cc_extension([m
[32m+[m[32m    name = "config",[m
[32m+[m[32m    srcs = ["config.cc"],[m
[32m+[m[32m    hdrs = ["config.h"],[m
[32m+[m[32m    deps = [[m
[32m+[m[32m        "//envoy/formatter:substitution_formatter_interface",[m
[32m+[m[32m        "//envoy/matcher:matcher_interface",[m
[32m+[m[32m        "//envoy/registry",[m
[32m+[m[32m        "//envoy/server:factory_context_interface",[m
[32m+[m[32m        "//source/common/formatter:substitution_format_string_lib",[m
[32m+[m[32m        "//source/common/http:header_map_lib",[m
[32m+[m[32m        "//source/common/matcher:matcher_lib",[m
[32m+[m[32m        "//source/server:filter_chain_manager_lib",[m
[32m+[m[32m        "@envoy_api//envoy/config/core/v3:pkg_cc_proto",[m
[32m+[m[32m    ],[m
[32m+[m[32m)[m
[1mdiff --git a/source/extensions/matching/actions/format_string/config.cc b/source/extensions/matching/actions/format_string/config.cc[m
[1mnew file mode 100644[m
[1mindex 0000000000..038e6ed6bd[m
[1m--- /dev/null[m
[1m+++ b/source/extensions/matching/actions/format_string/config.cc[m
[36m@@ -0,0 +1,46 @@[m
[32m+[m[32m#include "source/extensions/matching/actions/format_string/config.h"[m
[32m+[m
[32m+[m[32m#include "envoy/registry/registry.h"[m
[32m+[m
[32m+[m[32m#include "source/common/formatter/substitution_format_string.h"[m
[32m+[m[32m#include "source/common/http/header_map_impl.h"[m
[32m+[m[32m#include "source/common/protobuf/utility.h"[m
[32m+[m
[32m+[m[32mnamespace Envoy {[m
[32m+[m[32mnamespace Extensions {[m
[32m+[m[32mnamespace Matching {[m
[32m+[m[32mnamespace Actions {[m
[32m+[m[32mnamespace FormatString {[m
[32m+[m
[32m+[m[32mconst Network::FilterChain* ActionImpl::get(const Server::FilterChainsByName& filter_chains_by_name,[m
[32m+[m[32m                                            const StreamInfo::StreamInfo& info) const {[m
[32m+[m[32m  const std::string name =[m
[32m+[m[32m      formatter_->format(*Http::StaticEmptyHeaders::get().request_headers,[m
[32m+[m[32m                         *Http::StaticEmptyHeaders::get().response_headers,[m
[32m+[m[32m                         *Http::StaticEmptyHeaders::get().response_trailers, info, "");[m
[32m+[m[32m  const auto chain_match = filter_chains_by_name.find(name);[m
[32m+[m[32m  if (chain_match != filter_chains_by_name.end()) {[m
[32m+[m[32m    return chain_match->second.get();[m
[32m+[m[32m  }[m
[32m+[m[32m  return nullptr;[m
[32m+[m[32m}[m
[32m+[m
[32m+[m[32mMatcher::ActionFactoryCb[m
[32m+[m[32mActionFactory::createActionFactoryCb(const Protobuf::Message& proto_config,[m
[32m+[m[32m                                     Server::FilterChainActionFactoryContext& context,[m
[32m+[m[32m                                     ProtobufMessage::ValidationVisitor& validator) {[m
[32m+[m[32m  const auto& config =[m
[32m+[m[32m      MessageUtil::downcastAndValidate<const envoy::config::core::v3::SubstitutionFormatString&>([m
[32m+[m[32m          proto_config, validator);[m
[32m+[m[32m  Formatter::FormatterConstSharedPtr formatter =[m
[32m+[m[32m      Formatter::SubstitutionFormatStringUtils::fromProtoConfig(config, context);[m
[32m+[m[32m  return [formatter]() { return std::make_unique<ActionImpl>(formatter); };[m
[32m+[m[32m}[m
[32m+[m
[32m+[m[32mREGISTER_FACTORY(ActionFactory, Matcher::ActionFactory<Server::FilterChainActionFactoryContext>);[m
[32m+[m
[32m+[m[32m} // namespace FormatString[m
[32m+[m[32m} // namespace Actions[m
[32m+[m[32m} // namespace Matching[m
[32m+[m[32m} // namespace Extensions[m
[32m+[m[32m} // namespace Envoy[m
[1mdiff --git a/source/extensions/matching/actions/format_string/config.h b/source/extensions/matching/actions/format_string/config.h[m
[1mnew file mode 100644[m
[1mindex 0000000000..7006a3e4aa[m
[1m--- /dev/null[m
[1m+++ b/source/extensions/matching/actions/format_string/config.h[m
[36m@@ -0,0 +1,45 @@[m
[32m+[m[32m#pragma once[m
[32m+[m
[32m+[m[32m#include "envoy/config/core/v3/substitution_format_string.pb.h"[m
[32m+[m[32m#include "envoy/config/core/v3/substitution_format_string.pb.validate.h"[m
[32m+[m[32m#include "envoy/formatter/substitution_formatter.h"[m
[32m+[m[32m#include "envoy/matcher/matcher.h"[m
[32m+[m[32m#include "envoy/server/factory_context.h"[m
[32m+[m
[32m+[m[32m#include "source/common/matcher/matcher.h"[m
[32m+[m[32m#include "source/server/filter_chain_manager_impl.h"[m
[32m+[m
[32m+[m[32mnamespace Envoy {[m
[32m+[m[32mnamespace Extensions {[m
[32m+[m[32mnamespace Matching {[m
[32m+[m[32mnamespace Actions {[m
[32m+[m[32mnamespace FormatString {[m
[32m+[m
[32m+[m[32mclass ActionImpl : public Matcher::ActionBase<envoy::config::core::v3::SubstitutionFormatString,[m
[32m+[m[32m                                              Server::FilterChainBaseAction> {[m
[32m+[m[32mpublic:[m
[32m+[m[32m  ActionImpl(const Formatter::FormatterConstSharedPtr& formatter) : formatter_(formatter) {}[m
[32m+[m[32m  const Network::FilterChain* get(const Server::FilterChainsByName& filter_chains_by_name,[m
[32m+[m[32m                                  const StreamInfo::StreamInfo& info) const override;[m
[32m+[m
[32m+[m[32mprivate:[m
[32m+[m[32m  const Formatter::FormatterConstSharedPtr formatter_;[m
[32m+[m[32m};[m
[32m+[m
[32m+[m[32mclass ActionFactory : public Matcher::ActionFactory<Server::FilterChainActionFactoryContext> {[m
[32m+[m[32mpublic:[m
[32m+[m[32m  std::string name() const override { return "envoy.matching.actions.format_string"; }[m
[32m+[m[32m  Matcher::ActionFactoryCb[m
[32m+[m[32m  createActionFactoryCb(const Protobuf::Message& proto_config,[m
[32m+[m[32m                        Server::FilterChainActionFactoryContext& context,[m
[32m+[m[32m                        ProtobufMessage::ValidationVisitor& validator) override;[m
[32m+[m[32m  ProtobufTypes::MessagePtr createEmptyConfigProto() override {[m
[32m+[m[32m    return std::make_unique<envoy::config::core::v3::SubstitutionFormatString>();[m
[32m+[m[32m  }[m
[32m+[m[32m};[m
[32m+[m
[32m+[m[32m} // namespace FormatString[m
[32m+[m[32m} // namespace Actions[m
[32m+[m[32m} // namespace Matching[m
[32m+[m[32m} // namespace Extensions[m
[32m+[m[32m} // namespace Envoy[m
[1mdiff --git a/source/extensions/stat_sinks/common/statsd/statsd.cc b/source/extensions/stat_sinks/common/statsd/statsd.cc[m
[1mindex e18c54cd2f..47eef9a1d2 100644[m
[1m--- a/source/extensions/stat_sinks/common/statsd/statsd.cc[m
[1m+++ b/source/extensions/stat_sinks/common/statsd/statsd.cc[m
[36m@@ -330,7 +330,7 @@[m [mvoid TcpStatsdSink::TlsSink::write(Buffer::Instance& buffer) {[m
   //       since if we stay over, the other threads will eventually kill their connections too.[m
   // TODO(mattklein123): The use of the stat is somewhat of a hack, and should be replaced with[m
   // real flow control callbacks once they are available.[m
[31m-  if (parent_.cluster_info_->stats().upstream_cx_tx_bytes_buffered_.value() >[m
[32m+[m[32m  if (parent_.cluster_info_->trafficStats().upstream_cx_tx_bytes_buffered_.value() >[m
       MAX_BUFFERED_STATS_BYTES) {[m
     if (connection_) {[m
       connection_->close(Network::ConnectionCloseType::NoFlush);[m
[36m@@ -354,11 +354,12 @@[m [mvoid TcpStatsdSink::TlsSink::write(Buffer::Instance& buffer) {[m
 [m
     connection_ = std::move(info.connection_);[m
     connection_->addConnectionCallbacks(*this);[m
[31m-    connection_->setConnectionStats({parent_.cluster_info_->stats().upstream_cx_rx_bytes_total_,[m
[31m-                                     parent_.cluster_info_->stats().upstream_cx_rx_bytes_buffered_,[m
[31m-                                     parent_.cluster_info_->stats().upstream_cx_tx_bytes_total_,[m
[31m-                                     parent_.cluster_info_->stats().upstream_cx_tx_bytes_buffered_,[m
[31m-                                     &parent_.cluster_info_->stats().bind_errors_, nullptr});[m
[32m+[m[32m    connection_->setConnectionStats([m
[32m+[m[32m        {parent_.cluster_info_->trafficStats().upstream_cx_rx_bytes_total_,[m
[32m+[m[32m         parent_.cluster_info_->trafficStats().upstream_cx_rx_bytes_buffered_,[m
[32m+[m[32m         parent_.cluster_info_->trafficStats().upstream_cx_tx_bytes_total_,[m
[32m+[m[32m         parent_.cluster_info_->trafficStats().upstream_cx_tx_bytes_buffered_,[m
[32m+[m[32m         &parent_.cluster_info_->trafficStats().bind_errors_, nullptr});[m
     connection_->connect();[m
   }[m
 [m
[1mdiff --git a/source/extensions/stat_sinks/hystrix/hystrix.cc b/source/extensions/stat_sinks/hystrix/hystrix.cc[m
[1mindex c912c42cf2..dac8610db6 100644[m
[1m--- a/source/extensions/stat_sinks/hystrix/hystrix.cc[m
[1m+++ b/source/extensions/stat_sinks/hystrix/hystrix.cc[m
[36m@@ -90,7 +90,7 @@[m [muint64_t HystrixSink::getRollingValue(RollingWindow rolling_window) {[m
 [m
 void HystrixSink::updateRollingWindowMap(const Upstream::ClusterInfo& cluster_info,[m
                                          ClusterStatsCache& cluster_stats_cache) {[m
[31m-  Upstream::ClusterStats& cluster_stats = cluster_info.stats();[m
[32m+[m[32m  Upstream::ClusterTrafficStats& cluster_stats = cluster_info.trafficStats();[m
   Stats::Scope& cluster_stats_scope = cluster_info.statsScope();[m
 [m
   // Combining timeouts+retries - retries are counted  as separate requests[m
[1mdiff --git a/source/server/filter_chain_manager_impl.cc b/source/server/filter_chain_manager_impl.cc[m
[1mindex b92ebf490b..8624ae3911 100644[m
[1m--- a/source/server/filter_chain_manager_impl.cc[m
[1m+++ b/source/server/filter_chain_manager_impl.cc[m
[36m@@ -31,30 +31,29 @@[m [mNetwork::Address::InstanceConstSharedPtr fakeAddress() {[m
                          Network::Utility::parseInternetAddress("255.255.255.255"));[m
 }[m
 [m
[31m-struct FilterChainNameAction : public Matcher::ActionBase<ProtobufWkt::StringValue> {[m
[31m-  explicit FilterChainNameAction(Network::DrainableFilterChainSharedPtr chain) : chain_(chain) {}[m
[31m-  const Network::DrainableFilterChainSharedPtr chain_;[m
[32m+[m[32mstruct FilterChainNameAction[m
[32m+[m[32m    : public Matcher::ActionBase<ProtobufWkt::StringValue, FilterChainBaseAction> {[m
[32m+[m[32m  explicit FilterChainNameAction(const std::string& name) : name_(name) {}[m
[32m+[m[32m  const Network::FilterChain* get(const FilterChainsByName& filter_chains_by_name,[m
[32m+[m[32m                                  const StreamInfo::StreamInfo&) const override {[m
[32m+[m[32m    const auto chain_match = filter_chains_by_name.find(name_);[m
[32m+[m[32m    if (chain_match != filter_chains_by_name.end()) {[m
[32m+[m[32m      return chain_match->second.get();[m
[32m+[m[32m    }[m
[32m+[m[32m    return nullptr;[m
[32m+[m[32m  }[m
[32m+[m[32m  const std::string name_;[m
 };[m
 [m
[31m-using FilterChainActionFactoryContext =[m
[31m-    absl::flat_hash_map<std::string, Network::DrainableFilterChainSharedPtr>;[m
[31m-[m
 class FilterChainNameActionFactory : public Matcher::ActionFactory<FilterChainActionFactoryContext>,[m
                                      Logger::Loggable<Logger::Id::config> {[m
 public:[m
   std::string name() const override { return "filter-chain-name"; }[m
   Matcher::ActionFactoryCb createActionFactoryCb(const Protobuf::Message& config,[m
[31m-                                                 FilterChainActionFactoryContext& filter_chains,[m
[32m+[m[32m                                                 FilterChainActionFactoryContext&,[m
                                                  ProtobufMessage::ValidationVisitor&) override {[m
[31m-    Network::DrainableFilterChainSharedPtr chain = nullptr;[m
     const auto& name = dynamic_cast<const ProtobufWkt::StringValue&>(config);[m
[31m-    const auto chain_match = filter_chains.find(name.value());[m
[31m-    if (chain_match != filter_chains.end()) {[m
[31m-      chain = chain_match->second;[m
[31m-    } else {[m
[31m-      ENVOY_LOG(debug, "matcher API points to an absent filter chain '{}'", name.value());[m
[31m-    }[m
[31m-    return [chain]() { return std::make_unique<FilterChainNameAction>(chain); };[m
[32m+[m[32m    return [value = name.value()]() { return std::make_unique<FilterChainNameAction>(value); };[m
   }[m
   ProtobufTypes::MessagePtr createEmptyConfigProto() override {[m
     return std::make_unique<ProtobufWkt::StringValue>();[m
[36m@@ -216,7 +215,7 @@[m [mvoid FilterChainManagerImpl::addFilterChains([m
                       MessageUtil>[m
       filter_chains;[m
   uint32_t new_filter_chain_size = 0;[m
[31m-  absl::flat_hash_map<std::string, Network::DrainableFilterChainSharedPtr> filter_chains_by_name;[m
[32m+[m[32m  FilterChainsByName filter_chains_by_name;[m
 [m
   for (const auto& filter_chain : filter_chain_span) {[m
     const auto& filter_chain_match = filter_chain->filter_chain_match();[m
[36m@@ -312,9 +311,11 @@[m [mvoid FilterChainManagerImpl::addFilterChains([m
                                   context_creator);[m
   // Construct matcher if it is present in the listener configuration.[m
   if (filter_chain_matcher) {[m
[32m+[m[32m    filter_chains_by_name_ = filter_chains_by_name;[m
     FilterChainNameActionValidationVisitor validation_visitor;[m
     Matcher::MatchTreeFactory<Network::MatchingData, FilterChainActionFactoryContext> factory([m
[31m-        filter_chains_by_name, parent_context_.getServerFactoryContext(), validation_visitor);[m
[32m+[m[32m        parent_context_.getServerFactoryContext(), parent_context_.getServerFactoryContext(),[m
[32m+[m[32m        validation_visitor);[m
     matcher_ = factory.create(*filter_chain_matcher)();[m
   }[m
   ENVOY_LOG(debug, "new fc_contexts has {} filter chains, including {} newly built",[m
[36m@@ -583,14 +584,14 @@[m [mFilterChainManagerImpl::findFilterChain(const Network::ConnectionSocket& socket,[m
 [m
 const Network::FilterChain*[m
 FilterChainManagerImpl::findFilterChainUsingMatcher(const Network::ConnectionSocket& socket,[m
[31m-                                                    const StreamInfo::StreamInfo&) const {[m
[32m+[m[32m                                                    const StreamInfo::StreamInfo& info) const {[m
   Network::Matching::MatchingDataImpl data(socket);[m
   const auto& match_result = Matcher::evaluateMatch<Network::MatchingData>(*matcher_, data);[m
   ASSERT(match_result.match_state_ == Matcher::MatchState::MatchComplete,[m
          "Matching must complete for network streams.");[m
   if (match_result.result_) {[m
     const auto result = match_result.result_();[m
[31m-    return result->getTyped<FilterChainNameAction>().chain_.get();[m
[32m+[m[32m    return result->getTyped<FilterChainBaseAction>().get(filter_chains_by_name_, info);[m
   }[m
   return default_filter_chain_.get();[m
 }[m
[1mdiff --git a/source/server/filter_chain_manager_impl.h b/source/server/filter_chain_manager_impl.h[m
[1mindex 8814d091f9..4ae38de72a 100644[m
[1m--- a/source/server/filter_chain_manager_impl.h[m
[1m+++ b/source/server/filter_chain_manager_impl.h[m
[36m@@ -100,6 +100,15 @@[m [mprivate:[m
   std::atomic<bool> is_draining_{false};[m
 };[m
 [m
[32m+[m[32musing FilterChainActionFactoryContext = Configuration::ServerFactoryContext;[m
[32m+[m[32musing FilterChainsByName = absl::flat_hash_map<std::string, Network::DrainableFilterChainSharedPtr>;[m
[32m+[m
[32m+[m[32mclass FilterChainBaseAction : public Matcher::Action {[m
[32m+[m[32mpublic:[m
[32m+[m[32m  virtual const Network::FilterChain* get(const FilterChainsByName& filter_chains_by_name,[m
[32m+[m[32m                                          const StreamInfo::StreamInfo& info) const PURE;[m
[32m+[m[32m};[m
[32m+[m
 class FilterChainImpl : public Network::DrainableFilterChain {[m
 public:[m
   FilterChainImpl(Network::DownstreamTransportSocketFactoryPtr&& transport_socket_factory,[m
[36m@@ -395,6 +404,9 @@[m [mprivate:[m
 [m
   // Matcher selecting the filter chain name.[m
   Matcher::MatchTreePtr<Network::MatchingData> matcher_;[m
[32m+[m
[32m+[m[32m  // Index filter chains by name, used by the matcher actions.[m
[32m+[m[32m  FilterChainsByName filter_chains_by_name_;[m
 };[m
 } // namespace Server[m
 } // namespace Envoy[m
[1mdiff --git a/test/common/common/BUILD b/test/common/common/BUILD[m
[1mindex 9a9d483594..5655a09485 100644[m
[1m--- a/test/common/common/BUILD[m
[1m+++ b/test/common/common/BUILD[m
[36m@@ -231,6 +231,7 @@[m [menvoy_cc_test([m
         "//source/common/common:matchers_lib",[m
         "//source/common/config:metadata_lib",[m
         "//source/common/protobuf:utility_lib",[m
[32m+[m[32m        "//source/common/stream_info:filter_state_lib",[m
         "//test/test_common:utility_lib",[m
         "@envoy_api//envoy/config/core/v3:pkg_cc_proto",[m
         "@envoy_api//envoy/type/matcher/v3:pkg_cc_proto",[m
[1mdiff --git a/test/common/common/matchers_test.cc b/test/common/common/matchers_test.cc[m
[1mindex 581967af8b..0c152a5982 100644[m
[1m--- a/test/common/common/matchers_test.cc[m
[1m+++ b/test/common/common/matchers_test.cc[m
[36m@@ -7,6 +7,7 @@[m
 #include "source/common/common/matchers.h"[m
 #include "source/common/config/metadata.h"[m
 #include "source/common/protobuf/protobuf.h"[m
[32m+[m[32m#include "source/common/stream_info/filter_state_impl.h"[m
 [m
 #include "test/test_common/utility.h"[m
 [m
[36m@@ -475,6 +476,59 @@[m [mTEST(PathMatcher, MatchRegexPath) {[m
   EXPECT_FALSE(Matchers::PathMatcher(matcher).match("/regez#regex"));[m
 }[m
 [m
[32m+[m[32mTEST(FilterStateMatcher, MatchAbsentFilterState) {[m
[32m+[m[32m  envoy::type::matcher::v3::FilterStateMatcher matcher;[m
[32m+[m[32m  matcher.set_key("test.key");[m
[32m+[m[32m  matcher.mutable_string_match()->set_exact("exact");[m
[32m+[m[32m  StreamInfo::FilterStateImpl filter_state(StreamInfo::FilterState::LifeSpan::Connection);[m
[32m+[m[32m  EXPECT_FALSE(Matchers::FilterStateMatcher(matcher).match(filter_state));[m
[32m+[m[32m}[m
[32m+[m
[32m+[m[32mclass TestObject : public StreamInfo::FilterState::Object {[m
[32m+[m[32mpublic:[m
[32m+[m[32m  TestObject(absl::optional<std::string> value) : value_(value) {}[m
[32m+[m[32m  absl::optional<std::string> serializeAsString() const override { return value_; }[m
[32m+[m
[32m+[m[32mprivate:[m
[32m+[m[32m  absl::optional<std::string> value_;[m
[32m+[m[32m};[m
[32m+[m
[32m+[m[32mTEST(FilterStateMatcher, MatchFilterStateWithoutString) {[m
[32m+[m[32m  const std::string key = "test.key";[m
[32m+[m[32m  envoy::type::matcher::v3::FilterStateMatcher matcher;[m
[32m+[m[32m  matcher.set_key(key);[m
[32m+[m[32m  matcher.mutable_string_match()->set_exact("exact");[m
[32m+[m[32m  StreamInfo::FilterStateImpl filter_state(StreamInfo::FilterState::LifeSpan::Connection);[m
[32m+[m[32m  filter_state.setData(key, std::make_shared<TestObject>(absl::nullopt),[m
[32m+[m[32m                       StreamInfo::FilterState::StateType::ReadOnly);[m
[32m+[m[32m  EXPECT_FALSE(Matchers::FilterStateMatcher(matcher).match(filter_state));[m
[32m+[m[32m}[m
[32m+[m
[32m+[m[32mTEST(FilterStateMatcher, MatchFilterStateDifferentString) {[m
[32m+[m[32m  const std::string key = "test.key";[m
[32m+[m[32m  const std::string value = "exact_value";[m
[32m+[m[32m  envoy::type::matcher::v3::FilterStateMatcher matcher;[m
[32m+[m[32m  matcher.set_key(key);[m
[32m+[m[32m  matcher.mutable_string_match()->set_exact(value);[m
[32m+[m[32m  StreamInfo::FilterStateImpl filter_state(StreamInfo::FilterState::LifeSpan::Connection);[m
[32m+[m[32m  filter_state.setData(key,[m
[32m+[m[32m                       std::make_shared<TestObject>(absl::make_optional<std::string>("different")),[m
[32m+[m[32m                       StreamInfo::FilterState::StateType::ReadOnly);[m
[32m+[m[32m  EXPECT_FALSE(Matchers::FilterStateMatcher(matcher).match(filter_state));[m
[32m+[m[32m}[m
[32m+[m
[32m+[m[32mTEST(FilterStateMatcher, MatchFilterState) {[m
[32m+[m[32m  const std::string key = "test.key";[m
[32m+[m[32m  const std::string value = "exact_value";[m
[32m+[m[32m  envoy::type::matcher::v3::FilterStateMatcher matcher;[m
[32m+[m[32m  matcher.set_key(key);[m
[32m+[m[32m  matcher.mutable_string_match()->set_exact(value);[m
[32m+[m[32m  StreamInfo::FilterStateImpl filter_state(StreamInfo::FilterState::LifeSpan::Connection);[m
[32m+[m[32m  filter_state.setData(key, std::make_shared<TestObject>(absl::make_optional<std::string>(value)),[m
[32m+[m[32m                       StreamInfo::FilterState::StateType::ReadOnly);[m
[32m+[m[32m  EXPECT_TRUE(Matchers::FilterStateMatcher(matcher).match(filter_state));[m
[32m+[m[32m}[m
[32m+[m
 } // namespace[m
 } // namespace Matcher[m
 } // namespace Envoy[m
[1mdiff --git a/test/common/conn_pool/conn_pool_base_test.cc b/test/common/conn_pool/conn_pool_base_test.cc[m
[1mindex 3554583027..fb53487b21 100644[m
[1m--- a/test/common/conn_pool/conn_pool_base_test.cc[m
[1m+++ b/test/common/conn_pool/conn_pool_base_test.cc[m
[36m@@ -374,13 +374,13 @@[m [mTEST_F(ConnPoolImplDispatcherBaseTest, MaxConnectionDurationBusy) {[m
   // Verify that advancing to just before the connection duration timeout doesn't drain the[m
   // connection.[m
   advanceTimeAndRun(max_connection_duration_ - 1);[m
[31m-  EXPECT_EQ(0, pool_.host()->cluster().stats().upstream_cx_max_duration_reached_.value());[m
[32m+[m[32m  EXPECT_EQ(0, pool_.host()->cluster().trafficStats().upstream_cx_max_duration_reached_.value());[m
   EXPECT_EQ(ActiveClient::State::Busy, clients_.back()->state());[m
 [m
   // Verify that advancing past the connection duration timeout drains the connection,[m
   // because there's a busy client.[m
   advanceTimeAndRun(2);[m
[31m-  EXPECT_EQ(1, pool_.host()->cluster().stats().upstream_cx_max_duration_reached_.value());[m
[32m+[m[32m  EXPECT_EQ(1, pool_.host()->cluster().trafficStats().upstream_cx_max_duration_reached_.value());[m
   EXPECT_EQ(ActiveClient::State::Draining, clients_.back()->state());[m
   closeStream();[m
 }[m
[36m@@ -395,13 +395,13 @@[m [mTEST_F(ConnPoolImplDispatcherBaseTest, MaxConnectionDurationReady) {[m
   // Verify that advancing to just before the connection duration timeout doesn't close the[m
   // connection.[m
   advanceTimeAndRun(max_connection_duration_ - 1);[m
[31m-  EXPECT_EQ(0, pool_.host()->cluster().stats().upstream_cx_max_duration_reached_.value());[m
[32m+[m[32m  EXPECT_EQ(0, pool_.host()->cluster().trafficStats().upstream_cx_max_duration_reached_.value());[m
   EXPECT_EQ(ActiveClient::State::Ready, clients_.back()->state());[m
 [m
   // Verify that advancing past the connection duration timeout closes the connection,[m
   // because there's nothing to drain.[m
   advanceTimeAndRun(2);[m
[31m-  EXPECT_EQ(1, pool_.host()->cluster().stats().upstream_cx_max_duration_reached_.value());[m
[32m+[m[32m  EXPECT_EQ(1, pool_.host()->cluster().trafficStats().upstream_cx_max_duration_reached_.value());[m
 }[m
 [m
 TEST_F(ConnPoolImplDispatcherBaseTest, MaxConnectionDurationAlreadyDraining) {[m
[36m@@ -411,7 +411,7 @@[m [mTEST_F(ConnPoolImplDispatcherBaseTest, MaxConnectionDurationAlreadyDraining) {[m
   // Verify that advancing past the connection duration timeout does nothing to an active client[m
   // that is already draining.[m
   advanceTimeAndRun(max_connection_duration_ + 1);[m
[31m-  EXPECT_EQ(0, pool_.host()->cluster().stats().upstream_cx_max_duration_reached_.value());[m
[32m+[m[32m  EXPECT_EQ(0, pool_.host()->cluster().trafficStats().upstream_cx_max_duration_reached_.value());[m
   EXPECT_EQ(ActiveClient::State::Draining, clients_.back()->state());[m
   closeStream();[m
 }[m
[36m@@ -423,7 +423,7 @@[m [mTEST_F(ConnPoolImplDispatcherBaseTest, MaxConnectionDurationAlreadyClosed) {[m
   // Verify that advancing past the connection duration timeout does nothing to the active[m
   // client that is already closed.[m
   advanceTimeAndRun(max_connection_duration_ + 1);[m
[31m-  EXPECT_EQ(0, pool_.host()->cluster().stats().upstream_cx_max_duration_reached_.value());[m
[32m+[m[32m  EXPECT_EQ(0, pool_.host()->cluster().trafficStats().upstream_cx_max_duration_reached_.value());[m
 }[m
 [m
 TEST_F(ConnPoolImplDispatcherBaseTest, MaxConnectionDurationCallbackWhileClosedBug) {[m
[36m@@ -565,7 +565,7 @@[m [mTEST_F(ConnPoolImplDispatcherBaseTest, ConnectedZeroRttSendsEarlyData) {[m
   pool_.onUpstreamReadyForEarlyData(client_ref);[m
 [m
   CHECK_STATE(1 /*active*/, 0 /*pending*/, concurrent_streams_ - 1 /*connecting capacity*/);[m
[31m-  EXPECT_EQ(1, pool_.host()->cluster().stats().upstream_rq_0rtt_.value());[m
[32m+[m[32m  EXPECT_EQ(1, pool_.host()->cluster().trafficStats().upstream_rq_0rtt_.value());[m
 [m
   EXPECT_NE(nullptr, pool_.newStreamImpl(context_, /*can_send_early_data=*/false));[m
   CHECK_STATE(1 /*active*/, 1 /*pending*/, concurrent_streams_ - 1 /*connecting capacity*/);[m
[36m@@ -574,7 +574,7 @@[m [mTEST_F(ConnPoolImplDispatcherBaseTest, ConnectedZeroRttSendsEarlyData) {[m
   clients_.back()->onEvent(Network::ConnectionEvent::Connected);[m
 [m
   CHECK_STATE(2 /*active*/, 0 /*pending*/, 0 /*connecting capacity*/);[m
[31m-  EXPECT_EQ(1, pool_.host()->cluster().stats().upstream_rq_0rtt_.value());[m
[32m+[m[32m  EXPECT_EQ(1, pool_.host()->cluster().trafficStats().upstream_rq_0rtt_.value());[m
 [m
   // Clean up.[m
   closeStreamAndDrainClient();[m
[36m@@ -596,12 +596,12 @@[m [mTEST_F(ConnPoolImplDispatcherBaseTest, EarlyDataStreamsReachConcurrentStreamLimi[m
   pool_.onUpstreamReadyForEarlyData(client_ref);[m
 [m
   CHECK_STATE(1 /*active*/, 0 /*pending*/, concurrent_streams_ - 1 /*connecting capacity*/);[m
[31m-  EXPECT_EQ(1, pool_.host()->cluster().stats().upstream_rq_0rtt_.value());[m
[32m+[m[32m  EXPECT_EQ(1, pool_.host()->cluster().trafficStats().upstream_rq_0rtt_.value());[m
 [m
   EXPECT_CALL(pool_, onPoolReady);[m
   EXPECT_EQ(nullptr, pool_.newStreamImpl(context_, /*can_send_early_data=*/true));[m
   CHECK_STATE(2 /*active*/, 0 /*pending*/, concurrent_streams_ - 2 /*connecting capacity*/);[m
[31m-  EXPECT_EQ(2, pool_.host()->cluster().stats().upstream_rq_0rtt_.value());[m
[32m+[m[32m  EXPECT_EQ(2, pool_.host()->cluster().trafficStats().upstream_rq_0rtt_.value());[m
   EXPECT_EQ(ActiveClient::State::Busy, clients_.back()->state());[m
 [m
   // After 1 stream gets closed, the client should transit to ReadyForEarlyData.[m
[1mdiff --git a/test/common/http/http1/codec_impl_test.cc b/test/common/http/http1/codec_impl_test.cc[m
[1mindex 657dcc53c9..ac477115f1 100644[m
[1m--- a/test/common/http/http1/codec_impl_test.cc[m
[1m+++ b/test/common/http/http1/codec_impl_test.cc[m
[36m@@ -904,10 +904,6 @@[m [mTEST_P(Http1ServerConnectionImplTest, Http10MultipleResponses) {[m
 [m
   // Now send an HTTP/1.1 request and make sure the protocol is tracked correctly.[m
   {[m
[31m-    TestRequestHeaderMapImpl expected_headers{{":authority", "www.somewhere.com"},[m
[31m-                                              {":scheme", "http"},[m
[31m-                                              {":path", "/foobar"},[m
[31m-                                              {":method", "GET"}};[m
     Buffer::OwnedImpl buffer("GET /foobar HTTP/1.1\r\nHost: www.somewhere.com\r\n\r\n");[m
 [m
     Http::ResponseEncoder* response_encoder = nullptr;[m
[36m@@ -1141,6 +1137,7 @@[m [mTEST_P(Http1ServerConnectionImplTest, SimpleGet) {[m
   auto status = codec_->dispatch(buffer);[m
   EXPECT_TRUE(status.ok());[m
   EXPECT_EQ(0U, buffer.length());[m
[32m+[m[32m  EXPECT_EQ(Protocol::Http11, codec_->protocol());[m
 }[m
 [m
 // Test that if the stream is not created at the time an error is detected, it[m
[36m@@ -1550,6 +1547,7 @@[m [mTEST_P(Http1ServerConnectionImplTest, HeaderOnlyResponse) {[m
   TestResponseHeaderMapImpl headers{{":status", "200"}};[m
   response_encoder->encodeHeaders(headers, true);[m
   EXPECT_EQ("HTTP/1.1 200 OK\r\ncontent-length: 0\r\n\r\n", output);[m
[32m+[m[32m  EXPECT_EQ(Protocol::Http11, codec_->protocol());[m
 }[m
 [m
 // As with Http1ClientConnectionImplTest.LargeHeaderRequestEncode but validate[m
[36m@@ -3766,5 +3764,25 @@[m [mTEST_P(Http1ServerConnectionImplTest, ParseUrl) {[m
   }[m
 }[m
 [m
[32m+[m[32m// The client's HTTP parser does not have access to the request.[m
[32m+[m[32m// Test that it determines the HTTP version based on the response correctly.[m
[32m+[m[32mTEST_P(Http1ClientConnectionImplTest, ResponseHttpVersion) {[m
[32m+[m[32m  for (Protocol http_version : {Protocol::Http10, Protocol::Http11}) {[m
[32m+[m[32m    initialize();[m
[32m+[m[32m    NiceMock<MockResponseDecoder> response_decoder;[m
[32m+[m[32m    Http::RequestEncoder& request_encoder = codec_->newStream(response_decoder);[m
[32m+[m[32m    TestRequestHeaderMapImpl headers{{":method", "GET"}, {":path", "/"}, {":authority", "host"}};[m
[32m+[m[32m    EXPECT_TRUE(request_encoder.encodeHeaders(headers, true).ok());[m
[32m+[m
[32m+[m[32m    EXPECT_CALL(response_decoder, decodeHeaders_(_, true));[m
[32m+[m[32m    Buffer::OwnedImpl response(http_version == Protocol::Http10[m
[32m+[m[32m                                   ? "HTTP/1.0 200 OK\r\nContent-Length: 0\r\n\r\n"[m
[32m+[m[32m                                   : "HTTP/1.1 200 OK\r\nContent-Length: 0\r\n\r\n");[m
[32m+[m[32m    auto status = codec_->dispatch(response);[m
[32m+[m[32m    EXPECT_TRUE(status.ok());[m
[32m+[m[32m    EXPECT_EQ(http_version, codec_->protocol());[m
[32m+[m[32m  }[m
[32m+[m[32m}[m
[32m+[m
 } // namespace Http[m
 } // namespace Envoy[m
[1mdiff --git a/test/common/http/utility_test.cc b/test/common/http/utility_test.cc[m
[1mindex 1794081394..c86d46af9e 100644[m
[1m--- a/test/common/http/utility_test.cc[m
[1m+++ b/test/common/http/utility_test.cc[m
[36m@@ -27,6 +27,28 @@[m [musing testing::Return;[m
 [m
 namespace Envoy {[m
 namespace Http {[m
[32m+[m[32mnamespace {[m
[32m+[m
[32m+[m[32mvoid sendLocalReplyTestHelper(const bool& is_reset, StreamDecoderFilterCallbacks& callbacks,[m
[32m+[m[32m                              const Utility::LocalReplyData& local_reply_data) {[m
[32m+[m[32m  absl::string_view details;[m
[32m+[m[32m  if (callbacks.streamInfo().responseCodeDetails().has_value()) {[m
[32m+[m[32m    details = callbacks.streamInfo().responseCodeDetails().value();[m
[32m+[m[32m  };[m
[32m+[m
[32m+[m[32m  Utility::sendLocalReply([m
[32m+[m[32m      is_reset,[m
[32m+[m[32m      Utility::EncodeFunctions{nullptr, nullptr,[m
[32m+[m[32m                               [&](ResponseHeaderMapPtr&& headers, bool end_stream) -> void {[m
[32m+[m[32m                                 callbacks.encodeHeaders(std::move(headers), end_stream, details);[m
[32m+[m[32m                               },[m
[32m+[m[32m                               [&](Buffer::Instance& data, bool end_stream) -> void {[m
[32m+[m[32m                                 callbacks.encodeData(data, end_stream);[m
[32m+[m[32m                               }},[m
[32m+[m[32m      local_reply_data);[m
[32m+[m[32m}[m
[32m+[m
[32m+[m[32m} // namespace[m
 [m
 TEST(HttpUtility, parseQueryString) {[m
   EXPECT_EQ(Utility::QueryParams(), Utility::parseQueryString("/hello"));[m
[36m@@ -843,7 +865,7 @@[m [mTEST(HttpUtility, SendLocalReply) {[m
   EXPECT_CALL(callbacks, encodeHeaders_(_, false));[m
   EXPECT_CALL(callbacks, encodeData(_, true));[m
   EXPECT_CALL(callbacks, streamInfo());[m
[31m-  Utility::sendLocalReply([m
[32m+[m[32m  sendLocalReplyTestHelper([m
       is_reset, callbacks,[m
       Utility::LocalReplyData{false, Http::Code::PayloadTooLarge, "large", absl::nullopt, false});[m
 }[m
[36m@@ -862,7 +884,7 @@[m [mTEST(HttpUtility, SendLocalGrpcReply) {[m
         EXPECT_NE(headers.GrpcMessage(), nullptr);[m
         EXPECT_EQ(headers.getGrpcMessageValue(), "large");[m
       }));[m
[31m-  Utility::sendLocalReply([m
[32m+[m[32m  sendLocalReplyTestHelper([m
       is_reset, callbacks,[m
       Utility::LocalReplyData{true, Http::Code::PayloadTooLarge, "large", absl::nullopt, false});[m
 }[m
[36m@@ -881,7 +903,7 @@[m [mTEST(HttpUtility, SendLocalGrpcReplyGrpcStatusAlreadyExists) {[m
         EXPECT_NE(headers.GrpcMessage(), nullptr);[m
         EXPECT_EQ(headers.getGrpcMessageValue(), "large");[m
       }));[m
[31m-  Utility::sendLocalReply([m
[32m+[m[32m  sendLocalReplyTestHelper([m
       is_reset, callbacks,[m
       Utility::LocalReplyData{true, Http::Code::PayloadTooLarge, "large",[m
                               Grpc::Status::WellKnownGrpcStatus::InvalidArgument, false});[m
[36m@@ -940,7 +962,7 @@[m [mTEST(HttpUtility, SendLocalGrpcReplyWithUpstreamJsonPayload) {[m
         const auto& encoded = Utility::PercentEncoding::encode(json);[m
         EXPECT_EQ(headers.getGrpcMessageValue(), encoded);[m
       }));[m
[31m-  Utility::sendLocalReply([m
[32m+[m[32m  sendLocalReplyTestHelper([m
       is_reset, callbacks,[m
       Utility::LocalReplyData{true, Http::Code::Unauthorized, json, absl::nullopt, false});[m
 }[m
[36m@@ -955,7 +977,7 @@[m [mTEST(HttpUtility, RateLimitedGrpcStatus) {[m
         EXPECT_EQ(headers.getGrpcStatusValue(),[m
                   std::to_string(enumToInt(Grpc::Status::WellKnownGrpcStatus::Unavailable)));[m
       }));[m
[31m-  Utility::sendLocalReply([m
[32m+[m[32m  sendLocalReplyTestHelper([m
       false, callbacks,[m
       Utility::LocalReplyData{true, Http::Code::TooManyRequests, "", absl::nullopt, false});[m
 [m
[36m@@ -965,7 +987,7 @@[m [mTEST(HttpUtility, RateLimitedGrpcStatus) {[m
         EXPECT_EQ(headers.getGrpcStatusValue(),[m
                   std::to_string(enumToInt(Grpc::Status::WellKnownGrpcStatus::ResourceExhausted)));[m
       }));[m
[31m-  Utility::sendLocalReply([m
[32m+[m[32m  sendLocalReplyTestHelper([m
       false, callbacks,[m
       Utility::LocalReplyData{true, Http::Code::TooManyRequests, "",[m
                               absl::make_optional<Grpc::Status::GrpcStatus>([m
[36m@@ -982,7 +1004,7 @@[m [mTEST(HttpUtility, SendLocalReplyDestroyedEarly) {[m
     is_reset = true;[m
   }));[m
   EXPECT_CALL(callbacks, encodeData(_, true)).Times(0);[m
[31m-  Utility::sendLocalReply([m
[32m+[m[32m  sendLocalReplyTestHelper([m
       is_reset, callbacks,[m
       Utility::LocalReplyData{false, Http::Code::PayloadTooLarge, "large", absl::nullopt, false});[m
 }[m
[36m@@ -995,7 +1017,7 @@[m [mTEST(HttpUtility, SendLocalReplyHeadRequest) {[m
       .WillOnce(Invoke([&](const ResponseHeaderMap& headers, bool) -> void {[m
         EXPECT_EQ(headers.getContentLengthValue(), fmt::format("{}", strlen("large")));[m
       }));[m
[31m-  Utility::sendLocalReply([m
[32m+[m[32m  sendLocalReplyTestHelper([m
       is_reset, callbacks,[m
       Utility::LocalReplyData{false, Http::Code::PayloadTooLarge, "large", absl::nullopt, true});[m
 }[m
[1mdiff --git a/test/common/matcher/test_utility.h b/test/common/matcher/test_utility.h[m
[1mindex 66d1ebcea6..b9e9482780 100644[m
[1m--- a/test/common/matcher/test_utility.h[m
[1m+++ b/test/common/matcher/test_utility.h[m
[36m@@ -243,7 +243,7 @@[m [mvoid verifyImmediateMatch(const MatchTree<TestData>::MatchResult& result,[m
   EXPECT_EQ(nullptr, result.on_match_->matcher_);[m
   EXPECT_NE(result.on_match_->action_cb_, nullptr);[m
 [m
[31m-  EXPECT_EQ(*static_cast<StringAction*>(result.on_match_->action_cb_().get()),[m
[32m+[m[32m  EXPECT_EQ(result.on_match_->action_cb_().get()->getTyped<StringAction>(),[m
             *stringValue(expected_value));[m
 }[m
 [m
[1mdiff --git a/test/common/network/connection_impl_test.cc b/test/common/network/connection_impl_test.cc[m
[1mindex 5735ec57d0..21557b902c 100644[m
[1m--- a/test/common/network/connection_impl_test.cc[m
[1m+++ b/test/common/network/connection_impl_test.cc[m
[36m@@ -467,7 +467,7 @@[m [mTEST_P(ConnectionImplTest, SetServerTransportSocketTimeout) {[m
   auto server_connection = std::make_unique<Network::ServerConnectionImpl>([m
       *mocks.dispatcher_,[m
       std::make_unique<ConnectionSocketImpl>(std::move(io_handle), nullptr, nullptr),[m
[31m-      std::move(mocks.transport_socket_), stream_info_, true);[m
[32m+[m[32m      std::move(mocks.transport_socket_), stream_info_);[m
 [m
   EXPECT_CALL(*mock_timer, enableTimer(std::chrono::milliseconds(3 * 1000), _));[m
   Stats::MockCounter timeout_counter;[m
[36m@@ -488,7 +488,7 @@[m [mTEST_P(ConnectionImplTest, SetServerTransportSocketTimeoutAfterConnect) {[m
   auto server_connection = std::make_unique<Network::ServerConnectionImpl>([m
       *mocks.dispatcher_,[m
       std::make_unique<ConnectionSocketImpl>(std::move(io_handle), nullptr, nullptr),[m
[31m-      std::move(mocks.transport_socket_), stream_info_, true);[m
[32m+[m[32m      std::move(mocks.transport_socket_), stream_info_);[m
 [m
   transport_socket->callbacks_->raiseEvent(ConnectionEvent::Connected);[m
   // This should be a no-op. No timer should be created.[m
[36m@@ -512,7 +512,7 @@[m [mTEST_P(ConnectionImplTest, ServerTransportSocketTimeoutDisabledOnConnect) {[m
   auto server_connection = std::make_unique<Network::ServerConnectionImpl>([m
       *mocks.dispatcher_,[m
       std::make_unique<ConnectionSocketImpl>(std::move(io_handle), nullptr, nullptr),[m
[31m-      std::move(mocks.transport_socket_), stream_info_, true);[m
[32m+[m[32m      std::move(mocks.transport_socket_), stream_info_);[m
 [m
   bool timer_destroyed = false;[m
   mock_timer->timer_destroyed_ = &timer_destroyed;[m
[36m@@ -2009,7 +2009,7 @@[m [mTEST_P(ConnectionImplTest, NetworkConnectionDumpsWithoutAllocatingMemory) {[m
   auto server_connection = std::make_unique<Network::ServerConnectionImpl>([m
       *mocks.dispatcher_,[m
       std::make_unique<ConnectionSocketImpl>(std::move(io_handle), nullptr, nullptr),[m
[31m-      std::move(mocks.transport_socket_), stream_info_, true);[m
[32m+[m[32m      std::move(mocks.transport_socket_), stream_info_);[m
 [m
   // Start measuring memory and dump state.[m
   Stats::TestUtil::MemoryTest memory_test;[m
[1mdiff --git a/test/common/router/retry_state_impl_test.cc b/test/common/router/retry_state_impl_test.cc[m
[1mindex 65d47c7245..6abd42db62 100644[m
[1m--- a/test/common/router/retry_state_impl_test.cc[m
[1m+++ b/test/common/router/retry_state_impl_test.cc[m
[36m@@ -157,9 +157,9 @@[m [mpublic:[m
     EXPECT_EQ(RetryStatus::NoRetryLimitExceeded,[m
               state_->shouldRetryHeaders(response_headers, request_headers, header_callback_));[m
 [m
[31m-    EXPECT_EQ(1UL, cluster_.stats().upstream_rq_retry_limit_exceeded_.value());[m
[32m+[m[32m    EXPECT_EQ(1UL, cluster_.trafficStats().upstream_rq_retry_limit_exceeded_.value());[m
     EXPECT_EQ(1UL, virtual_cluster_.stats().upstream_rq_retry_limit_exceeded_.value());[m
[31m-    EXPECT_EQ(1UL, cluster_.stats().upstream_rq_retry_.value());[m
[32m+[m[32m    EXPECT_EQ(1UL, cluster_.trafficStats().upstream_rq_retry_.value());[m
     EXPECT_EQ(1UL, virtual_cluster_.stats().upstream_rq_retry_.value());[m
   }[m
 [m
[36m@@ -217,10 +217,10 @@[m [mTEST_F(RouterRetryStateImplTest, PolicyRefusedStream) {[m
             state_->shouldRetryReset(remote_refused_stream_reset_, RetryState::Http3Used::No,[m
                                      reset_callback_));[m
 [m
[31m-  EXPECT_EQ(1UL, cluster_.stats().upstream_rq_retry_limit_exceeded_.value());[m
[32m+[m[32m  EXPECT_EQ(1UL, cluster_.trafficStats().upstream_rq_retry_limit_exceeded_.value());[m
   EXPECT_EQ(1UL, virtual_cluster_.stats().upstream_rq_retry_limit_exceeded_.value());[m
   EXPECT_EQ(1UL, route_stats_context_.stats().upstream_rq_retry_limit_exceeded_.value());[m
[31m-  EXPECT_EQ(1UL, cluster_.stats().upstream_rq_retry_.value());[m
[32m+[m[32m  EXPECT_EQ(1UL, cluster_.trafficStats().upstream_rq_retry_.value());[m
   EXPECT_EQ(1UL, virtual_cluster_.stats().upstream_rq_retry_.value());[m
   EXPECT_EQ(1UL, route_stats_context_.stats().upstream_rq_retry_.value());[m
 }[m
[36m@@ -253,10 +253,10 @@[m [mTEST_F(RouterRetryStateImplTest, PolicyAltProtocolPostHandshakeFailure) {[m
             state_->shouldRetryReset(remote_refused_stream_reset_, RetryState::Http3Used::No,[m
                                      reset_callback_));[m
 [m
[31m-  EXPECT_EQ(1UL, cluster_.stats().upstream_rq_retry_limit_exceeded_.value());[m
[32m+[m[32m  EXPECT_EQ(1UL, cluster_.trafficStats().upstream_rq_retry_limit_exceeded_.value());[m
   EXPECT_EQ(1UL, virtual_cluster_.stats().upstream_rq_retry_limit_exceeded_.value());[m
   EXPECT_EQ(1UL, route_stats_context_.stats().upstream_rq_retry_limit_exceeded_.value());[m
[31m-  EXPECT_EQ(1UL, cluster_.stats().upstream_rq_retry_.value());[m
[32m+[m[32m  EXPECT_EQ(1UL, cluster_.trafficStats().upstream_rq_retry_.value());[m
   EXPECT_EQ(1UL, virtual_cluster_.stats().upstream_rq_retry_.value());[m
   EXPECT_EQ(1UL, route_stats_context_.stats().upstream_rq_retry_.value());[m
 }[m
[36m@@ -300,10 +300,10 @@[m [mTEST_F(RouterRetryStateImplTest, Policy5xxRemoteReset) {[m
   EXPECT_EQ(RetryStatus::NoRetryLimitExceeded,[m
             state_->shouldRetryReset(remote_reset_, RetryState::Http3Used::No, reset_callback_));[m
 [m
[31m-  EXPECT_EQ(1UL, cluster_.stats().upstream_rq_retry_limit_exceeded_.value());[m
[32m+[m[32m  EXPECT_EQ(1UL, cluster_.trafficStats().upstream_rq_retry_limit_exceeded_.value());[m
   EXPECT_EQ(1UL, virtual_cluster_.stats().upstream_rq_retry_limit_exceeded_.value());[m
   EXPECT_EQ(1UL, route_stats_context_.stats().upstream_rq_retry_limit_exceeded_.value());[m
[31m-  EXPECT_EQ(1UL, cluster_.stats().upstream_rq_retry_.value());[m
[32m+[m[32m  EXPECT_EQ(1UL, cluster_.trafficStats().upstream_rq_retry_.value());[m
   EXPECT_EQ(1UL, virtual_cluster_.stats().upstream_rq_retry_.value());[m
   EXPECT_EQ(1UL, route_stats_context_.stats().upstream_rq_retry_limit_exceeded_.value());[m
 }[m
[36m@@ -390,10 +390,10 @@[m [mTEST_F(RouterRetryStateImplTest, PolicyGatewayErrorRemoteReset) {[m
   EXPECT_EQ(RetryStatus::NoRetryLimitExceeded,[m
             state_->shouldRetryReset(remote_reset_, RetryState::Http3Used::No, reset_callback_));[m
 [m
[31m-  EXPECT_EQ(1UL, cluster_.stats().upstream_rq_retry_limit_exceeded_.value());[m
[32m+[m[32m  EXPECT_EQ(1UL, cluster_.trafficStats().upstream_rq_retry_limit_exceeded_.value());[m
   EXPECT_EQ(1UL, virtual_cluster_.stats().upstream_rq_retry_limit_exceeded_.value());[m
   EXPECT_EQ(1UL, route_stats_context_.stats().upstream_rq_retry_limit_exceeded_.value());[m
[31m-  EXPECT_EQ(1UL, cluster_.stats().upstream_rq_retry_.value());[m
[32m+[m[32m  EXPECT_EQ(1UL, cluster_.trafficStats().upstream_rq_retry_.value());[m
   EXPECT_EQ(1UL, virtual_cluster_.stats().upstream_rq_retry_.value());[m
   EXPECT_EQ(1UL, route_stats_context_.stats().upstream_rq_retry_limit_exceeded_.value());[m
 }[m
[36m@@ -437,10 +437,10 @@[m [mTEST_F(RouterRetryStateImplTest, Policy5xxRemote200RemoteReset) {[m
   EXPECT_EQ(RetryStatus::NoRetryLimitExceeded,[m
             state_->shouldRetryReset(remote_reset_, RetryState::Http3Used::No, reset_callback_));[m
 [m
[31m-  EXPECT_EQ(1UL, cluster_.stats().upstream_rq_retry_limit_exceeded_.value());[m
[32m+[m[32m  EXPECT_EQ(1UL, cluster_.trafficStats().upstream_rq_retry_limit_exceeded_.value());[m
   EXPECT_EQ(1UL, virtual_cluster_.stats().upstream_rq_retry_limit_exceeded_.value());[m
   EXPECT_EQ(1UL, route_stats_context_.stats().upstream_rq_retry_limit_exceeded_.value());[m
[31m-  EXPECT_EQ(1UL, cluster_.stats().upstream_rq_retry_.value());[m
[32m+[m[32m  EXPECT_EQ(1UL, cluster_.trafficStats().upstream_rq_retry_.value());[m
   EXPECT_EQ(1UL, virtual_cluster_.stats().upstream_rq_retry_.value());[m
   EXPECT_EQ(1UL, route_stats_context_.stats().upstream_rq_retry_limit_exceeded_.value());[m
 }[m
[36m@@ -532,7 +532,7 @@[m [mTEST_F(RouterRetryStateImplTest, NoRetryUponTooEarlyStatusCodeWithDownstreamEarl[m
   EXPECT_EQ(RetryStatus::No,[m
             state_->shouldRetryHeaders(response_headers, request_headers, header_callback_));[m
 [m
[31m-  EXPECT_EQ(0UL, cluster_.stats().upstream_rq_retry_.value());[m
[32m+[m[32m  EXPECT_EQ(0UL, cluster_.trafficStats().upstream_rq_retry_.value());[m
   EXPECT_EQ(0UL, virtual_cluster_.stats().upstream_rq_retry_.value());[m
   EXPECT_EQ(0UL, route_stats_context_.stats().upstream_rq_retry_.value());[m
 }[m
[36m@@ -828,10 +828,10 @@[m [mTEST_F(RouterRetryStateImplTest, PolicyResetRemoteReset) {[m
   EXPECT_EQ(RetryStatus::NoRetryLimitExceeded,[m
             state_->shouldRetryReset(remote_reset_, RetryState::Http3Used::No, reset_callback_));[m
 [m
[31m-  EXPECT_EQ(1UL, cluster_.stats().upstream_rq_retry_limit_exceeded_.value());[m
[32m+[m[32m  EXPECT_EQ(1UL, cluster_.trafficStats().upstream_rq_retry_limit_exceeded_.value());[m
   EXPECT_EQ(1UL, virtual_cluster_.stats().upstream_rq_retry_limit_exceeded_.value());[m
   EXPECT_EQ(1UL, route_stats_context_.stats().upstream_rq_retry_limit_exceeded_.value());[m
[31m-  EXPECT_EQ(1UL, cluster_.stats().upstream_rq_retry_.value());[m
[32m+[m[32m  EXPECT_EQ(1UL, cluster_.trafficStats().upstream_rq_retry_.value());[m
   EXPECT_EQ(1UL, virtual_cluster_.stats().upstream_rq_retry_.value());[m
   EXPECT_EQ(1UL, route_stats_context_.stats().upstream_rq_retry_limit_exceeded_.value());[m
 }[m
[36m@@ -915,10 +915,10 @@[m [mTEST_F(RouterRetryStateImplTest, RouteConfigNoRetriesAllowed) {[m
       RetryStatus::NoRetryLimitExceeded,[m
       state_->shouldRetryReset(connect_failure_, RetryState::Http3Used::Unknown, reset_callback_));[m
 [m
[31m-  EXPECT_EQ(1UL, cluster_.stats().upstream_rq_retry_limit_exceeded_.value());[m
[32m+[m[32m  EXPECT_EQ(1UL, cluster_.trafficStats().upstream_rq_retry_limit_exceeded_.value());[m
   EXPECT_EQ(1UL, virtual_cluster_.stats().upstream_rq_retry_limit_exceeded_.value());[m
   EXPECT_EQ(1UL, route_stats_context_.stats().upstream_rq_retry_limit_exceeded_.value());[m
[31m-  EXPECT_EQ(0UL, cluster_.stats().upstream_rq_retry_.value());[m
[32m+[m[32m  EXPECT_EQ(0UL, cluster_.trafficStats().upstream_rq_retry_.value());[m
   EXPECT_EQ(0UL, virtual_cluster_.stats().upstream_rq_retry_.value());[m
   EXPECT_EQ(1UL, route_stats_context_.stats().upstream_rq_retry_limit_exceeded_.value());[m
 }[m
[36m@@ -948,7 +948,7 @@[m [mTEST_F(RouterRetryStateImplTest, NoAvailableRetries) {[m
   EXPECT_EQ([m
       RetryStatus::NoOverflow,[m
       state_->shouldRetryReset(connect_failure_, RetryState::Http3Used::Unknown, reset_callback_));[m
[31m-  EXPECT_EQ(1UL, cluster_.stats().upstream_rq_retry_overflow_.value());[m
[32m+[m[32m  EXPECT_EQ(1UL, cluster_.trafficStats().upstream_rq_retry_overflow_.value());[m
   EXPECT_EQ(1UL, virtual_cluster_.stats().upstream_rq_retry_overflow_.value());[m
   EXPECT_EQ(1UL, route_stats_context_.stats().upstream_rq_retry_overflow_.value());[m
 }[m
[36m@@ -988,9 +988,9 @@[m [mTEST_F(RouterRetryStateImplTest, MaxRetriesHeader) {[m
       RetryStatus::NoRetryLimitExceeded,[m
       state_->shouldRetryReset(connect_failure_, RetryState::Http3Used::Unknown, reset_callback_));[m
 [m
[31m-  EXPECT_EQ(3UL, cluster_.stats().upstream_rq_retry_.value());[m
[31m-  EXPECT_EQ(0UL, cluster_.stats().upstream_rq_retry_success_.value());[m
[31m-  EXPECT_EQ(1UL, cluster_.stats().upstream_rq_retry_limit_exceeded_.value());[m
[32m+[m[32m  EXPECT_EQ(3UL, cluster_.trafficStats().upstream_rq_retry_.value());[m
[32m+[m[32m  EXPECT_EQ(0UL, cluster_.trafficStats().upstream_rq_retry_success_.value());[m
[32m+[m[32m  EXPECT_EQ(1UL, cluster_.trafficStats().upstream_rq_retry_limit_exceeded_.value());[m
   EXPECT_EQ(3UL, virtual_cluster_.stats().upstream_rq_retry_.value());[m
   EXPECT_EQ(0UL, virtual_cluster_.stats().upstream_rq_retry_success_.value());[m
   EXPECT_EQ(1UL, virtual_cluster_.stats().upstream_rq_retry_limit_exceeded_.value());[m
[36m@@ -1062,8 +1062,8 @@[m [mTEST_F(RouterRetryStateImplTest, Backoff) {[m
   EXPECT_EQ(RetryStatus::No,[m
             state_->shouldRetryHeaders(response_headers, request_headers, header_callback_));[m
 [m
[31m-  EXPECT_EQ(5UL, cluster_.stats().upstream_rq_retry_.value());[m
[31m-  EXPECT_EQ(1UL, cluster_.stats().upstream_rq_retry_success_.value());[m
[32m+[m[32m  EXPECT_EQ(5UL, cluster_.trafficStats().upstream_rq_retry_.value());[m
[32m+[m[32m  EXPECT_EQ(1UL, cluster_.trafficStats().upstream_rq_retry_success_.value());[m
   EXPECT_EQ(5UL, virtual_cluster_.stats().upstream_rq_retry_.value());[m
   EXPECT_EQ(1UL, virtual_cluster_.stats().upstream_rq_retry_success_.value());[m
   EXPECT_EQ(5UL, route_stats_context_.stats().upstream_rq_retry_.value());[m
[36m@@ -1306,8 +1306,8 @@[m [mTEST_F(RouterRetryStateImplTest, RateLimitedRetryBackoffStrategy) {[m
       RetryStatus::NoRetryLimitExceeded,[m
       state_->shouldRetryHeaders(response_headers_reset_2, request_headers, header_callback_));[m
 [m
[31m-  EXPECT_EQ(2UL, cluster_.stats().upstream_rq_retry_backoff_ratelimited_.value());[m
[31m-  EXPECT_EQ(2UL, cluster_.stats().upstream_rq_retry_backoff_exponential_.value());[m
[32m+[m[32m  EXPECT_EQ(2UL, cluster_.trafficStats().upstream_rq_retry_backoff_ratelimited_.value());[m
[32m+[m[32m  EXPECT_EQ(2UL, cluster_.trafficStats().upstream_rq_retry_backoff_exponential_.value());[m
 }[m
 [m
 TEST_F(RouterRetryStateImplTest, HostSelectionAttempts) {[m
[36m@@ -1343,10 +1343,10 @@[m [mTEST_F(RouterRetryStateImplTest, ZeroMaxRetriesHeader) {[m
       RetryStatus::NoRetryLimitExceeded,[m
       state_->shouldRetryReset(connect_failure_, RetryState::Http3Used::Unknown, reset_callback_));[m
 [m
[31m-  EXPECT_EQ(1UL, cluster_.stats().upstream_rq_retry_limit_exceeded_.value());[m
[32m+[m[32m  EXPECT_EQ(1UL, cluster_.trafficStats().upstream_rq_retry_limit_exceeded_.value());[m
   EXPECT_EQ(1UL, virtual_cluster_.stats().upstream_rq_retry_limit_exceeded_.value());[m
   EXPECT_EQ(1UL, route_stats_context_.stats().upstream_rq_retry_limit_exceeded_.value());[m
[31m-  EXPECT_EQ(0UL, cluster_.stats().upstream_rq_retry_.value());[m
[32m+[m[32m  EXPECT_EQ(0UL, cluster_.trafficStats().upstream_rq_retry_.value());[m
   EXPECT_EQ(0UL, virtual_cluster_.stats().upstream_rq_retry_.value());[m
   EXPECT_EQ(0UL, route_stats_context_.stats().upstream_rq_retry_.value());[m
 }[m
[36m@@ -1367,10 +1367,10 @@[m [mTEST_F(RouterRetryStateImplTest, NoPreferredOverLimitExceeded) {[m
   EXPECT_EQ(RetryStatus::No,[m
             state_->shouldRetryHeaders(good_response_headers, request_headers, header_callback_));[m
 [m
[31m-  EXPECT_EQ(0UL, cluster_.stats().upstream_rq_retry_limit_exceeded_.value());[m
[32m+[m[32m  EXPECT_EQ(0UL, cluster_.trafficStats().upstream_rq_retry_limit_exceeded_.value());[m
   EXPECT_EQ(0UL, virtual_cluster_.stats().upstream_rq_retry_limit_exceeded_.value());[m
   EXPECT_EQ(0UL, route_stats_context_.stats().upstream_rq_retry_limit_exceeded_.value());[m
[31m-  EXPECT_EQ(1UL, cluster_.stats().upstream_rq_retry_.value());[m
[32m+[m[32m  EXPECT_EQ(1UL, cluster_.trafficStats().upstream_rq_retry_.value());[m
   EXPECT_EQ(1UL, virtual_cluster_.stats().upstream_rq_retry_.value());[m
   EXPECT_EQ(0UL, route_stats_context_.stats().upstream_rq_retry_limit_exceeded_.value());[m
 }[m
[1mdiff --git a/test/common/upstream/eds_test.cc b/test/common/upstream/eds_test.cc[m
[1mindex 88fb82a0c9..2d0dbed694 100644[m
[1m--- a/test/common/upstream/eds_test.cc[m
[1m+++ b/test/common/upstream/eds_test.cc[m
[36m@@ -872,6 +872,118 @@[m [mTEST_F(EdsTest, EndpointRemovalEdsFailButActiveHcSuccess) {[m
   }[m
 }[m
 [m
[32m+[m[32m// Verify the add and removal of hosts with disable active hc flag during eds update.[m
[32m+[m[32mTEST_F(EdsTest, DisableActiveHCEndpoints) {[m
[32m+[m[32m  envoy::config::endpoint::v3::ClusterLoadAssignment cluster_load_assignment;[m
[32m+[m[32m  cluster_load_assignment.set_cluster_name("fare");[m
[32m+[m[32m  resetCluster();[m
[32m+[m[32m  initialize();[m
[32m+[m
[32m+[m[32m  auto health_checker = std::make_shared<MockHealthChecker>();[m
[32m+[m[32m  EXPECT_CALL(*health_checker, start());[m
[32m+[m[32m  EXPECT_CALL(*health_checker, addHostCheckCompleteCb(_)).Times(2);[m
[32m+[m[32m  cluster_->setHealthChecker(health_checker);[m
[32m+[m
[32m+[m[32m  auto add_endpoint = [&cluster_load_assignment](int port, bool disable_hc, bool healthy) {[m
[32m+[m[32m    auto* lb_endpoint = cluster_load_assignment.add_endpoints()->add_lb_endpoints();[m
[32m+[m[32m    auto* endpoint = lb_endpoint->mutable_endpoint();[m
[32m+[m[32m    auto* socket_address = endpoint->mutable_address()->mutable_socket_address();[m
[32m+[m[32m    socket_address->set_address("1.2.3.4");[m
[32m+[m[32m    socket_address->set_port_value(port);[m
[32m+[m[32m    endpoint->mutable_health_check_config()->set_disable_active_health_check(disable_hc);[m
[32m+[m[32m    if (disable_hc) {[m
[32m+[m[32m      if (healthy) {[m
[32m+[m[32m        lb_endpoint->set_health_status(envoy::config::core::v3::HEALTHY);[m
[32m+[m[32m      } else {[m
[32m+[m[32m        lb_endpoint->set_health_status(envoy::config::core::v3::TIMEOUT);[m
[32m+[m[32m      }[m
[32m+[m[32m    }[m
[32m+[m[32m  };[m
[32m+[m
[32m+[m[32m  // First endpoint with disabled active HC.[m
[32m+[m[32m  add_endpoint(80, true, false);[m
[32m+[m[32m  // Second endpoint with enabled active HC.[m
[32m+[m[32m  add_endpoint(81, false, false);[m
[32m+[m[32m  doOnConfigUpdateVerifyNoThrow(cluster_load_assignment);[m
[32m+[m[32m  {[m
[32m+[m[32m    auto& hosts = cluster_->prioritySet().hostSetsPerPriority()[0]->hosts();[m
[32m+[m[32m    EXPECT_EQ(hosts.size(), 2);[m
[32m+[m
[32m+[m[32m    // The endpoint with disabled active health check should not be set FAILED_ACTIVE_HC[m
[32m+[m[32m    // and PENDING_ACTIVE_HC at beginning.[m
[32m+[m[32m    EXPECT_FALSE(hosts[0]->healthFlagGet(Host::HealthFlag::FAILED_ACTIVE_HC));[m
[32m+[m[32m    EXPECT_FALSE(hosts[0]->healthFlagGet(Host::HealthFlag::PENDING_DYNAMIC_REMOVAL));[m
[32m+[m[32m    EXPECT_TRUE(hosts[0]->healthFlagGet(Host::HealthFlag::FAILED_EDS_HEALTH));[m
[32m+[m[32m    EXPECT_TRUE(hosts[1]->healthFlagGet(Host::HealthFlag::FAILED_ACTIVE_HC));[m
[32m+[m
[32m+[m[32m    EXPECT_EQ(Host::Health::Unhealthy, hosts[0]->coarseHealth());[m
[32m+[m[32m    EXPECT_EQ(Host::Health::Unhealthy, hosts[1]->coarseHealth());[m
[32m+[m
[32m+[m[32m    // Remove the pending HC & mark the second host as healthy.[m
[32m+[m[32m    // This is normally done by the health checker.[m
[32m+[m[32m    hosts[1]->healthFlagClear(Host::HealthFlag::PENDING_ACTIVE_HC);[m
[32m+[m[32m    hosts[1]->healthFlagClear(Host::HealthFlag::FAILED_ACTIVE_HC);[m
[32m+[m
[32m+[m[32m    // After the active health check status is changed, run the callbacks to reload hosts.[m
[32m+[m[32m    health_checker->runCallbacks(hosts[1], HealthTransition::Changed);[m
[32m+[m
[32m+[m[32m    auto& hosts_reload = cluster_->prioritySet().hostSetsPerPriority()[0]->hosts();[m
[32m+[m[32m    EXPECT_EQ(hosts_reload.size(), 2);[m
[32m+[m[32m    EXPECT_EQ(Host::Health::Healthy, hosts_reload[1]->coarseHealth());[m
[32m+[m[32m    EXPECT_EQ(1UL, cluster_->prioritySet().hostSetsPerPriority()[0]->healthyHosts().size());[m
[32m+[m[32m  }[m
[32m+[m
[32m+[m[32m  // Now mark the port 80 endpoint as healthy through EDS, no change for the other one.[m
[32m+[m[32m  cluster_load_assignment.clear_endpoints();[m
[32m+[m[32m  add_endpoint(80, true, true);[m
[32m+[m[32m  add_endpoint(81, false, false);[m
[32m+[m[32m  doOnConfigUpdateVerifyNoThrow(cluster_load_assignment);[m
[32m+[m[32m  HostSharedPtr removed_host;[m
[32m+[m[32m  {[m
[32m+[m[32m    auto& hosts = cluster_->prioritySet().hostSetsPerPriority()[0]->hosts();[m
[32m+[m[32m    EXPECT_EQ(hosts.size(), 2);[m
[32m+[m[32m    EXPECT_FALSE(hosts[0]->healthFlagGet(Host::HealthFlag::FAILED_ACTIVE_HC));[m
[32m+[m[32m    EXPECT_FALSE(hosts[0]->healthFlagGet(Host::HealthFlag::FAILED_EDS_HEALTH));[m
[32m+[m[32m    EXPECT_FALSE(hosts[0]->healthFlagGet(Host::HealthFlag::PENDING_DYNAMIC_REMOVAL));[m
[32m+[m
[32m+[m[32m    removed_host = hosts[1];[m
[32m+[m[32m    EXPECT_FALSE(hosts[1]->healthFlagGet(Host::HealthFlag::PENDING_DYNAMIC_REMOVAL));[m
[32m+[m[32m    EXPECT_EQ(2UL, cluster_->prioritySet().hostSetsPerPriority()[0]->healthyHosts().size());[m
[32m+[m[32m  }[m
[32m+[m
[32m+[m[32m  // Disable active health check for both endpoints.[m
[32m+[m[32m  cluster_load_assignment.clear_endpoints();[m
[32m+[m[32m  add_endpoint(80, true, true);[m
[32m+[m[32m  add_endpoint(81, true, true);[m
[32m+[m[32m  doOnConfigUpdateVerifyNoThrow(cluster_load_assignment);[m
[32m+[m[32m  {[m
[32m+[m[32m    // Both hosts should be present, and both should not be PENDING_DYNAMIC_REMOVAL.[m
[32m+[m[32m    auto& hosts = cluster_->prioritySet().hostSetsPerPriority()[0]->hosts();[m
[32m+[m[32m    EXPECT_EQ(hosts.size(), 2);[m
[32m+[m[32m    EXPECT_FALSE(hosts[0]->healthFlagGet(Host::HealthFlag::FAILED_ACTIVE_HC));[m
[32m+[m[32m    EXPECT_FALSE(hosts[0]->healthFlagGet(Host::HealthFlag::FAILED_EDS_HEALTH));[m
[32m+[m[32m    EXPECT_FALSE(hosts[0]->healthFlagGet(Host::HealthFlag::PENDING_DYNAMIC_REMOVAL));[m
[32m+[m
[32m+[m[32m    // Verify that we have a new host. The host is removed even it is active[m
[32m+[m[32m    // healthy when the active hc flag is changed.[m
[32m+[m[32m    EXPECT_EQ(removed_host->address()->asString(), hosts[1]->address()->asString());[m
[32m+[m[32m    EXPECT_NE(removed_host, hosts[1]);[m
[32m+[m[32m    EXPECT_FALSE(hosts[1]->healthFlagGet(Host::HealthFlag::PENDING_DYNAMIC_REMOVAL));[m
[32m+[m[32m    EXPECT_EQ(2UL, cluster_->prioritySet().hostSetsPerPriority()[0]->healthyHosts().size());[m
[32m+[m[32m  }[m
[32m+[m
[32m+[m[32m  // Enable the active health check for the port 80 endpoint.[m
[32m+[m[32m  cluster_load_assignment.clear_endpoints();[m
[32m+[m[32m  add_endpoint(80, false, true);[m
[32m+[m[32m  doOnConfigUpdateVerifyNoThrow(cluster_load_assignment);[m
[32m+[m[32m  {[m
[32m+[m[32m    auto& hosts = cluster_->prioritySet().hostSetsPerPriority()[0]->hosts();[m
[32m+[m[32m    EXPECT_EQ(hosts.size(), 1);[m
[32m+[m[32m    EXPECT_TRUE(hosts[0]->healthFlagGet(Host::HealthFlag::FAILED_ACTIVE_HC));[m
[32m+[m[32m    EXPECT_EQ(0UL, cluster_->prioritySet().hostSetsPerPriority()[0]->healthyHosts().size());[m
[32m+[m[32m  }[m
[32m+[m[32m}[m
[32m+[m
 // Validate that onConfigUpdate() removes endpoints that are marked as healthy[m
 // when configured to drain on host removal.[m
 TEST_F(EdsTest, EndpointRemovalClusterDrainOnHostRemoval) {[m
[1mdiff --git a/test/common/upstream/hds_test.cc b/test/common/upstream/hds_test.cc[m
[1mindex c831352432..6879ee2640 100644[m
[1m--- a/test/common/upstream/hds_test.cc[m
[1m+++ b/test/common/upstream/hds_test.cc[m
[36m@@ -153,7 +153,8 @@[m [mprotected:[m
   // Creates a HealthCheckSpecifier message that contains several clusters, endpoints, localities,[m
   // with only one health check type.[m
   std::unique_ptr<envoy::service::health::v3::HealthCheckSpecifier>[m
[31m-  createComplexSpecifier(uint32_t n_clusters, uint32_t n_localities, uint32_t n_endpoints) {[m
[32m+[m[32m  createComplexSpecifier(uint32_t n_clusters, uint32_t n_localities, uint32_t n_endpoints,[m
[32m+[m[32m                         bool disable_hc = false) {[m
     // Final specifier to return.[m
     std::unique_ptr<envoy::service::health::v3::HealthCheckSpecifier> msg =[m
         std::make_unique<envoy::service::health::v3::HealthCheckSpecifier>();[m
[36m@@ -188,11 +189,13 @@[m [mprotected:[m
 [m
         // add some endpoints to the locality group with iterative naming for verification.[m
         for (uint32_t endpoint_num = 0; endpoint_num < n_endpoints; endpoint_num++) {[m
[31m-          auto* socket_address =[m
[31m-              locality_endpoints->add_endpoints()->mutable_address()->mutable_socket_address();[m
[32m+[m[32m          auto* endpoint = locality_endpoints->add_endpoints();[m
[32m+[m
[32m+[m[32m          auto* socket_address = endpoint->mutable_address()->mutable_socket_address();[m
           socket_address->set_address([m
               absl::StrCat("127.", cluster_num, ".", loc_num, ".", endpoint_num));[m
           socket_address->set_port_value(1234);[m
[32m+[m[32m          endpoint->mutable_health_check_config()->set_disable_active_health_check(disable_hc);[m
         }[m
       }[m
     }[m
[36m@@ -893,6 +896,42 @@[m [mTEST_F(HdsTest, TestUpdateEndpoints) {[m
   checkHdsCounters(3, 0, 0, 3);[m
 }[m
 [m
[32m+[m[32m// Skip the endpoints with disabled active health check during message processing.[m
[32m+[m[32mTEST_F(HdsTest, TestUpdateEndpointsWithActiveHCflag) {[m
[32m+[m[32m  EXPECT_CALL(*async_client_, startRaw(_, _, _, _)).WillOnce(Return(&async_stream_));[m
[32m+[m[32m  EXPECT_CALL(async_stream_, sendMessageRaw_(_, _));[m
[32m+[m[32m  createHdsDelegate();[m
[32m+[m
[32m+[m[32m  // Create Message, and later add/remove endpoints from the second cluster.[m
[32m+[m[32m  message.reset(createSimpleMessage());[m
[32m+[m[32m  message->MergeFrom(*createComplexSpecifier(1, 1, 2));[m
[32m+[m
[32m+[m[32m  // Create a new active connection on request, setting its status to connected[m
[32m+[m[32m  // to mock a found endpoint.[m
[32m+[m[32m  expectCreateClientConnection();[m
[32m+[m
[32m+[m[32m  EXPECT_CALL(*server_response_timer_, enableTimer(_, _)).Times(AtLeast(1));[m
[32m+[m[32m  EXPECT_CALL(async_stream_, sendMessageRaw_(_, false));[m
[32m+[m[32m  EXPECT_CALL(test_factory_, createClusterInfo(_)).WillRepeatedly(Return(cluster_info_));[m
[32m+[m[32m  EXPECT_CALL(server_context_.dispatcher_, deferredDelete_(_)).Times(AtLeast(1));[m
[32m+[m[32m  // Process message[m
[32m+[m[32m  hds_delegate_->onReceiveMessage(std::move(message));[m
[32m+[m[32m  hds_delegate_->sendResponse();[m
[32m+[m
[32m+[m[32m  // Save list of hosts/endpoints for comparison later.[m
[32m+[m[32m  auto original_hosts = hds_delegate_->hdsClusters()[1]->hosts();[m
[32m+[m[32m  ASSERT_EQ(original_hosts.size(), 2);[m
[32m+[m
[32m+[m[32m  // Ignoring the endpoints with disabled active health check.[m
[32m+[m[32m  message.reset(createSimpleMessage());[m
[32m+[m[32m  message->MergeFrom(*createComplexSpecifier(1, 1, 2, true));[m
[32m+[m[32m  hds_delegate_->onReceiveMessage(std::move(message));[m
[32m+[m
[32m+[m[32m  // Get the new clusters list from HDS.[m
[32m+[m[32m  auto new_hosts = hds_delegate_->hdsClusters()[1]->hosts();[m
[32m+[m[32m  ASSERT_EQ(new_hosts.size(), 0);[m
[32m+[m[32m}[m
[32m+[m
 // Test adding, reusing, and removing health checks.[m
 TEST_F(HdsTest, TestUpdateHealthCheckers) {[m
   EXPECT_CALL(*async_client_, startRaw(_, _, _, _)).WillOnce(Return(&async_stream_));[m
[1mdiff --git a/test/common/upstream/health_check_fuzz.cc b/test/common/upstream/health_check_fuzz.cc[m
[1mindex 06aecdb303..e845aceb08 100644[m
[1m--- a/test/common/upstream/health_check_fuzz.cc[m
[1m+++ b/test/common/upstream/health_check_fuzz.cc[m
[36m@@ -106,7 +106,7 @@[m [mvoid HttpHealthCheckFuzz::initialize(test::common::upstream::HealthCheckTestCase[m
   cluster_->prioritySet().getMockHostSet(0)->hosts_ = {[m
       makeTestHost(cluster_->info_, "tcp://127.0.0.1:80", *time_source)};[m
   if (input.upstream_cx_success()) {[m
[31m-    cluster_->info_->stats().upstream_cx_total_.inc();[m
[32m+[m[32m    cluster_->info_->trafficStats().upstream_cx_total_.inc();[m
   }[m
   expectSessionCreate();[m
   expectStreamCreate(0);[m
[36m@@ -217,7 +217,7 @@[m [mvoid TcpHealthCheckFuzz::initialize(test::common::upstream::HealthCheckTestCase[m
   cluster_->prioritySet().getMockHostSet(0)->hosts_ = {[m
       makeTestHost(cluster_->info_, "tcp://127.0.0.1:80", *time_source)};[m
   if (input.upstream_cx_success()) {[m
[31m-    cluster_->info_->stats().upstream_cx_total_.inc();[m
[32m+[m[32m    cluster_->info_->trafficStats().upstream_cx_total_.inc();[m
   }[m
   expectSessionCreate();[m
   expectClientCreate();[m
[36m@@ -326,7 +326,7 @@[m [mvoid GrpcHealthCheckFuzz::initialize(test::common::upstream::HealthCheckTestCase[m
   cluster_->prioritySet().getMockHostSet(0)->hosts_ = {[m
       makeTestHost(cluster_->info_, "tcp://127.0.0.1:80", *time_source)};[m
   if (input.upstream_cx_success()) {[m
[31m-    cluster_->info_->stats().upstream_cx_total_.inc();[m
[32m+[m[32m    cluster_->info_->trafficStats().upstream_cx_total_.inc();[m
   }[m
   expectSessionCreate();[m
   ON_CALL(dispatcher_, createClientConnection_(_, _, _, _))[m
[1mdiff --git a/test/common/upstream/health_checker_impl_test.cc b/test/common/upstream/health_checker_impl_test.cc[m
[1mindex cccc700549..3fd5f68733 100644[m
[1m--- a/test/common/upstream/health_checker_impl_test.cc[m
[1m+++ b/test/common/upstream/health_checker_impl_test.cc[m
[36m@@ -858,7 +858,7 @@[m [mTEST_F(HttpHealthCheckerImplTest, Success) {[m
 [m
   cluster_->prioritySet().getMockHostSet(0)->hosts_ = {[m
       makeTestHost(cluster_->info_, "tcp://127.0.0.1:80", simTime())};[m
[31m-  cluster_->info_->stats().upstream_cx_total_.inc();[m
[32m+[m[32m  cluster_->info_->trafficStats().upstream_cx_total_.inc();[m
   expectSessionCreate();[m
   expectStreamCreate(0);[m
   EXPECT_CALL(*test_sessions_[0]->timeout_timer_, enableTimer(_, _));[m
[36m@@ -881,7 +881,7 @@[m [mTEST_F(HttpHealthCheckerImplTest, Degraded) {[m
 [m
   cluster_->prioritySet().getMockHostSet(0)->hosts_ = {[m
       makeTestHost(cluster_->info_, "tcp://127.0.0.1:80", simTime())};[m
[31m-  cluster_->info_->stats().upstream_cx_total_.inc();[m
[32m+[m[32m  cluster_->info_->trafficStats().upstream_cx_total_.inc();[m
   expectSessionCreate();[m
   expectStreamCreate(0);[m
   EXPECT_CALL(*test_sessions_[0]->timeout_timer_, enableTimer(_, _));[m
[36m@@ -1010,7 +1010,7 @@[m [mTEST_F(HttpHealthCheckerImplTest, SuccessIntervalJitterPercent) {[m
 [m
   cluster_->prioritySet().getMockHostSet(0)->hosts_ = {[m
       makeTestHost(cluster_->info_, "tcp://127.0.0.1:80", simTime())};[m
[31m-  cluster_->info_->stats().upstream_cx_total_.inc();[m
[32m+[m[32m  cluster_->info_->trafficStats().upstream_cx_total_.inc();[m
   expectSessionCreate();[m
   expectStreamCreate(0);[m
   EXPECT_CALL(*test_sessions_[0]->timeout_timer_, enableTimer(_, _));[m
[36m@@ -1041,7 +1041,7 @@[m [mTEST_F(HttpHealthCheckerImplTest, SuccessWithSpurious1xx) {[m
 [m
   cluster_->prioritySet().getMockHostSet(0)->hosts_ = {[m
       makeTestHost(cluster_->info_, "tcp://127.0.0.1:80", simTime())};[m
[31m-  cluster_->info_->stats().upstream_cx_total_.inc();[m
[32m+[m[32m  cluster_->info_->trafficStats().upstream_cx_total_.inc();[m
   expectSessionCreate();[m
   expectStreamCreate(0);[m
   EXPECT_CALL(*test_sessions_[0]->timeout_timer_, enableTimer(_, _));[m
[36m@@ -1069,7 +1069,7 @@[m [mTEST_F(HttpHealthCheckerImplTest, SuccessWithSpuriousMetadata) {[m
 [m
   cluster_->prioritySet().getMockHostSet(0)->hosts_ = {[m
       makeTestHost(cluster_->info_, "tcp://127.0.0.1:80", simTime())};[m
[31m-  cluster_->info_->stats().upstream_cx_total_.inc();[m
[32m+[m[32m  cluster_->info_->trafficStats().upstream_cx_total_.inc();[m
   expectSessionCreate();[m
   expectStreamCreate(0);[m
   EXPECT_CALL(*test_sessions_[0]->timeout_timer_, enableTimer(_, _));[m
[36m@@ -1099,8 +1099,8 @@[m [mTEST_F(HttpHealthCheckerImplTest, SuccessWithMultipleHosts) {[m
   cluster_->prioritySet().getMockHostSet(0)->hosts_ = {[m
       makeTestHost(cluster_->info_, "tcp://127.0.0.1:80", simTime()),[m
       makeTestHost(cluster_->info_, "tcp://127.0.0.1:81", simTime())};[m
[31m-  cluster_->info_->stats().upstream_cx_total_.inc();[m
[31m-  cluster_->info_->stats().upstream_cx_total_.inc();[m
[32m+[m[32m  cluster_->info_->trafficStats().upstream_cx_total_.inc();[m
[32m+[m[32m  cluster_->info_->trafficStats().upstream_cx_total_.inc();[m
   expectSessionCreate();[m
   expectStreamCreate(0);[m
   EXPECT_CALL(*test_sessions_[0]->timeout_timer_, enableTimer(_, _));[m
[36m@@ -1136,8 +1136,8 @@[m [mTEST_F(HttpHealthCheckerImplTest, SuccessWithMultipleHostSets) {[m
       makeTestHost(cluster_->info_, "tcp://127.0.0.1:80", simTime())};[m
   cluster_->prioritySet().getMockHostSet(1)->hosts_ = {[m
       makeTestHost(cluster_->info_, "tcp://127.0.0.1:81", simTime())};[m
[31m-  cluster_->info_->stats().upstream_cx_total_.inc();[m
[31m-  cluster_->info_->stats().upstream_cx_total_.inc();[m
[32m+[m[32m  cluster_->info_->trafficStats().upstream_cx_total_.inc();[m
[32m+[m[32m  cluster_->info_->trafficStats().upstream_cx_total_.inc();[m
   expectSessionCreate();[m
   expectStreamCreate(0);[m
   EXPECT_CALL(*test_sessions_[0]->timeout_timer_, enableTimer(_, _));[m
[36m@@ -1170,7 +1170,7 @@[m [mTEST_F(HttpHealthCheckerImplTest, SuccessExpectedResponseCheck) {[m
 [m
   cluster_->prioritySet().getMockHostSet(0)->hosts_ = {[m
       makeTestHost(cluster_->info_, "tcp://127.0.0.1:80", simTime())};[m
[31m-  cluster_->info_->stats().upstream_cx_total_.inc();[m
[32m+[m[32m  cluster_->info_->trafficStats().upstream_cx_total_.inc();[m
   expectSessionCreate();[m
   expectStreamCreate(0);[m
   EXPECT_CALL(*test_sessions_[0]->timeout_timer_, enableTimer(_, _));[m
[36m@@ -1193,7 +1193,7 @@[m [mTEST_F(HttpHealthCheckerImplTest, SuccessExpectedResponseStringContainsCheck) {[m
 [m
   cluster_->prioritySet().getMockHostSet(0)->hosts_ = {[m
       makeTestHost(cluster_->info_, "tcp://127.0.0.1:80", simTime())};[m
[31m-  cluster_->info_->stats().upstream_cx_total_.inc();[m
[32m+[m[32m  cluster_->info_->trafficStats().upstream_cx_total_.inc();[m
   expectSessionCreate();[m
   expectStreamCreate(0);[m
   EXPECT_CALL(*test_sessions_[0]->timeout_timer_, enableTimer(_, _));[m
[36m@@ -1217,7 +1217,7 @@[m [mTEST_F(HttpHealthCheckerImplTest, SuccessExpectedResponseHexStringContainsCheck)[m
 [m
   cluster_->prioritySet().getMockHostSet(0)->hosts_ = {[m
       makeTestHost(cluster_->info_, "tcp://127.0.0.1:80", simTime())};[m
[31m-  cluster_->info_->stats().upstream_cx_total_.inc();[m
[32m+[m[32m  cluster_->info_->trafficStats().upstream_cx_total_.inc();[m
   expectSessionCreate();[m
   expectStreamCreate(0);[m
   EXPECT_CALL(*test_sessions_[0]->timeout_timer_, enableTimer(_, _));[m
[36m@@ -1240,7 +1240,7 @@[m [mTEST_F(HttpHealthCheckerImplTest, SuccessExpectedResponseCheckBuffer) {[m
 [m
   cluster_->prioritySet().getMockHostSet(0)->hosts_ = {[m
       makeTestHost(cluster_->info_, "tcp://127.0.0.1:80", simTime())};[m
[31m-  cluster_->info_->stats().upstream_cx_total_.inc();[m
[32m+[m[32m  cluster_->info_->trafficStats().upstream_cx_total_.inc();[m
   expectSessionCreate();[m
   expectStreamCreate(0);[m
   EXPECT_CALL(*test_sessions_[0]->timeout_timer_, enableTimer(_, _));[m
[36m@@ -1265,7 +1265,7 @@[m [mTEST_F(HttpHealthCheckerImplTest, SuccessExpectedResponseCheckMaxBuffer) {[m
 [m
   cluster_->prioritySet().getMockHostSet(0)->hosts_ = {[m
       makeTestHost(cluster_->info_, "tcp://127.0.0.1:80", simTime())};[m
[31m-  cluster_->info_->stats().upstream_cx_total_.inc();[m
[32m+[m[32m  cluster_->info_->trafficStats().upstream_cx_total_.inc();[m
   expectSessionCreate();[m
   expectStreamCreate(0);[m
   EXPECT_CALL(*test_sessions_[0]->timeout_timer_, enableTimer(_, _));[m
[36m@@ -1305,7 +1305,7 @@[m [mTEST_F(HttpHealthCheckerImplTest, SuccessExpectedResponseCheckHttp2) {[m
 [m
   cluster_->prioritySet().getMockHostSet(0)->hosts_ = {[m
       makeTestHost(cluster_->info_, "tcp://127.0.0.1:80", simTime())};[m
[31m-  cluster_->info_->stats().upstream_cx_total_.inc();[m
[32m+[m[32m  cluster_->info_->trafficStats().upstream_cx_total_.inc();[m
   expectSessionCreate();[m
   expectStreamCreate(0);[m
   EXPECT_CALL(*test_sessions_[0]->timeout_timer_, enableTimer(_, _));[m
[36m@@ -1330,7 +1330,7 @@[m [mTEST_F(HttpHealthCheckerImplTest, FailExpectedResponseCheck) {[m
 [m
   cluster_->prioritySet().getMockHostSet(0)->hosts_ = {[m
       makeTestHost(cluster_->info_, "tcp://127.0.0.1:80", simTime())};[m
[31m-  cluster_->info_->stats().upstream_cx_total_.inc();[m
[32m+[m[32m  cluster_->info_->trafficStats().upstream_cx_total_.inc();[m
   expectSessionCreate();[m
   expectStreamCreate(0);[m
   EXPECT_CALL(*test_sessions_[0]->timeout_timer_, enableTimer(_, _));[m
[36m@@ -1356,7 +1356,7 @@[m [mTEST_F(HttpHealthCheckerImplTest, FailStatusCheckWithExpectedResponseCheck) {[m
 [m
   cluster_->prioritySet().getMockHostSet(0)->hosts_ = {[m
       makeTestHost(cluster_->info_, "tcp://127.0.0.1:80", simTime())};[m
[31m-  cluster_->info_->stats().upstream_cx_total_.inc();[m
[32m+[m[32m  cluster_->info_->trafficStats().upstream_cx_total_.inc();[m
   expectSessionCreate();[m
   expectStreamCreate(0);[m
   EXPECT_CALL(*test_sessions_[0]->timeout_timer_, enableTimer(_, _));[m
[36m@@ -1381,7 +1381,7 @@[m [mTEST_F(HttpHealthCheckerImplTest, ImmediateFailExpectedResponseCheck) {[m
 [m
   cluster_->prioritySet().getMockHostSet(0)->hosts_ = {[m
       makeTestHost(cluster_->info_, "tcp://127.0.0.1:80", simTime())};[m
[31m-  cluster_->info_->stats().upstream_cx_total_.inc();[m
[32m+[m[32m  cluster_->info_->trafficStats().upstream_cx_total_.inc();[m
   expectSessionCreate();[m
   expectStreamCreate(0);[m
   EXPECT_CALL(*test_sessions_[0]->timeout_timer_, enableTimer(_, _));[m
[36m@@ -1428,7 +1428,7 @@[m [mTEST_F(HttpHealthCheckerImplTest, ZeroRetryInterval) {[m
 [m
   cluster_->prioritySet().getMockHostSet(0)->hosts_ = {[m
       makeTestHost(cluster_->info_, "tcp://127.0.0.1:80", simTime())};[m
[31m-  cluster_->info_->stats().upstream_cx_total_.inc();[m
[32m+[m[32m  cluster_->info_->trafficStats().upstream_cx_total_.inc();[m
   expectSessionCreate();[m
   expectStreamCreate(0);[m
   EXPECT_CALL(*test_sessions_[0]->timeout_timer_, enableTimer(_, _));[m
[36m@@ -1487,7 +1487,7 @@[m [mTEST_F(HttpHealthCheckerImplTest, TlsOptions) {[m
   allocHealthChecker(yaml);[m
   cluster_->prioritySet().getMockHostSet(0)->hosts_ = {[m
       makeTestHost(cluster_->info_, "tcp://127.0.0.1:80", simTime())};[m
[31m-  cluster_->info_->stats().upstream_cx_total_.inc();[m
[32m+[m[32m  cluster_->info_->trafficStats().upstream_cx_total_.inc();[m
   expectSessionCreate();[m
   expectStreamCreate(0);[m
   EXPECT_CALL(*test_sessions_[0]->timeout_timer_, enableTimer(_, _));[m
[36m@@ -1505,7 +1505,7 @@[m [mTEST_F(HttpHealthCheckerImplTest, SuccessServiceCheck) {[m
 [m
   cluster_->prioritySet().getMockHostSet(0)->hosts_ = {[m
       makeTestHost(cluster_->info_, "tcp://127.0.0.1:80", simTime())};[m
[31m-  cluster_->info_->stats().upstream_cx_total_.inc();[m
[32m+[m[32m  cluster_->info_->trafficStats().upstream_cx_total_.inc();[m
   expectSessionCreate();[m
   expectStreamCreate(0);[m
   EXPECT_CALL(*test_sessions_[0]->timeout_timer_, enableTimer(_, _));[m
[36m@@ -1541,7 +1541,7 @@[m [mTEST_F(HttpHealthCheckerImplTest, SuccessServicePrefixPatternCheck) {[m
 [m
   cluster_->prioritySet().getMockHostSet(0)->hosts_ = {[m
       makeTestHost(cluster_->info_, "tcp://127.0.0.1:80", simTime())};[m
[31m-  cluster_->info_->stats().upstream_cx_total_.inc();[m
[32m+[m[32m  cluster_->info_->trafficStats().upstream_cx_total_.inc();[m
   expectSessionCreate();[m
   expectStreamCreate(0);[m
   EXPECT_CALL(*test_sessions_[0]->timeout_timer_, enableTimer(_, _));[m
[36m@@ -1577,7 +1577,7 @@[m [mTEST_F(HttpHealthCheckerImplTest, SuccessServiceExactPatternCheck) {[m
 [m
   cluster_->prioritySet().getMockHostSet(0)->hosts_ = {[m
       makeTestHost(cluster_->info_, "tcp://127.0.0.1:80", simTime())};[m
[31m-  cluster_->info_->stats().upstream_cx_total_.inc();[m
[32m+[m[32m  cluster_->info_->trafficStats().upstream_cx_total_.inc();[m
   expectSessionCreate();[m
   expectStreamCreate(0);[m
   EXPECT_CALL(*test_sessions_[0]->timeout_timer_, enableTimer(_, _));[m
[36m@@ -1613,7 +1613,7 @@[m [mTEST_F(HttpHealthCheckerImplTest, SuccessServiceRegexPatternCheck) {[m
 [m
   cluster_->prioritySet().getMockHostSet(0)->hosts_ = {[m
       makeTestHost(cluster_->info_, "tcp://127.0.0.1:80", simTime())};[m
[31m-  cluster_->info_->stats().upstream_cx_total_.inc();[m
[32m+[m[32m  cluster_->info_->trafficStats().upstream_cx_total_.inc();[m
   expectSessionCreate();[m
   expectStreamCreate(0);[m
   EXPECT_CALL(*test_sessions_[0]->timeout_timer_, enableTimer(_, _));[m
[36m@@ -1657,7 +1657,7 @@[m [mTEST_F(HttpHealthCheckerImplTest, SuccessServiceCheckWithCustomHostValueOnTheHos[m
   EXPECT_CALL(*this, onHostStatus(_, HealthTransition::Unchanged));[m
 [m
   cluster_->prioritySet().getMockHostSet(0)->hosts_ = {test_host};[m
[31m-  cluster_->info_->stats().upstream_cx_total_.inc();[m
[32m+[m[32m  cluster_->info_->trafficStats().upstream_cx_total_.inc();[m
   expectSessionCreate();[m
   expectStreamCreate(0);[m
   EXPECT_CALL(*test_sessions_[0]->timeout_timer_, enableTimer(_, _));[m
[36m@@ -1703,7 +1703,7 @@[m [mTEST_F(HttpHealthCheckerImplTest,[m
   EXPECT_CALL(*this, onHostStatus(_, HealthTransition::Unchanged));[m
 [m
   cluster_->prioritySet().getMockHostSet(0)->hosts_ = {test_host};[m
[31m-  cluster_->info_->stats().upstream_cx_total_.inc();[m
[32m+[m[32m  cluster_->info_->trafficStats().upstream_cx_total_.inc();[m
   expectSessionCreate();[m
   expectStreamCreate(0);[m
   EXPECT_CALL(*test_sessions_[0]->timeout_timer_, enableTimer(_, _));[m
[36m@@ -1739,7 +1739,7 @@[m [mTEST_F(HttpHealthCheckerImplTest, SuccessServiceCheckWithCustomHostValue) {[m
 [m
   cluster_->prioritySet().getMockHostSet(0)->hosts_ = {[m
       makeTestHost(cluster_->info_, "tcp://127.0.0.1:80", simTime())};[m
[31m-  cluster_->info_->stats().upstream_cx_total_.inc();[m
[32m+[m[32m  cluster_->info_->trafficStats().upstream_cx_total_.inc();[m
   expectSessionCreate();[m
   expectStreamCreate(0);[m
   EXPECT_CALL(*test_sessions_[0]->timeout_timer_, enableTimer(_, _));[m
[36m@@ -1804,7 +1804,7 @@[m [mTEST_F(HttpHealthCheckerImplTest, SuccessServiceCheckWithAdditionalHeaders) {[m
 [m
   cluster_->prioritySet().getMockHostSet(0)->hosts_ = {[m
       makeTestHost(cluster_->info_, "tcp://127.0.0.1:80", metadata, simTime())};[m
[31m-  cluster_->info_->stats().upstream_cx_total_.inc();[m
[32m+[m[32m  cluster_->info_->trafficStats().upstream_cx_total_.inc();[m
   expectSessionCreate();[m
   expectStreamCreate(0);[m
   EXPECT_CALL(*test_sessions_[0]->timeout_timer_, enableTimer(_, _));[m
[36m@@ -1869,7 +1869,7 @@[m [mTEST_F(HttpHealthCheckerImplTest, SuccessServiceCheckWithoutUserAgent) {[m
   std::string current_start_time;[m
   cluster_->prioritySet().getMockHostSet(0)->hosts_ = {[m
       makeTestHost(cluster_->info_, "tcp://127.0.0.1:80", metadata, simTime())};[m
[31m-  cluster_->info_->stats().upstream_cx_total_.inc();[m
[32m+[m[32m  cluster_->info_->trafficStats().upstream_cx_total_.inc();[m
   expectSessionCreate();[m
   expectStreamCreate(0);[m
   EXPECT_CALL(*test_sessions_[0]->timeout_timer_, enableTimer(_, _));[m
[36m@@ -1907,7 +1907,7 @@[m [mTEST_F(HttpHealthCheckerImplTest, ServiceDoesNotMatchFail) {[m
 [m
   cluster_->prioritySet().getMockHostSet(0)->hosts_ = {[m
       makeTestHost(cluster_->info_, "tcp://127.0.0.1:80", simTime())};[m
[31m-  cluster_->info_->stats().upstream_cx_total_.inc();[m
[32m+[m[32m  cluster_->info_->trafficStats().upstream_cx_total_.inc();[m
   expectSessionCreate();[m
   expectStreamCreate(0);[m
   EXPECT_CALL(*test_sessions_[0]->timeout_timer_, enableTimer(_, _));[m
[36m@@ -1938,7 +1938,7 @@[m [mTEST_F(HttpHealthCheckerImplTest, ServicePatternDoesNotMatchFail) {[m
 [m
   cluster_->prioritySet().getMockHostSet(0)->hosts_ = {[m
       makeTestHost(cluster_->info_, "tcp://127.0.0.1:80", simTime())};[m
[31m-  cluster_->info_->stats().upstream_cx_total_.inc();[m
[32m+[m[32m  cluster_->info_->trafficStats().upstream_cx_total_.inc();[m
   expectSessionCreate();[m
   expectStreamCreate(0);[m
   EXPECT_CALL(*test_sessions_[0]->timeout_timer_, enableTimer(_, _));[m
[36m@@ -1969,7 +1969,7 @@[m [mTEST_F(HttpHealthCheckerImplTest, ServiceNotPresentInResponseFail) {[m
 [m
   cluster_->prioritySet().getMockHostSet(0)->hosts_ = {[m
       makeTestHost(cluster_->info_, "tcp://127.0.0.1:80", simTime())};[m
[31m-  cluster_->info_->stats().upstream_cx_total_.inc();[m
[32m+[m[32m  cluster_->info_->trafficStats().upstream_cx_total_.inc();[m
   expectSessionCreate();[m
   expectStreamCreate(0);[m
   EXPECT_CALL(*test_sessions_[0]->timeout_timer_, enableTimer(_, _));[m
[36m@@ -1997,7 +1997,7 @@[m [mTEST_F(HttpHealthCheckerImplTest, ServiceCheckRuntimeOff) {[m
 [m
   cluster_->prioritySet().getMockHostSet(0)->hosts_ = {[m
       makeTestHost(cluster_->info_, "tcp://127.0.0.1:80", simTime())};[m
[31m-  cluster_->info_->stats().upstream_cx_total_.inc();[m
[32m+[m[32m  cluster_->info_->trafficStats().upstream_cx_total_.inc();[m
   expectSessionCreate();[m
   expectStreamCreate(0);[m
   EXPECT_CALL(*test_sessions_[0]->timeout_timer_, enableTimer(_, _));[m
[36m@@ -2024,7 +2024,7 @@[m [mTEST_F(HttpHealthCheckerImplTest, ServiceCheckRuntimeOffWithStringPattern) {[m
 [m
   cluster_->prioritySet().getMockHostSet(0)->hosts_ = {[m
       makeTestHost(cluster_->info_, "tcp://127.0.0.1:80", simTime())};[m
[31m-  cluster_->info_->stats().upstream_cx_total_.inc();[m
[32m+[m[32m  cluster_->info_->trafficStats().upstream_cx_total_.inc();[m
   expectSessionCreate();[m
   expectStreamCreate(0);[m
   EXPECT_CALL(*test_sessions_[0]->timeout_timer_, enableTimer(_, _));[m
[36m@@ -2478,6 +2478,54 @@[m [mTEST_F(HttpHealthCheckerImplTest, DynamicAddAndRemove) {[m
   cluster_->prioritySet().getMockHostSet(0)->runCallbacks({}, removed);[m
 }[m
 [m
[32m+[m[32m// Verify the removal when disable active health check for a host works.[m
[32m+[m[32mTEST_F(HttpHealthCheckerImplTest, DynamicRemoveDisableHC) {[m
[32m+[m[32m  setupNoServiceValidationHC();[m
[32m+[m
[32m+[m[32m  expectSessionCreate();[m
[32m+[m[32m  expectStreamCreate(0);[m
[32m+[m[32m  EXPECT_CALL(*test_sessions_[0]->timeout_timer_, enableTimer(_, _));[m
[32m+[m
[32m+[m[32m  envoy::config::endpoint::v3::Endpoint::HealthCheckConfig health_check_config;[m
[32m+[m[32m  health_check_config.set_disable_active_health_check(false);[m
[32m+[m[32m  auto enable_host = std::make_shared<HostImpl>([m
[32m+[m[32m      cluster_->info_, "test_host", Network::Utility::resolveUrl("tcp://127.0.0.1:80"), nullptr, 1,[m
[32m+[m[32m      envoy::config::core::v3::Locality(), health_check_config, 0, envoy::config::core::v3::UNKNOWN,[m
[32m+[m[32m      simTime());[m
[32m+[m[32m  cluster_->prioritySet().getMockHostSet(0)->hosts_ = {enable_host};[m
[32m+[m[32m  health_checker_->start();[m
[32m+[m[32m  EXPECT_CALL(*this, onHostStatus(_, HealthTransition::Unchanged));[m
[32m+[m
[32m+[m[32m  health_check_config.set_disable_active_health_check(true);[m
[32m+[m[32m  auto disable_host = std::make_shared<HostImpl>([m
[32m+[m[32m      cluster_->info_, "test_host", Network::Utility::resolveUrl("tcp://127.0.0.1:80"), nullptr, 1,[m
[32m+[m[32m      envoy::config::core::v3::Locality(), health_check_config, 0, envoy::config::core::v3::UNKNOWN,[m
[32m+[m[32m      simTime());[m
[32m+[m[32m  cluster_->prioritySet().getMockHostSet(0)->hosts_ = {disable_host};[m
[32m+[m[32m  EXPECT_CALL(*test_sessions_[0]->client_connection_, close(_));[m
[32m+[m[32m  cluster_->prioritySet().runUpdateCallbacks(0, {disable_host}, {enable_host});[m
[32m+[m[32m}[m
[32m+[m
[32m+[m[32m// Verify a session in health checker is not created when disable health check.[m
[32m+[m[32mTEST_F(HttpHealthCheckerImplTest, AddDisableHC) {[m
[32m+[m[32m  setupNoServiceValidationHC();[m
[32m+[m
[32m+[m[32m  TestSessionPtr new_test_session(new TestSession());[m
[32m+[m[32m  test_sessions_.emplace_back(std::move(new_test_session));[m
[32m+[m[32m  EXPECT_CALL(dispatcher_, createClientConnection_(_, _, _, _)).Times(0);[m
[32m+[m[32m  EXPECT_CALL(*health_checker_, createCodecClient_(_)).Times(0);[m
[32m+[m
[32m+[m[32m  envoy::config::endpoint::v3::Endpoint::HealthCheckConfig health_check_config;[m
[32m+[m[32m  health_check_config.set_disable_active_health_check(true);[m
[32m+[m[32m  auto disable_host = std::make_shared<HostImpl>([m
[32m+[m[32m      cluster_->info_, "test_host", Network::Utility::resolveUrl("tcp://127.0.0.1:80"), nullptr, 1,[m
[32m+[m[32m      envoy::config::core::v3::Locality(), health_check_config, 0, envoy::config::core::v3::UNKNOWN,[m
[32m+[m[32m      simTime());[m
[32m+[m[32m  cluster_->prioritySet().getMockHostSet(0)->hosts_ = {disable_host};[m
[32m+[m[32m  health_checker_->start();[m
[32m+[m[32m  EXPECT_CALL(*this, onHostStatus(_, HealthTransition::Unchanged)).Times(0);[m
[32m+[m[32m}[m
[32m+[m
 TEST_F(HttpHealthCheckerImplTest, ConnectionClose) {[m
   setupNoServiceValidationHC();[m
   EXPECT_CALL(*this, onHostStatus(_, HealthTransition::Unchanged));[m
[36m@@ -2538,7 +2586,7 @@[m [mTEST_F(HttpHealthCheckerImplTest, HealthCheckIntervals) {[m
   EXPECT_CALL(*test_sessions_[0]->interval_timer_, enableTimer(std::chrono::milliseconds(5000), _));[m
   EXPECT_CALL(*test_sessions_[0]->timeout_timer_, disableTimer());[m
   respond(0, "200", false);[m
[31m-  cluster_->info_->stats().upstream_cx_total_.inc();[m
[32m+[m[32m  cluster_->info_->trafficStats().upstream_cx_total_.inc();[m
 [m
   EXPECT_CALL(*test_sessions_[0]->timeout_timer_, enableTimer(_, _));[m
   // Needed after a response is sent.[m
[36m@@ -2877,7 +2925,7 @@[m [mTEST_F(HttpHealthCheckerImplTest, SuccessServiceCheckWithAltPort) {[m
   // Prepares a host with its designated health check port.[m
   const HostWithHealthCheckMap hosts{{"127.0.0.1:80", makeHealthCheckConfig(8000)}};[m
   appendTestHosts(cluster_, hosts);[m
[31m-  cluster_->info_->stats().upstream_cx_total_.inc();[m
[32m+[m[32m  cluster_->info_->trafficStats().upstream_cx_total_.inc();[m
   expectSessionCreate(hosts);[m
   expectStreamCreate(0);[m
   EXPECT_CALL(*test_sessions_[0]->timeout_timer_, enableTimer(_, _));[m
[36m@@ -2910,8 +2958,8 @@[m [mTEST_F(HttpHealthCheckerImplTest, SuccessWithMultipleHostsAndAltPort) {[m
   const HostWithHealthCheckMap hosts = {{"127.0.0.1:80", makeHealthCheckConfig(8000)},[m
                                         {"127.0.0.1:81", makeHealthCheckConfig(8001)}};[m
   appendTestHosts(cluster_, hosts);[m
[31m-  cluster_->info_->stats().upstream_cx_total_.inc();[m
[31m-  cluster_->info_->stats().upstream_cx_total_.inc();[m
[32m+[m[32m  cluster_->info_->trafficStats().upstream_cx_total_.inc();[m
[32m+[m[32m  cluster_->info_->trafficStats().upstream_cx_total_.inc();[m
   expectSessionCreate(hosts);[m
   expectStreamCreate(0);[m
   EXPECT_CALL(*test_sessions_[0]->timeout_timer_, enableTimer(_, _));[m
[36m@@ -2951,7 +2999,7 @@[m [mTEST_F(HttpHealthCheckerImplTest, SuccessServiceCheckWithAltAddress) {[m
   const HostWithHealthCheckMap hosts{[m
       {"127.0.0.1:80", makeHealthCheckConfigAltAddress("127.0.0.2", 8000)}};[m
   appendTestHosts(cluster_, hosts);[m
[31m-  cluster_->info_->stats().upstream_cx_total_.inc();[m
[32m+[m[32m  cluster_->info_->trafficStats().upstream_cx_total_.inc();[m
   expectSessionCreate(hosts);[m
   expectStreamCreate(0);[m
   EXPECT_CALL(*test_sessions_[0]->timeout_timer_, enableTimer(_, _));[m
[36m@@ -2985,8 +3033,8 @@[m [mTEST_F(HttpHealthCheckerImplTest, SuccessWithMultipleHostsAndAltAddress) {[m
       {"127.0.0.1:80", makeHealthCheckConfigAltAddress("127.0.0.2", 8000)},[m
       {"127.0.0.2:81", makeHealthCheckConfigAltAddress("127.0.0.2", 8000)}};[m
   appendTestHosts(cluster_, hosts);[m
[31m-  cluster_->info_->stats().upstream_cx_total_.inc();[m
[31m-  cluster_->info_->stats().upstream_cx_total_.inc();[m
[32m+[m[32m  cluster_->info_->trafficStats().upstream_cx_total_.inc();[m
[32m+[m[32m  cluster_->info_->trafficStats().upstream_cx_total_.inc();[m
   expectSessionCreate(hosts);[m
   expectStreamCreate(0);[m
   EXPECT_CALL(*test_sessions_[0]->timeout_timer_, enableTimer(_, _));[m
[36m@@ -3082,7 +3130,7 @@[m [mTEST_F(HttpHealthCheckerImplTest, TransportSocketMatchCriteria) {[m
 [m
   cluster_->prioritySet().getMockHostSet(0)->hosts_ = {[m
       makeTestHost(cluster_->info_, "tcp://127.0.0.1:80", simTime())};[m
[31m-  cluster_->info_->stats().upstream_cx_total_.inc();[m
[32m+[m[32m  cluster_->info_->trafficStats().upstream_cx_total_.inc();[m
   expectSessionCreate();[m
   expectStreamCreate(0);[m
   EXPECT_CALL(*test_sessions_[0]->timeout_timer_, enableTimer(_, _));[m
[36m@@ -3123,7 +3171,7 @@[m [mTEST_F(HttpHealthCheckerImplTest, NoTransportSocketMatchCriteria) {[m
 [m
   cluster_->prioritySet().getMockHostSet(0)->hosts_ = {[m
       makeTestHost(cluster_->info_, "tcp://127.0.0.1:80", simTime())};[m
[31m-  cluster_->info_->stats().upstream_cx_total_.inc();[m
[32m+[m[32m  cluster_->info_->trafficStats().upstream_cx_total_.inc();[m
   expectSessionCreate();[m
   expectStreamCreate(0);[m
   EXPECT_CALL(*test_sessions_[0]->timeout_timer_, enableTimer(_, _));[m
[36m@@ -3140,7 +3188,7 @@[m [mTEST_F(HttpHealthCheckerImplTest, GoAwayErrorProbeInProgress) {[m
 [m
   cluster_->prioritySet().getMockHostSet(0)->hosts_ = {[m
       makeTestHost(cluster_->info_, "tcp://127.0.0.1:80", simTime())};[m
[31m-  cluster_->info_->stats().upstream_cx_total_.inc();[m
[32m+[m[32m  cluster_->info_->trafficStats().upstream_cx_total_.inc();[m
   expectSessionCreate();[m
   expectStreamCreate(0);[m
   EXPECT_CALL(*test_sessions_[0]->timeout_timer_, enableTimer(_, _));[m
[36m@@ -3188,7 +3236,7 @@[m [mTEST_F(HttpHealthCheckerImplTest, GoAwayProbeInProgress) {[m
       .WillRepeatedly(Return(false));[m
   cluster_->prioritySet().getMockHostSet(0)->hosts_ = {[m
       makeTestHost(cluster_->info_, "tcp://127.0.0.1:80", simTime())};[m
[31m-  cluster_->info_->stats().upstream_cx_total_.inc();[m
[32m+[m[32m  cluster_->info_->trafficStats().upstream_cx_total_.inc();[m
 [m
   expectSessionCreate();[m
   expectStreamCreate(0);[m
[36m@@ -3487,7 +3535,7 @@[m [mTEST_F(HttpHealthCheckerImplTest, ServiceNameMatch) {[m
 [m
   cluster_->prioritySet().getMockHostSet(0)->hosts_ = {[m
       makeTestHost(cluster_->info_, "tcp://127.0.0.1:80", simTime())};[m
[31m-  cluster_->info_->stats().upstream_cx_total_.inc();[m
[32m+[m[32m  cluster_->info_->trafficStats().upstream_cx_total_.inc();[m
   expectSessionCreate();[m
   expectStreamCreate(0);[m
   EXPECT_CALL(*test_sessions_[0]->timeout_timer_, enableTimer(_, _));[m
[36m@@ -3523,7 +3571,7 @@[m [mTEST_F(HttpHealthCheckerImplTest, ServiceNameMismatch) {[m
 [m
   cluster_->prioritySet().getMockHostSet(0)->hosts_ = {[m
       makeTestHost(cluster_->info_, "tcp://127.0.0.1:80", simTime())};[m
[31m-  cluster_->info_->stats().upstream_cx_total_.inc();[m
[32m+[m[32m  cluster_->info_->trafficStats().upstream_cx_total_.inc();[m
   expectSessionCreate();[m
   expectStreamCreate(0);[m
   EXPECT_CALL(*test_sessions_[0]->timeout_timer_, enableTimer(_, _));[m
[36m@@ -3549,7 +3597,7 @@[m [mTEST_F(HttpHealthCheckerImplTest, DefaultMethodGet) {[m
 [m
   cluster_->prioritySet().getMockHostSet(0)->hosts_ = {[m
       makeTestHost(cluster_->info_, "tcp://127.0.0.1:80", simTime())};[m
[31m-  cluster_->info_->stats().upstream_cx_total_.inc();[m
[32m+[m[32m  cluster_->info_->trafficStats().upstream_cx_total_.inc();[m
   expectSessionCreate();[m
   expectStreamCreate(0);[m
   EXPECT_CALL(*test_sessions_[0]->timeout_timer_, enableTimer(_, _));[m
[36m@@ -3578,7 +3626,7 @@[m [mTEST_F(HttpHealthCheckerImplTest, MethodHead) {[m
 [m
   cluster_->prioritySet().getMockHostSet(0)->hosts_ = {[m
       makeTestHost(cluster_->info_, "tcp://127.0.0.1:80", simTime())};[m
[31m-  cluster_->info_->stats().upstream_cx_total_.inc();[m
[32m+[m[32m  cluster_->info_->trafficStats().upstream_cx_total_.inc();[m
   expectSessionCreate();[m
   expectStreamCreate(0);[m
   EXPECT_CALL(*test_sessions_[0]->timeout_timer_, enableTimer(_, _));[m
[36m@@ -4818,7 +4866,7 @@[m [mpublic:[m
   // performed during test case (but possibly on many hosts).[m
   void expectHealthchecks(HealthTransition host_changed_state, size_t num_healthchecks) {[m
     for (size_t i = 0; i < num_healthchecks; i++) {[m
[31m-      cluster_->info_->stats().upstream_cx_total_.inc();[m
[32m+[m[32m      cluster_->info_->trafficStats().upstream_cx_total_.inc();[m
       expectSessionCreate();[m
       expectHealthcheckStart(i);[m
     }[m
[36m@@ -4921,7 +4969,7 @@[m [mpublic:[m
 [m
   void runHealthCheck(std::string expected_host) {[m
 [m
[31m-    cluster_->info_->stats().upstream_cx_total_.inc();[m
[32m+[m[32m    cluster_->info_->trafficStats().upstream_cx_total_.inc();[m
 [m
     expectSessionCreate();[m
     expectHealthcheckStart(0);[m
[36m@@ -5071,7 +5119,7 @@[m [mTEST_F(GrpcHealthCheckerImplTest, SuccessWithAdditionalHeaders) {[m
   cluster_->prioritySet().getMockHostSet(0)->hosts_ = {[m
       makeTestHost(cluster_->info_, "tcp://127.0.0.1:80", metadata, simTime())};[m
 [m
[31m-  cluster_->info_->stats().upstream_cx_total_.inc();[m
[32m+[m[32m  cluster_->info_->trafficStats().upstream_cx_total_.inc();[m
 [m
   expectSessionCreate();[m
   expectHealthcheckStart(0);[m
[36m@@ -5543,7 +5591,7 @@[m [mTEST_F(GrpcHealthCheckerImplTest, HealthCheckIntervals) {[m
   EXPECT_CALL(*test_sessions_[0]->interval_timer_, enableTimer(std::chrono::milliseconds(5000), _));[m
   EXPECT_CALL(*test_sessions_[0]->timeout_timer_, disableTimer());[m
   respondServiceStatus(0, grpc::health::v1::HealthCheckResponse::SERVING);[m
[31m-  cluster_->info_->stats().upstream_cx_total_.inc();[m
[32m+[m[32m  cluster_->info_->trafficStats().upstream_cx_total_.inc();[m
 [m
   EXPECT_CALL(*test_sessions_[0]->timeout_timer_, enableTimer(_, _));[m
   // Needed after a response is sent.[m
[1mdiff --git a/test/common/upstream/load_balancer_benchmark.cc b/test/common/upstream/load_balancer_benchmark.cc[m
[1mindex 6a855bae9a..bf7feac8fe 100644[m
[1m--- a/test/common/upstream/load_balancer_benchmark.cc[m
[1m+++ b/test/common/upstream/load_balancer_benchmark.cc[m
[36m@@ -66,8 +66,8 @@[m [mpublic:[m
   PrioritySetImpl priority_set_;[m
   PrioritySetImpl local_priority_set_;[m
   Stats::IsolatedStoreImpl stats_store_;[m
[31m-  ClusterStatNames stat_names_{stats_store_.symbolTable()};[m
[31m-  ClusterStats stats_{ClusterInfoImpl::generateStats(stats_store_, stat_names_)};[m
[32m+[m[32m  ClusterLbStatNames stat_names_{stats_store_.symbolTable()};[m
[32m+[m[32m  ClusterLbStats stats_{stat_names_, stats_store_};[m
   NiceMock<Runtime::MockLoader> runtime_;[m
   Random::RandomGeneratorImpl random_;[m
   envoy::config::cluster::v3::Cluster::CommonLbConfig common_config_;[m
[1mdiff --git a/test/common/upstream/load_balancer_fuzz_base.h b/test/common/upstream/load_balancer_fuzz_base.h[m
[1mindex 727da68904..f8a33319e0 100644[m
[1m--- a/test/common/upstream/load_balancer_fuzz_base.h[m
[1m+++ b/test/common/upstream/load_balancer_fuzz_base.h[m
[36m@@ -21,8 +21,7 @@[m [mnamespace Upstream {[m
 class LoadBalancerFuzzBase {[m
 public:[m
   LoadBalancerFuzzBase()[m
[31m-      : stat_names_(stats_store_.symbolTable()),[m
[31m-        stats_(ClusterInfoImpl::generateStats(stats_store_, stat_names_)){};[m
[32m+[m[32m      : stat_names_(stats_store_.symbolTable()), stats_(stat_names_, stats_store_){};[m
 [m
   // Initializes load balancer components shared amongst every load balancer, random_, and[m
   // priority_set_[m
[36m@@ -42,8 +41,8 @@[m [mpublic:[m
   // These public objects shared amongst all types of load balancers will be used to construct load[m
   // balancers in specific load balancer fuzz classes[m
   Stats::IsolatedStoreImpl stats_store_;[m
[31m-  ClusterStatNames stat_names_;[m
[31m-  ClusterStats stats_;[m
[32m+[m[32m  ClusterLbStatNames stat_names_;[m
[32m+[m[32m  ClusterLbStats stats_;[m
   NiceMock<Runtime::MockLoader> runtime_;[m
   Random::PsuedoRandomGenerator64 random_;[m
   NiceMock<MockPrioritySet> priority_set_;[m
[1mdiff --git a/test/common/upstream/load_balancer_impl_test.cc b/test/common/upstream/load_balancer_impl_test.cc[m
[1mindex d657bcd74f..b503014a30 100644[m
[1m--- a/test/common/upstream/load_balancer_impl_test.cc[m
[1m+++ b/test/common/upstream/load_balancer_impl_test.cc[m
[36m@@ -54,10 +54,11 @@[m [mpublic:[m
 class TestZoneAwareLoadBalancer : public ZoneAwareLoadBalancerBase {[m
 public:[m
   TestZoneAwareLoadBalancer([m
[31m-      const PrioritySet& priority_set, ClusterStats& stats, Runtime::Loader& runtime,[m
[32m+[m[32m      const PrioritySet& priority_set, ClusterLbStats& lb_stats, Runtime::Loader& runtime,[m
       Random::RandomGenerator& random,[m
       const envoy::config::cluster::v3::Cluster::CommonLbConfig& common_config)[m
[31m-      : ZoneAwareLoadBalancerBase(priority_set, nullptr, stats, runtime, random, common_config) {}[m
[32m+[m[32m      : ZoneAwareLoadBalancerBase(priority_set, nullptr, lb_stats, runtime, random, common_config) {[m
[32m+[m[32m  }[m
   void runInvalidLocalitySourceType() {[m
     localitySourceType(static_cast<LoadBalancerBase::HostAvailability>(123));[m
   }[m
[36m@@ -76,14 +77,13 @@[m [mprotected:[m
   MockHostSet& hostSet() { return GetParam() ? host_set_ : failover_host_set_; }[m
 [m
   LoadBalancerTestBase()[m
[31m-      : stat_names_(stats_store_.symbolTable()),[m
[31m-        stats_(ClusterInfoImpl::generateStats(stats_store_, stat_names_)) {[m
[32m+[m[32m      : stat_names_(stats_store_.symbolTable()), stats_(stat_names_, stats_store_) {[m
     least_request_lb_config_.mutable_choice_count()->set_value(2);[m
   }[m
 [m
   Stats::IsolatedStoreImpl stats_store_;[m
[31m-  ClusterStatNames stat_names_;[m
[31m-  ClusterStats stats_;[m
[32m+[m[32m  ClusterLbStatNames stat_names_;[m
[32m+[m[32m  ClusterLbStats stats_;[m
   NiceMock<Runtime::MockLoader> runtime_;[m
   NiceMock<Random::MockRandomGenerator> random_;[m
   NiceMock<MockPrioritySet> priority_set_;[m
[36m@@ -97,10 +97,10 @@[m [mprotected:[m
 [m
 class TestLb : public LoadBalancerBase {[m
 public:[m
[31m-  TestLb(const PrioritySet& priority_set, ClusterStats& stats, Runtime::Loader& runtime,[m
[32m+[m[32m  TestLb(const PrioritySet& priority_set, ClusterLbStats& lb_stats, Runtime::Loader& runtime,[m
          Random::RandomGenerator& random,[m
          const envoy::config::cluster::v3::Cluster::CommonLbConfig& common_config)[m
[31m-      : LoadBalancerBase(priority_set, stats, runtime, random, common_config) {}[m
[32m+[m[32m      : LoadBalancerBase(priority_set, lb_stats, runtime, random, common_config) {}[m
   using LoadBalancerBase::chooseHostSet;[m
   using LoadBalancerBase::isInPanic;[m
   using LoadBalancerBase::percentageDegradedLoad;[m
[36m@@ -581,10 +581,11 @@[m [mTEST_P(LoadBalancerBaseTest, BoundaryConditions) {[m
 [m
 class TestZoneAwareLb : public ZoneAwareLoadBalancerBase {[m
 public:[m
[31m-  TestZoneAwareLb(const PrioritySet& priority_set, ClusterStats& stats, Runtime::Loader& runtime,[m
[31m-                  Random::RandomGenerator& random,[m
[32m+[m[32m  TestZoneAwareLb(const PrioritySet& priority_set, ClusterLbStats& lb_stats,[m
[32m+[m[32m                  Runtime::Loader& runtime, Random::RandomGenerator& random,[m
                   const envoy::config::cluster::v3::Cluster::CommonLbConfig& common_config)[m
[31m-      : ZoneAwareLoadBalancerBase(priority_set, nullptr, stats, runtime, random, common_config) {}[m
[32m+[m[32m      : ZoneAwareLoadBalancerBase(priority_set, nullptr, lb_stats, runtime, random, common_config) {[m
[32m+[m[32m  }[m
 [m
   HostConstSharedPtr chooseHostOnce(LoadBalancerContext*) override {[m
     return choose_host_once_host_;[m
[36m@@ -1959,14 +1960,12 @@[m [mTEST_P(LeastRequestLoadBalancerTest, SingleHost) {[m
   // Host weight is 1.[m
   {[m
     EXPECT_CALL(random_, random()).WillOnce(Return(0)).WillOnce(Return(2)).WillOnce(Return(3));[m
[31m-    stats_.max_host_weight_.set(1UL);[m
     EXPECT_EQ(hostSet().healthy_hosts_[0], lb_.chooseHost(nullptr));[m
   }[m
 [m
   // Host weight is 100.[m
   {[m
     EXPECT_CALL(random_, random()).WillOnce(Return(0)).WillOnce(Return(2)).WillOnce(Return(3));[m
[31m-    stats_.max_host_weight_.set(100UL);[m
     EXPECT_EQ(hostSet().healthy_hosts_[0], lb_.chooseHost(nullptr));[m
   }[m
 [m
[36m@@ -1991,7 +1990,6 @@[m [mTEST_P(LeastRequestLoadBalancerTest, SingleHost) {[m
 TEST_P(LeastRequestLoadBalancerTest, Normal) {[m
   hostSet().healthy_hosts_ = {makeTestHost(info_, "tcp://127.0.0.1:80", simTime()),[m
                               makeTestHost(info_, "tcp://127.0.0.1:81", simTime())};[m
[31m-  stats_.max_host_weight_.set(1UL);[m
   hostSet().hosts_ = hostSet().healthy_hosts_;[m
   hostSet().runCallbacks({}, {}); // Trigger callbacks. The added/removed lists are not relevant.[m
 [m
[36m@@ -2011,7 +2009,6 @@[m [mTEST_P(LeastRequestLoadBalancerTest, PNC) {[m
                               makeTestHost(info_, "tcp://127.0.0.1:81", simTime()),[m
                               makeTestHost(info_, "tcp://127.0.0.1:82", simTime()),[m
                               makeTestHost(info_, "tcp://127.0.0.1:83", simTime())};[m
[31m-  stats_.max_host_weight_.set(1UL);[m
   hostSet().hosts_ = hostSet().healthy_hosts_;[m
   hostSet().runCallbacks({}, {}); // Trigger callbacks. The added/removed lists are not relevant.[m
 [m
[36m@@ -2058,7 +2055,6 @@[m [mTEST_P(LeastRequestLoadBalancerTest, PNC) {[m
 TEST_P(LeastRequestLoadBalancerTest, WeightImbalance) {[m
   hostSet().healthy_hosts_ = {makeTestHost(info_, "tcp://127.0.0.1:80", simTime(), 1),[m
                               makeTestHost(info_, "tcp://127.0.0.1:81", simTime(), 2)};[m
[31m-  stats_.max_host_weight_.set(2UL);[m
 [m
   hostSet().hosts_ = hostSet().healthy_hosts_;[m
   hostSet().runCallbacks({}, {}); // Trigger callbacks. The added/removed lists are not relevant.[m
[36m@@ -2183,7 +2179,6 @@[m [mTEST_P(LeastRequestLoadBalancerTest, WeightImbalanceWithCustomActiveRequestBias)[m
 TEST_P(LeastRequestLoadBalancerTest, WeightImbalanceCallbacks) {[m
   hostSet().healthy_hosts_ = {makeTestHost(info_, "tcp://127.0.0.1:80", simTime(), 1),[m
                               makeTestHost(info_, "tcp://127.0.0.1:81", simTime(), 2)};[m
[31m-  stats_.max_host_weight_.set(2UL);[m
 [m
   hostSet().hosts_ = hostSet().healthy_hosts_;[m
   hostSet().runCallbacks({}, {}); // Trigger callbacks. The added/removed lists are not relevant.[m
[1mdiff --git a/test/common/upstream/load_balancer_simulation_test.cc b/test/common/upstream/load_balancer_simulation_test.cc[m
[1mindex 22d081562f..600cff25d4 100644[m
[1m--- a/test/common/upstream/load_balancer_simulation_test.cc[m
[1m+++ b/test/common/upstream/load_balancer_simulation_test.cc[m
[36m@@ -70,16 +70,15 @@[m [mTEST(DISABLED_LeastRequestLoadBalancerWeightTest, Weight) {[m
       {}, hosts, {}, absl::nullopt);[m
 [m
   Stats::IsolatedStoreImpl stats_store;[m
[31m-  ClusterStatNames stat_names(stats_store.symbolTable());[m
[31m-  ClusterStats stats{ClusterInfoImpl::generateStats(stats_store, stat_names)};[m
[31m-  stats.max_host_weight_.set(weight);[m
[32m+[m[32m  ClusterLbStatNames stat_names(stats_store.symbolTable());[m
[32m+[m[32m  ClusterLbStats lb_stats{stat_names, stats_store};[m
   NiceMock<Runtime::MockLoader> runtime;[m
   auto time_source = std::make_unique<NiceMock<MockTimeSystem>>();[m
   Random::RandomGeneratorImpl random;[m
   envoy::config::cluster::v3::Cluster::LeastRequestLbConfig least_request_lb_config;[m
   envoy::config::cluster::v3::Cluster::CommonLbConfig common_config;[m
   LeastRequestLoadBalancer lb_{[m
[31m-      priority_set, nullptr, stats, runtime, random, common_config, least_request_lb_config,[m
[32m+[m[32m      priority_set, nullptr, lb_stats, runtime, random, common_config, least_request_lb_config,[m
       *time_source};[m
 [m
   absl::node_hash_map<HostConstSharedPtr, uint64_t> host_hits;[m
[36m@@ -105,11 +104,10 @@[m [mTEST(DISABLED_LeastRequestLoadBalancerWeightTest, Weight) {[m
 /**[m
  * This test is for simulation only and should not be run as part of unit tests.[m
  */[m
[31m-class DISABLED_SimulationTest : public testing::Test {[m
[32m+[m[32mclass DISABLED_SimulationTest : public testing::Test { // NOLINT(readability-identifier-naming)[m
 public:[m
   DISABLED_SimulationTest()[m
[31m-      : stat_names_(stats_store_.symbolTable()),[m
[31m-        stats_(ClusterInfoImpl::generateStats(stats_store_, stat_names_)) {[m
[32m+[m[32m      : stat_names_(stats_store_.symbolTable()), stats_(stat_names_, stats_store_) {[m
     ON_CALL(runtime_.snapshot_, getInteger("upstream.healthy_panic_threshold", 50U))[m
         .WillByDefault(Return(50U));[m
     ON_CALL(runtime_.snapshot_, featureEnabled("upstream.zone_routing.enabled", 100))[m
[36m@@ -247,8 +245,8 @@[m [mpublic:[m
   NiceMock<MockTimeSystem> time_source_;[m
   Random::RandomGeneratorImpl random_;[m
   Stats::IsolatedStoreImpl stats_store_;[m
[31m-  ClusterStatNames stat_names_;[m
[31m-  ClusterStats stats_;[m
[32m+[m[32m  ClusterLbStatNames stat_names_;[m
[32m+[m[32m  ClusterLbStats stats_;[m
   envoy::config::cluster::v3::Cluster::CommonLbConfig common_config_;[m
 };[m
 [m
[1mdiff --git a/test/common/upstream/maglev_lb_test.cc b/test/common/upstream/maglev_lb_test.cc[m
[1mindex b9aef9fabf..a668aafaf7 100644[m
[1m--- a/test/common/upstream/maglev_lb_test.cc[m
[1m+++ b/test/common/upstream/maglev_lb_test.cc[m
[36m@@ -45,8 +45,7 @@[m [mpublic:[m
 class MaglevLoadBalancerTest : public Event::TestUsingSimulatedTime, public testing::Test {[m
 public:[m
   MaglevLoadBalancerTest()[m
[31m-      : stat_names_(stats_store_.symbolTable()),[m
[31m-        stats_(ClusterInfoImpl::generateStats(stats_store_, stat_names_)) {}[m
[32m+[m[32m      : stat_names_(stats_store_.symbolTable()), stats_(stat_names_, stats_store_) {}[m
 [m
   void createLb() {[m
     lb_ = std::make_unique<MaglevLoadBalancer>(priority_set_, stats_, stats_store_, runtime_,[m
[36m@@ -65,8 +64,8 @@[m [mpublic:[m
   MockHostSet& host_set_ = *priority_set_.getMockHostSet(0);[m
   std::shared_ptr<MockClusterInfo> info_{new NiceMock<MockClusterInfo>()};[m
   Stats::IsolatedStoreImpl stats_store_;[m
[31m-  ClusterStatNames stat_names_;[m
[31m-  ClusterStats stats_;[m
[32m+[m[32m  ClusterLbStatNames stat_names_;[m
[32m+[m[32m  ClusterLbStats stats_;[m
   absl::optional<envoy::config::cluster::v3::Cluster::MaglevLbConfig> config_;[m
   envoy::config::cluster::v3::Cluster::CommonLbConfig common_config_;[m
   NiceMock<Runtime::MockLoader> runtime_;[m
[1mdiff --git a/test/common/upstream/ring_hash_lb_test.cc b/test/common/upstream/ring_hash_lb_test.cc[m
[1mindex fa76bdf721..84a24104c1 100644[m
[1m--- a/test/common/upstream/ring_hash_lb_test.cc[m
[1m+++ b/test/common/upstream/ring_hash_lb_test.cc[m
[36m@@ -58,8 +58,7 @@[m [mclass RingHashLoadBalancerTest : public Event::TestUsingSimulatedTime,[m
                                  public testing::TestWithParam<bool> {[m
 public:[m
   RingHashLoadBalancerTest()[m
[31m-      : stat_names_(stats_store_.symbolTable()),[m
[31m-        stats_(ClusterInfoImpl::generateStats(stats_store_, stat_names_)) {}[m
[32m+[m[32m      : stat_names_(stats_store_.symbolTable()), stats_(stat_names_, stats_store_) {}[m
 [m
   void init() {[m
     lb_ = std::make_unique<RingHashLoadBalancer>(priority_set_, stats_, stats_store_, runtime_,[m
[36m@@ -76,8 +75,8 @@[m [mpublic:[m
   MockHostSet& failover_host_set_ = *priority_set_.getMockHostSet(1);[m
   std::shared_ptr<MockClusterInfo> info_{new NiceMock<MockClusterInfo>()};[m
   Stats::IsolatedStoreImpl stats_store_;[m
[31m-  ClusterStatNames stat_names_;[m
[31m-  ClusterStats stats_;[m
[32m+[m[32m  ClusterLbStatNames stat_names_;[m
[32m+[m[32m  ClusterLbStats stats_;[m
   absl::optional<envoy::config::cluster::v3::Cluster::RingHashLbConfig> config_;[m
   envoy::config::cluster::v3::Cluster::CommonLbConfig common_config_;[m
   NiceMock<Runtime::MockLoader> runtime_;[m
[1mdiff --git a/test/common/upstream/subset_lb_test.cc b/test/common/upstream/subset_lb_test.cc[m
[1mindex 68c24dfda7..6011423dd3 100644[m
[1m--- a/test/common/upstream/subset_lb_test.cc[m
[1m+++ b/test/common/upstream/subset_lb_test.cc[m
[36m@@ -170,8 +170,7 @@[m [mclass SubsetLoadBalancerTest : public Event::TestUsingSimulatedTime,[m
 public:[m
   SubsetLoadBalancerTest()[m
       : scope_(stats_store_.createScope("testprefix")), stat_names_(stats_store_.symbolTable()),[m
[31m-        stats_(ClusterInfoImpl::generateStats(stats_store_, stat_names_)) {[m
[31m-    stats_.max_host_weight_.set(1UL);[m
[32m+[m[32m        stats_(stat_names_, stats_store_) {[m
     least_request_lb_config_.mutable_choice_count()->set_value(2);[m
   }[m
 [m
[36m@@ -528,8 +527,8 @@[m [mpublic:[m
   NiceMock<Random::MockRandomGenerator> random_;[m
   Stats::IsolatedStoreImpl stats_store_;[m
   Stats::ScopeSharedPtr scope_;[m
[31m-  ClusterStatNames stat_names_;[m
[31m-  ClusterStats stats_;[m
[32m+[m[32m  ClusterLbStatNames stat_names_;[m
[32m+[m[32m  ClusterLbStats stats_;[m
   PrioritySetImpl local_priority_set_;[m
   HostVectorSharedPtr local_hosts_;[m
   HostsPerLocalitySharedPtr local_hosts_per_locality_;[m
[1mdiff --git a/test/common/upstream/upstream_impl_test.cc b/test/common/upstream/upstream_impl_test.cc[m
[1mindex 837a8e3c13..0d2a0a9beb 100644[m
[1m--- a/test/common/upstream/upstream_impl_test.cc[m
[1m+++ b/test/common/upstream/upstream_impl_test.cc[m
[36m@@ -420,7 +420,7 @@[m [mTEST_F(StrictDnsClusterImplTest, Basic) {[m
   EXPECT_EQ(1U, cluster.info()->resourceManager(ResourcePriority::Default).maxConnectionsPerHost());[m
   EXPECT_EQ(990U, cluster.info()->resourceManager(ResourcePriority::High).maxConnectionsPerHost());[m
 [m
[31m-  cluster.info()->stats().upstream_rq_total_.inc();[m
[32m+[m[32m  cluster.info()->trafficStats().upstream_rq_total_.inc();[m
   EXPECT_EQ(1UL, stats_.counter("cluster.name.upstream_rq_total").value());[m
 [m
   EXPECT_CALL(runtime_.snapshot_, featureEnabled("upstream.maintenance_mode.name", 0));[m
[36m@@ -673,6 +673,73 @@[m [mTEST_F(StrictDnsClusterImplTest, HostRemovalAfterHcFail) {[m
   }[m
 }[m
 [m
[32m+[m[32mTEST_F(StrictDnsClusterImplTest, HostUpdateWithDisabledACEndpoint) {[m
[32m+[m[32m  const std::string yaml = R"EOF([m
[32m+[m[32m    name: name[m
[32m+[m[32m    connect_timeout: 0.25s[m
[32m+[m[32m    type: STRICT_DNS[m
[32m+[m[32m    lb_policy: ROUND_ROBIN[m
[32m+[m[32m    load_assignment:[m
[32m+[m[32m        endpoints:[m
[32m+[m[32m          - lb_endpoints:[m
[32m+[m[32m            - endpoint:[m
[32m+[m[32m                address:[m
[32m+[m[32m                  socket_address:[m
[32m+[m[32m                    address: foo.bar.com[m
[32m+[m[32m                    port_value: 443[m
[32m+[m[32m                health_check_config:[m
[32m+[m[32m                  disable_active_health_check: true[m
[32m+[m[32m  )EOF";[m
[32m+[m
[32m+[m[32m  ResolverData resolver(*dns_resolver_, server_context_.dispatcher_);[m
[32m+[m[32m  envoy::config::cluster::v3::Cluster cluster_config = parseClusterFromV3Yaml(yaml);[m
[32m+[m[32m  Envoy::Stats::ScopeSharedPtr scope = stats_.createScope(fmt::format([m
[32m+[m[32m      "cluster.{}.", cluster_config.alt_stat_name().empty() ? cluster_config.name()[m
[32m+[m[32m                                                            : cluster_config.alt_stat_name()));[m
[32m+[m[32m  Envoy::Server::Configuration::TransportSocketFactoryContextImpl factory_context([m
[32m+[m[32m      server_context_, ssl_context_manager_, *scope, server_context_.cluster_manager_, stats_,[m
[32m+[m[32m      validation_visitor_);[m
[32m+[m[32m  StrictDnsClusterImpl cluster(server_context_, cluster_config, runtime_, dns_resolver_,[m
[32m+[m[32m                               factory_context, std::move(scope), false);[m
[32m+[m[32m  std::shared_ptr<MockHealthChecker> health_checker(new MockHealthChecker());[m
[32m+[m[32m  EXPECT_CALL(*health_checker, start());[m
[32m+[m[32m  EXPECT_CALL(*health_checker, addHostCheckCompleteCb(_));[m
[32m+[m[32m  cluster.setHealthChecker(health_checker);[m
[32m+[m[32m  ReadyWatcher initialized;[m
[32m+[m[32m  cluster.initialize([&initialized]() { initialized.ready(); });[m
[32m+[m[32m  EXPECT_CALL(initialized, ready());[m
[32m+[m
[32m+[m[32m  EXPECT_CALL(*health_checker, addHostCheckCompleteCb(_));[m
[32m+[m[32m  EXPECT_CALL(*resolver.timer_, enableTimer(_, _)).Times(2);[m
[32m+[m[32m  resolver.dns_callback_(Network::DnsResolver::ResolutionStatus::Success,[m
[32m+[m[32m                         TestUtility::makeDnsResponse({"127.0.0.1", "127.0.0.2"}));[m
[32m+[m
[32m+[m[32m  {[m
[32m+[m[32m    const auto& hosts = cluster.prioritySet().hostSetsPerPriority()[0]->hosts();[m
[32m+[m[32m    EXPECT_EQ(2UL, hosts.size());[m
[32m+[m[32m    EXPECT_FALSE(hosts[0]->healthFlagGet(Host::HealthFlag::FAILED_ACTIVE_HC));[m
[32m+[m[32m    EXPECT_FALSE(hosts[1]->healthFlagGet(Host::HealthFlag::FAILED_ACTIVE_HC));[m
[32m+[m
[32m+[m[32m    EXPECT_EQ(2UL, cluster.prioritySet().hostSetsPerPriority()[0]->healthyHosts().size());[m
[32m+[m[32m    EXPECT_EQ(2UL, cluster.info()->endpointStats().membership_healthy_.value());[m
[32m+[m[32m    EXPECT_EQ(0UL, cluster.info()->endpointStats().membership_degraded_.value());[m
[32m+[m[32m  }[m
[32m+[m
[32m+[m[32m  // Re-resolve the DNS name with only one record, we should have 1 host.[m
[32m+[m[32m  resolver.dns_callback_(Network::DnsResolver::ResolutionStatus::Success,[m
[32m+[m[32m                         TestUtility::makeDnsResponse({"127.0.0.1"}));[m
[32m+[m
[32m+[m[32m  {[m
[32m+[m[32m    const auto& hosts = cluster.prioritySet().hostSetsPerPriority()[0]->hosts();[m
[32m+[m[32m    EXPECT_EQ(1UL, hosts.size());[m
[32m+[m[32m    EXPECT_FALSE(hosts[0]->healthFlagGet(Host::HealthFlag::PENDING_DYNAMIC_REMOVAL));[m
[32m+[m[32m    EXPECT_FALSE(hosts[0]->healthFlagGet(Host::HealthFlag::FAILED_ACTIVE_HC));[m
[32m+[m[32m    EXPECT_EQ(1UL, cluster.prioritySet().hostSetsPerPriority()[0]->healthyHosts().size());[m
[32m+[m[32m    EXPECT_EQ(1UL, cluster.info()->endpointStats().membership_healthy_.value());[m
[32m+[m[32m    EXPECT_EQ(0UL, cluster.info()->endpointStats().membership_degraded_.value());[m
[32m+[m[32m  }[m
[32m+[m[32m}[m
[32m+[m
 TEST_F(StrictDnsClusterImplTest, LoadAssignmentBasic) {[m
   // gmock matches in LIFO order which is why these are swapped.[m
   ResolverData resolver3(*dns_resolver_, server_context_.dispatcher_);[m
[36m@@ -770,7 +837,7 @@[m [mTEST_F(StrictDnsClusterImplTest, LoadAssignmentBasic) {[m
   EXPECT_EQ(3U, cluster.info()->maxRequestsPerConnection());[m
   EXPECT_EQ(0U, cluster.info()->http2Options().hpack_table_size().value());[m
 [m
[31m-  cluster.info()->stats().upstream_rq_total_.inc();[m
[32m+[m[32m  cluster.info()->trafficStats().upstream_rq_total_.inc();[m
   EXPECT_EQ(1UL, stats_.counter("cluster.name.upstream_rq_total").value());[m
 [m
   EXPECT_CALL(runtime_.snapshot_, featureEnabled("upstream.maintenance_mode.name", 0));[m
[36m@@ -1507,6 +1574,16 @@[m [mTEST_F(HostImplTest, HealthStatus) {[m
   EXPECT_EQ(Host::HealthStatus::DEGRADED, host->healthStatus());[m
 }[m
 [m
[32m+[m[32mTEST_F(HostImplTest, SkipActiveHealthCheckFlag) {[m
[32m+[m[32m  MockClusterMockPrioritySet cluster;[m
[32m+[m[32m  HostSharedPtr host = makeTestHost(cluster.info_, "tcp://10.0.0.1:1234", simTime(), 1);[m
[32m+[m
[32m+[m[32m  // To begin with, the default setting is false.[m
[32m+[m[32m  EXPECT_EQ(false, host->disableActiveHealthCheck());[m
[32m+[m[32m  host->setDisableActiveHealthCheck(true);[m
[32m+[m[32m  EXPECT_EQ(true, host->disableActiveHealthCheck());[m
[32m+[m[32m}[m
[32m+[m
 // Test that it's not possible to do a HostDescriptionImpl with a unix[m
 // domain socket host and a health check config with non-zero port.[m
 // This is a regression test for oss-fuzz issue[m
[36m@@ -1572,6 +1649,8 @@[m [mTEST_F(StaticClusterImplTest, InitialHosts) {[m
   cluster.initialize([] {});[m
 [m
   EXPECT_EQ(1UL, cluster.prioritySet().hostSetsPerPriority()[0]->healthyHosts().size());[m
[32m+[m[32m  const auto& hosts = cluster.prioritySet().hostSetsPerPriority()[0]->hosts();[m
[32m+[m[32m  EXPECT_FALSE(hosts[0]->healthFlagGet(Host::HealthFlag::FAILED_ACTIVE_HC));[m
   EXPECT_EQ("", cluster.prioritySet().hostSetsPerPriority()[0]->hosts()[0]->hostname());[m
   EXPECT_FALSE(cluster.info()->addedViaApi());[m
 }[m
[36m@@ -1859,7 +1938,7 @@[m [mTEST_F(StaticClusterImplTest, AltStatName) {[m
                             std::move(scope), false);[m
   cluster.initialize([] {});[m
   // Increment a stat and verify it is emitted with alt_stat_name[m
[31m-  cluster.info()->stats().upstream_rq_total_.inc();[m
[32m+[m[32m  cluster.info()->trafficStats().upstream_rq_total_.inc();[m
   EXPECT_EQ(1UL, stats_.counter("cluster.staticcluster_stats.upstream_rq_total").value());[m
 }[m
 [m
[36m@@ -2018,7 +2097,7 @@[m [mTEST_F(StaticClusterImplTest, OutlierDetector) {[m
   cluster.initialize([] {});[m
 [m
   EXPECT_EQ(2UL, cluster.prioritySet().hostSetsPerPriority()[0]->healthyHosts().size());[m
[31m-  EXPECT_EQ(2UL, cluster.info()->stats().membership_healthy_.value());[m
[32m+[m[32m  EXPECT_EQ(2UL, cluster.info()->endpointStats().membership_healthy_.value());[m
 [m
   // Set a single host as having failed and fire outlier detector callbacks. This should result[m
   // in only a single healthy host.[m
[36m@@ -2028,7 +2107,7 @@[m [mTEST_F(StaticClusterImplTest, OutlierDetector) {[m
       Host::HealthFlag::FAILED_OUTLIER_CHECK);[m
   detector->runCallbacks(cluster.prioritySet().hostSetsPerPriority()[0]->hosts()[0]);[m
   EXPECT_EQ(1UL, cluster.prioritySet().hostSetsPerPriority()[0]->healthyHosts().size());[m
[31m-  EXPECT_EQ(1UL, cluster.info()->stats().membership_healthy_.value());[m
[32m+[m[32m  EXPECT_EQ(1UL, cluster.info()->endpointStats().membership_healthy_.value());[m
   EXPECT_NE(cluster.prioritySet().hostSetsPerPriority()[0]->healthyHosts()[0],[m
             cluster.prioritySet().hostSetsPerPriority()[0]->hosts()[0]);[m
 [m
[36m@@ -2037,7 +2116,7 @@[m [mTEST_F(StaticClusterImplTest, OutlierDetector) {[m
       Host::HealthFlag::FAILED_OUTLIER_CHECK);[m
   detector->runCallbacks(cluster.prioritySet().hostSetsPerPriority()[0]->hosts()[0]);[m
   EXPECT_EQ(2UL, cluster.prioritySet().hostSetsPerPriority()[0]->healthyHosts().size());[m
[31m-  EXPECT_EQ(2UL, cluster.info()->stats().membership_healthy_.value());[m
[32m+[m[32m  EXPECT_EQ(2UL, cluster.info()->endpointStats().membership_healthy_.value());[m
 }[m
 [m
 TEST_F(StaticClusterImplTest, HealthyStat) {[m
[36m@@ -2082,8 +2161,8 @@[m [mTEST_F(StaticClusterImplTest, HealthyStat) {[m
 [m
   EXPECT_EQ(2UL, cluster.prioritySet().hostSetsPerPriority()[0]->hosts().size());[m
   EXPECT_EQ(0UL, cluster.prioritySet().hostSetsPerPriority()[0]->healthyHosts().size());[m
[31m-  EXPECT_EQ(0UL, cluster.info()->stats().membership_healthy_.value());[m
[31m-  EXPECT_EQ(0UL, cluster.info()->stats().membership_degraded_.value());[m
[32m+[m[32m  EXPECT_EQ(0UL, cluster.info()->endpointStats().membership_healthy_.value());[m
[32m+[m[32m  EXPECT_EQ(0UL, cluster.info()->endpointStats().membership_degraded_.value());[m
 [m
   cluster.prioritySet().hostSetsPerPriority()[0]->hosts()[0]->healthFlagClear([m
       Host::HealthFlag::FAILED_ACTIVE_HC);[m
[36m@@ -2099,46 +2178,46 @@[m [mTEST_F(StaticClusterImplTest, HealthyStat) {[m
       Host::HealthFlag::FAILED_OUTLIER_CHECK);[m
   outlier_detector->runCallbacks(cluster.prioritySet().hostSetsPerPriority()[0]->hosts()[0]);[m
   EXPECT_EQ(1UL, cluster.prioritySet().hostSetsPerPriority()[0]->healthyHosts().size());[m
[31m-  EXPECT_EQ(1UL, cluster.info()->stats().membership_healthy_.value());[m
[31m-  EXPECT_EQ(0UL, cluster.info()->stats().membership_degraded_.value());[m
[32m+[m[32m  EXPECT_EQ(1UL, cluster.info()->endpointStats().membership_healthy_.value());[m
[32m+[m[32m  EXPECT_EQ(0UL, cluster.info()->endpointStats().membership_degraded_.value());[m
 [m
   cluster.prioritySet().hostSetsPerPriority()[0]->hosts()[0]->healthFlagSet([m
       Host::HealthFlag::FAILED_ACTIVE_HC);[m
   health_checker->runCallbacks(cluster.prioritySet().hostSetsPerPriority()[0]->hosts()[0],[m
                                HealthTransition::Changed);[m
   EXPECT_EQ(1UL, cluster.prioritySet().hostSetsPerPriority()[0]->healthyHosts().size());[m
[31m-  EXPECT_EQ(1UL, cluster.info()->stats().membership_healthy_.value());[m
[31m-  EXPECT_EQ(0UL, cluster.info()->stats().membership_degraded_.value());[m
[32m+[m[32m  EXPECT_EQ(1UL, cluster.info()->endpointStats().membership_healthy_.value());[m
[32m+[m[32m  EXPECT_EQ(0UL, cluster.info()->endpointStats().membership_degraded_.value());[m
 [m
   cluster.prioritySet().hostSetsPerPriority()[0]->hosts()[0]->healthFlagClear([m
       Host::HealthFlag::FAILED_OUTLIER_CHECK);[m
   outlier_detector->runCallbacks(cluster.prioritySet().hostSetsPerPriority()[0]->hosts()[0]);[m
   EXPECT_EQ(1UL, cluster.prioritySet().hostSetsPerPriority()[0]->healthyHosts().size());[m
[31m-  EXPECT_EQ(1UL, cluster.info()->stats().membership_healthy_.value());[m
[31m-  EXPECT_EQ(0UL, cluster.info()->stats().membership_degraded_.value());[m
[32m+[m[32m  EXPECT_EQ(1UL, cluster.info()->endpointStats().membership_healthy_.value());[m
[32m+[m[32m  EXPECT_EQ(0UL, cluster.info()->endpointStats().membership_degraded_.value());[m
 [m
   cluster.prioritySet().hostSetsPerPriority()[0]->hosts()[0]->healthFlagClear([m
       Host::HealthFlag::FAILED_ACTIVE_HC);[m
   health_checker->runCallbacks(cluster.prioritySet().hostSetsPerPriority()[0]->hosts()[0],[m
                                HealthTransition::Changed);[m
   EXPECT_EQ(2UL, cluster.prioritySet().hostSetsPerPriority()[0]->healthyHosts().size());[m
[31m-  EXPECT_EQ(2UL, cluster.info()->stats().membership_healthy_.value());[m
[31m-  EXPECT_EQ(0UL, cluster.info()->stats().membership_degraded_.value());[m
[32m+[m[32m  EXPECT_EQ(2UL, cluster.info()->endpointStats().membership_healthy_.value());[m
[32m+[m[32m  EXPECT_EQ(0UL, cluster.info()->endpointStats().membership_degraded_.value());[m
 [m
   cluster.prioritySet().hostSetsPerPriority()[0]->hosts()[0]->healthFlagSet([m
       Host::HealthFlag::FAILED_OUTLIER_CHECK);[m
   outlier_detector->runCallbacks(cluster.prioritySet().hostSetsPerPriority()[0]->hosts()[0]);[m
   EXPECT_EQ(1UL, cluster.prioritySet().hostSetsPerPriority()[0]->healthyHosts().size());[m
[31m-  EXPECT_EQ(1UL, cluster.info()->stats().membership_healthy_.value());[m
[31m-  EXPECT_EQ(0UL, cluster.info()->stats().membership_degraded_.value());[m
[32m+[m[32m  EXPECT_EQ(1UL, cluster.info()->endpointStats().membership_healthy_.value());[m
[32m+[m[32m  EXPECT_EQ(0UL, cluster.info()->endpointStats().membership_degraded_.value());[m
 [m
   cluster.prioritySet().hostSetsPerPriority()[0]->hosts()[1]->healthFlagSet([m
       Host::HealthFlag::FAILED_ACTIVE_HC);[m
   health_checker->runCallbacks(cluster.prioritySet().hostSetsPerPriority()[0]->hosts()[1],[m
                                HealthTransition::Changed);[m
   EXPECT_EQ(0UL, cluster.prioritySet().hostSetsPerPriority()[0]->healthyHosts().size());[m
[31m-  EXPECT_EQ(0UL, cluster.info()->stats().membership_healthy_.value());[m
[31m-  EXPECT_EQ(0UL, cluster.info()->stats().membership_degraded_.value());[m
[32m+[m[32m  EXPECT_EQ(0UL, cluster.info()->endpointStats().membership_healthy_.value());[m
[32m+[m[32m  EXPECT_EQ(0UL, cluster.info()->endpointStats().membership_degraded_.value());[m
 [m
   cluster.prioritySet().hostSetsPerPriority()[0]->hosts()[1]->healthFlagSet([m
       Host::HealthFlag::DEGRADED_ACTIVE_HC);[m
[36m@@ -2148,8 +2227,8 @@[m [mTEST_F(StaticClusterImplTest, HealthyStat) {[m
                                HealthTransition::Changed);[m
   EXPECT_EQ(0UL, cluster.prioritySet().hostSetsPerPriority()[0]->healthyHosts().size());[m
   EXPECT_EQ(1UL, cluster.prioritySet().hostSetsPerPriority()[0]->degradedHosts().size());[m
[31m-  EXPECT_EQ(0UL, cluster.info()->stats().membership_healthy_.value());[m
[31m-  EXPECT_EQ(1UL, cluster.info()->stats().membership_degraded_.value());[m
[32m+[m[32m  EXPECT_EQ(0UL, cluster.info()->endpointStats().membership_healthy_.value());[m
[32m+[m[32m  EXPECT_EQ(1UL, cluster.info()->endpointStats().membership_degraded_.value());[m
 [m
   // Mark the endpoint as unhealthy. This should decrement the degraded stat.[m
   cluster.prioritySet().hostSetsPerPriority()[0]->hosts()[1]->healthFlagSet([m
[36m@@ -2158,8 +2237,8 @@[m [mTEST_F(StaticClusterImplTest, HealthyStat) {[m
                                HealthTransition::Changed);[m
   EXPECT_EQ(0UL, cluster.prioritySet().hostSetsPerPriority()[0]->healthyHosts().size());[m
   EXPECT_EQ(0UL, cluster.prioritySet().hostSetsPerPriority()[0]->degradedHosts().size());[m
[31m-  EXPECT_EQ(0UL, cluster.info()->stats().membership_healthy_.value());[m
[31m-  EXPECT_EQ(0UL, cluster.info()->stats().membership_degraded_.value());[m
[32m+[m[32m  EXPECT_EQ(0UL, cluster.info()->endpointStats().membership_healthy_.value());[m
[32m+[m[32m  EXPECT_EQ(0UL, cluster.info()->endpointStats().membership_degraded_.value());[m
 [m
   // Go back to degraded.[m
   cluster.prioritySet().hostSetsPerPriority()[0]->hosts()[1]->healthFlagClear([m
[36m@@ -2168,8 +2247,8 @@[m [mTEST_F(StaticClusterImplTest, HealthyStat) {[m
                                HealthTransition::Changed);[m
   EXPECT_EQ(0UL, cluster.prioritySet().hostSetsPerPriority()[0]->healthyHosts().size());[m
   EXPECT_EQ(1UL, cluster.prioritySet().hostSetsPerPriority()[0]->degradedHosts().size());[m
[31m-  EXPECT_EQ(0UL, cluster.info()->stats().membership_healthy_.value());[m
[31m-  EXPECT_EQ(1UL, cluster.info()->stats().membership_degraded_.value());[m
[32m+[m[32m  EXPECT_EQ(0UL, cluster.info()->endpointStats().membership_healthy_.value());[m
[32m+[m[32m  EXPECT_EQ(1UL, cluster.info()->endpointStats().membership_degraded_.value());[m
 [m
   // Then go healthy.[m
   cluster.prioritySet().hostSetsPerPriority()[0]->hosts()[1]->healthFlagClear([m
[36m@@ -2178,8 +2257,72 @@[m [mTEST_F(StaticClusterImplTest, HealthyStat) {[m
                                HealthTransition::Changed);[m
   EXPECT_EQ(1UL, cluster.prioritySet().hostSetsPerPriority()[0]->healthyHosts().size());[m
   EXPECT_EQ(0UL, cluster.prioritySet().hostSetsPerPriority()[0]->degradedHosts().size());[m
[31m-  EXPECT_EQ(1UL, cluster.info()->stats().membership_healthy_.value());[m
[31m-  EXPECT_EQ(0UL, cluster.info()->stats().membership_degraded_.value());[m
[32m+[m[32m  EXPECT_EQ(1UL, cluster.info()->endpointStats().membership_healthy_.value());[m
[32m+[m[32m  EXPECT_EQ(0UL, cluster.info()->endpointStats().membership_degraded_.value());[m
[32m+[m[32m}[m
[32m+[m
[32m+[m[32mTEST_F(StaticClusterImplTest, InitialHostsDisableHC) {[m
[32m+[m[32m  const std::string yaml = R"EOF([m
[32m+[m[32m    name: staticcluster[m
[32m+[m[32m    connect_timeout: 0.25s[m
[32m+[m[32m    type: STATIC[m
[32m+[m[32m    lb_policy: ROUND_ROBIN[m
[32m+[m[32m    load_assignment:[m
[32m+[m[32m        endpoints:[m
[32m+[m[32m          - lb_endpoints:[m
[32m+[m[32m            - endpoint:[m
[32m+[m[32m                address:[m
[32m+[m[32m                  socket_address:[m
[32m+[m[32m                    address: 10.0.0.1[m
[32m+[m[32m                    port_value: 11001[m
[32m+[m[32m                health_check_config:[m
[32m+[m[32m                  disable_active_health_check: true[m
[32m+[m[32m            - endpoint:[m
[32m+[m[32m                address:[m
[32m+[m[32m                  socket_address:[m
[32m+[m[32m                    address: 10.0.0.1[m
[32m+[m[32m                    port_value: 11002[m
[32m+[m[32m  )EOF";[m
[32m+[m
[32m+[m[32m  envoy::config::cluster::v3::Cluster cluster_config = parseClusterFromV3Yaml(yaml);[m
[32m+[m[32m  Envoy::Stats::ScopeSharedPtr scope = stats_.createScope(fmt::format([m
[32m+[m[32m      "cluster.{}.", cluster_config.alt_stat_name().empty() ? cluster_config.name()[m
[32m+[m[32m                                                            : cluster_config.alt_stat_name()));[m
[32m+[m[32m  Envoy::Server::Configuration::TransportSocketFactoryContextImpl factory_context([m
[32m+[m[32m      server_context_, ssl_context_manager_, *scope, server_context_.cluster_manager_, stats_,[m
[32m+[m[32m      validation_visitor_);[m
[32m+[m[32m  StaticClusterImpl cluster(server_context_, cluster_config, runtime_, factory_context,[m
[32m+[m[32m                            std::move(scope), false);[m
[32m+[m
[32m+[m[32m  Outlier::MockDetector* outlier_detector = new NiceMock<Outlier::MockDetector>();[m
[32m+[m[32m  cluster.setOutlierDetector(Outlier::DetectorSharedPtr{outlier_detector});[m
[32m+[m
[32m+[m[32m  std::shared_ptr<MockHealthChecker> health_checker(new NiceMock<MockHealthChecker>());[m
[32m+[m[32m  cluster.setHealthChecker(health_checker);[m
[32m+[m
[32m+[m[32m  ReadyWatcher initialized;[m
[32m+[m[32m  cluster.initialize([&initialized] { initialized.ready(); });[m
[32m+[m
[32m+[m[32m  // The endpoint with disabled active health check should not be set FAILED_ACTIVE_HC[m
[32m+[m[32m  // at beginning.[m
[32m+[m[32m  const auto& hosts = cluster.prioritySet().hostSetsPerPriority()[0]->hosts();[m
[32m+[m[32m  EXPECT_EQ(2UL, hosts.size());[m
[32m+[m[32m  EXPECT_FALSE(hosts[0]->healthFlagGet(Host::HealthFlag::FAILED_ACTIVE_HC));[m
[32m+[m[32m  EXPECT_TRUE(hosts[1]->healthFlagGet(Host::HealthFlag::FAILED_ACTIVE_HC));[m
[32m+[m
[32m+[m[32m  // The endpoint with disabled active health check is considered healthy.[m
[32m+[m[32m  EXPECT_EQ(2UL, cluster.prioritySet().hostSetsPerPriority()[0]->hosts().size());[m
[32m+[m[32m  EXPECT_EQ(1UL, cluster.prioritySet().hostSetsPerPriority()[0]->healthyHosts().size());[m
[32m+[m[32m  EXPECT_EQ(1UL, cluster.info()->endpointStats().membership_healthy_.value());[m
[32m+[m[32m  EXPECT_EQ(0UL, cluster.info()->endpointStats().membership_degraded_.value());[m
[32m+[m
[32m+[m[32m  // Perform a health check for the second host, and then the initialization is finished.[m
[32m+[m[32m  EXPECT_CALL(initialized, ready());[m
[32m+[m[32m  cluster.prioritySet().hostSetsPerPriority()[0]->hosts()[1]->healthFlagClear([m
[32m+[m[32m      Host::HealthFlag::FAILED_ACTIVE_HC);[m
[32m+[m[32m  health_checker->runCallbacks(cluster.prioritySet().hostSetsPerPriority()[0]->hosts()[0],[m
[32m+[m[32m                               HealthTransition::Changed);[m
[32m+[m[32m  EXPECT_EQ(2UL, cluster.prioritySet().hostSetsPerPriority()[0]->healthyHosts().size());[m
 }[m
 [m
 TEST_F(StaticClusterImplTest, UrlConfig) {[m
[1mdiff --git a/test/common/upstream/zone_aware_load_balancer_fuzz_base.cc b/test/common/upstream/zone_aware_load_balancer_fuzz_base.cc[m
[1mindex 73b3890089..e9ed21b670 100644[m
[1m--- a/test/common/upstream/zone_aware_load_balancer_fuzz_base.cc[m
[1m+++ b/test/common/upstream/zone_aware_load_balancer_fuzz_base.cc[m
[36m@@ -48,7 +48,6 @@[m [mvoid ZoneAwareLoadBalancerFuzzBase::setupZoneAwareLoadBalancingSpecificLogic() {[m
   // Having 3 possible weights, 1, 2, and 3 to provide the state space at least some variation[m
   // in regards to weights, which do affect the load balancing algorithm. Cap the amount of[m
   // weights at 3 for simplicity's sake[m
[31m-  stats_.max_host_weight_.set(3UL);[m
   addWeightsToHosts();[m
 }[m
 [m
[1mdiff --git a/test/extensions/clusters/aggregate/cluster_test.cc b/test/extensions/clusters/aggregate/cluster_test.cc[m
[1mindex 0a9845b7a8..ab83624bb6 100644[m
[1m--- a/test/extensions/clusters/aggregate/cluster_test.cc[m
[1m+++ b/test/extensions/clusters/aggregate/cluster_test.cc[m
[36m@@ -138,8 +138,8 @@[m [mpublic:[m
   Upstream::ThreadAwareLoadBalancerPtr thread_aware_lb_;[m
   Upstream::LoadBalancerFactorySharedPtr lb_factory_;[m
   Upstream::LoadBalancerPtr lb_;[m
[31m-  Upstream::ClusterStatNames stat_names_;[m
[31m-  Upstream::ClusterStats stats_;[m
[32m+[m[32m  Upstream::ClusterTrafficStatNames stat_names_;[m
[32m+[m[32m  Upstream::ClusterTrafficStats stats_;[m
   std::shared_ptr<Upstream::MockClusterInfo> primary_info_{[m
       new NiceMock<Upstream::MockClusterInfo>()};[m
   std::shared_ptr<Upstream::MockClusterInfo> secondary_info_{[m
[1mdiff --git a/test/extensions/clusters/redis/redis_cluster_test.cc b/test/extensions/clusters/redis/redis_cluster_test.cc[m
[1mindex 7fea1b0cd1..630da461e1 100644[m
[1m--- a/test/extensions/clusters/redis/redis_cluster_test.cc[m
[1m+++ b/test/extensions/clusters/redis/redis_cluster_test.cc[m
[36m@@ -791,7 +791,7 @@[m [mTEST_F(RedisClusterTest, AddressAsHostname) {[m
   EXPECT_CALL(*cluster_callback_, onClusterSlotUpdate(_, _));[m
   expectClusterSlotResponse(singleSlotPrimaryReplica("primary.com", "replica.org", 22120));[m
   expectHealthyHosts(std::list<std::string>({"127.0.1.1:22120", "127.0.1.2:22120"}));[m
[31m-  EXPECT_EQ(0U, cluster_->info()->stats().update_failure_.value());[m
[32m+[m[32m  EXPECT_EQ(0U, cluster_->info()->configUpdateStats().update_failure_.value());[m
 [m
   // 2. Single slot with just the primary hostname[m
   expectRedisResolve(true);[m
[36m@@ -802,7 +802,7 @@[m [mTEST_F(RedisClusterTest, AddressAsHostname) {[m
   EXPECT_CALL(*cluster_callback_, onClusterSlotUpdate(_, _));[m
   expectClusterSlotResponse(singleSlotPrimary("primary.com", 22120));[m
   expectHealthyHosts(std::list<std::string>({"127.0.1.1:22120"}));[m
[31m-  EXPECT_EQ(0U, cluster_->info()->stats().update_failure_.value());[m
[32m+[m[32m  EXPECT_EQ(0U, cluster_->info()->configUpdateStats().update_failure_.value());[m
 [m
   // 2. Single slot with just the primary IP address and replica hostname[m
   expectRedisResolve();[m
[36m@@ -813,7 +813,7 @@[m [mTEST_F(RedisClusterTest, AddressAsHostname) {[m
   EXPECT_CALL(*cluster_callback_, onClusterSlotUpdate(_, _));[m
   expectClusterSlotResponse(singleSlotPrimaryReplica("127.0.1.1", "replica.org", 22120));[m
   expectHealthyHosts(std::list<std::string>({"127.0.1.1:22120", "127.0.1.2:22120"}));[m
[31m-  EXPECT_EQ(0U, cluster_->info()->stats().update_failure_.value());[m
[32m+[m[32m  EXPECT_EQ(0U, cluster_->info()->configUpdateStats().update_failure_.value());[m
 }[m
 [m
 TEST_F(RedisClusterTest, AddressAsHostnameParallelResolution) {[m
[36m@@ -845,7 +845,7 @@[m [mTEST_F(RedisClusterTest, AddressAsHostnameParallelResolution) {[m
       "127.0.1.1:22120",[m
       "127.0.1.2:22120",[m
   }));[m
[31m-  EXPECT_EQ(0U, cluster_->info()->stats().update_failure_.value());[m
[32m+[m[32m  EXPECT_EQ(0U, cluster_->info()->configUpdateStats().update_failure_.value());[m
 }[m
 [m
 TEST_F(RedisClusterTest, AddressAsHostnameFailure) {[m
[36m@@ -875,7 +875,7 @@[m [mTEST_F(RedisClusterTest, AddressAsHostnameFailure) {[m
   EXPECT_EQ(1UL, cluster_->prioritySet().hostSetsPerPriority()[0]->hosts().size());[m
   EXPECT_EQ(1UL, cluster_->prioritySet().hostSetsPerPriority()[0]->healthyHosts().size());[m
   expectHealthyHosts(std::list<std::string>({"127.0.1.1:22120"}));[m
[31m-  EXPECT_EQ(1UL, cluster_->info()->stats().update_failure_.value());[m
[32m+[m[32m  EXPECT_EQ(1UL, cluster_->info()->configUpdateStats().update_failure_.value());[m
 [m
   // 2. Primary resolution fails, so replica resolution is not even called.[m
   // Expect cluster slot update to be successful, with just one healthy host, and failure counter to[m
[36m@@ -894,7 +894,7 @@[m [mTEST_F(RedisClusterTest, AddressAsHostnameFailure) {[m
   // healthy hosts is same as before, but failure count increases by 1[m
   EXPECT_EQ(1UL, cluster_->prioritySet().hostSetsPerPriority()[0]->hosts().size());[m
   EXPECT_EQ(1UL, cluster_->prioritySet().hostSetsPerPriority()[0]->healthyHosts().size());[m
[31m-  EXPECT_EQ(2UL, cluster_->info()->stats().update_failure_.value());[m
[32m+[m[32m  EXPECT_EQ(2UL, cluster_->info()->configUpdateStats().update_failure_.value());[m
 }[m
 [m
 TEST_F(RedisClusterTest, AddressAsHostnamePartialReplicaFailure) {[m
[36m@@ -940,7 +940,7 @@[m [mTEST_F(RedisClusterTest, EmptyDnsResponse) {[m
 [m
   EXPECT_EQ(0UL, cluster_->prioritySet().hostSetsPerPriority()[0]->hosts().size());[m
   EXPECT_EQ(0UL, cluster_->prioritySet().hostSetsPerPriority()[0]->healthyHosts().size());[m
[31m-  EXPECT_EQ(1U, cluster_->info()->stats().update_empty_.value());[m
[32m+[m[32m  EXPECT_EQ(1U, cluster_->info()->configUpdateStats().update_empty_.value());[m
 [m
   // Does not recreate the timer on subsequent DNS resolve calls.[m
   EXPECT_CALL(*dns_timer, enableTimer(_, _));[m
[36m@@ -949,7 +949,7 @@[m [mTEST_F(RedisClusterTest, EmptyDnsResponse) {[m
 [m
   EXPECT_EQ(0UL, cluster_->prioritySet().hostSetsPerPriority()[0]->hosts().size());[m
   EXPECT_EQ(0UL, cluster_->prioritySet().hostSetsPerPriority()[0]->healthyHosts().size());[m
[31m-  EXPECT_EQ(2U, cluster_->info()->stats().update_empty_.value());[m
[32m+[m[32m  EXPECT_EQ(2U, cluster_->info()->configUpdateStats().update_empty_.value());[m
 }[m
 [m
 TEST_F(RedisClusterTest, FailedDnsResponse) {[m
[36m@@ -965,7 +965,7 @@[m [mTEST_F(RedisClusterTest, FailedDnsResponse) {[m
 [m
   EXPECT_EQ(0UL, cluster_->prioritySet().hostSetsPerPriority()[0]->hosts().size());[m
   EXPECT_EQ(0UL, cluster_->prioritySet().hostSetsPerPriority()[0]->healthyHosts().size());[m
[31m-  EXPECT_EQ(0U, cluster_->info()->stats().update_empty_.value());[m
[32m+[m[32m  EXPECT_EQ(0U, cluster_->info()->configUpdateStats().update_empty_.value());[m
 [m
   // Does not recreate the timer on subsequent DNS resolve calls.[m
   EXPECT_CALL(*dns_timer, enableTimer(_, _));[m
[36m@@ -974,7 +974,7 @@[m [mTEST_F(RedisClusterTest, FailedDnsResponse) {[m
 [m
   EXPECT_EQ(0UL, cluster_->prioritySet().hostSetsPerPriority()[0]->hosts().size());[m
   EXPECT_EQ(0UL, cluster_->prioritySet().hostSetsPerPriority()[0]->healthyHosts().size());[m
[31m-  EXPECT_EQ(1U, cluster_->info()->stats().update_empty_.value());[m
[32m+[m[32m  EXPECT_EQ(1U, cluster_->info()->configUpdateStats().update_empty_.value());[m
 }[m
 [m
 TEST_F(RedisClusterTest, Basic) {[m
[36m@@ -1020,8 +1020,8 @@[m [mTEST_F(RedisClusterTest, RedisResolveFailure) {[m
 [m
   // Initialization will wait til the redis cluster succeed.[m
   expectClusterSlotFailure();[m
[31m-  EXPECT_EQ(1U, cluster_->info()->stats().update_attempt_.value());[m
[31m-  EXPECT_EQ(1U, cluster_->info()->stats().update_failure_.value());[m
[32m+[m[32m  EXPECT_EQ(1U, cluster_->info()->configUpdateStats().update_attempt_.value());[m
[32m+[m[32m  EXPECT_EQ(1U, cluster_->info()->configUpdateStats().update_failure_.value());[m
 [m
   expectRedisResolve(true);[m
   resolve_timer_->invokeCallback();[m
[36m@@ -1036,8 +1036,8 @@[m [mTEST_F(RedisClusterTest, RedisResolveFailure) {[m
   resolve_timer_->invokeCallback();[m
   expectClusterSlotFailure();[m
   expectHealthyHosts(std::list<std::string>({"127.0.0.1:22120", "127.0.0.2:22120"}));[m
[31m-  EXPECT_EQ(3U, cluster_->info()->stats().update_attempt_.value());[m
[31m-  EXPECT_EQ(2U, cluster_->info()->stats().update_failure_.value());[m
[32m+[m[32m  EXPECT_EQ(3U, cluster_->info()->configUpdateStats().update_attempt_.value());[m
[32m+[m[32m  EXPECT_EQ(2U, cluster_->info()->configUpdateStats().update_failure_.value());[m
 }[m
 [m
 TEST_F(RedisClusterTest, FactoryInitNotRedisClusterTypeFailure) {[m
[36m@@ -1092,8 +1092,8 @@[m [mTEST_F(RedisClusterTest, RedisErrorResponse) {[m
 [m
   EXPECT_CALL(*cluster_callback_, onClusterSlotUpdate(_, _)).Times(0);[m
   expectClusterSlotResponse(std::move(hello_world_response));[m
[31m-  EXPECT_EQ(1U, cluster_->info()->stats().update_attempt_.value());[m
[31m-  EXPECT_EQ(1U, cluster_->info()->stats().update_failure_.value());[m
[32m+[m[32m  EXPECT_EQ(1U, cluster_->info()->configUpdateStats().update_attempt_.value());[m
[32m+[m[32m  EXPECT_EQ(1U, cluster_->info()->configUpdateStats().update_failure_.value());[m
 [m
   expectRedisResolve();[m
   resolve_timer_->invokeCallback();[m
[36m@@ -1118,9 +1118,9 @@[m [mTEST_F(RedisClusterTest, RedisErrorResponse) {[m
     }[m
     expectClusterSlotResponse(createResponse(flags, no_replica));[m
     expectHealthyHosts(std::list<std::string>({"127.0.0.1:22120"}));[m
[31m-    EXPECT_EQ(++update_attempt, cluster_->info()->stats().update_attempt_.value());[m
[32m+[m[32m    EXPECT_EQ(++update_attempt, cluster_->info()->configUpdateStats().update_attempt_.value());[m
     if (!flags.all()) {[m
[31m-      EXPECT_EQ(++update_failure, cluster_->info()->stats().update_failure_.value());[m
[32m+[m[32m      EXPECT_EQ(++update_failure, cluster_->info()->configUpdateStats().update_failure_.value());[m
     }[m
   }[m
 }[m
[36m@@ -1155,9 +1155,9 @@[m [mTEST_F(RedisClusterTest, RedisReplicaErrorResponse) {[m
     }[m
     expectHealthyHosts(std::list<std::string>({"127.0.0.1:22120"}));[m
     expectClusterSlotResponse(createResponse(single_slot_primary, replica_flags));[m
[31m-    EXPECT_EQ(++update_attempt, cluster_->info()->stats().update_attempt_.value());[m
[32m+[m[32m    EXPECT_EQ(++update_attempt, cluster_->info()->configUpdateStats().update_attempt_.value());[m
     if (!(replica_flags.all() || replica_flags.none())) {[m
[31m-      EXPECT_EQ(++update_failure, cluster_->info()->stats().update_failure_.value());[m
[32m+[m[32m      EXPECT_EQ(++update_failure, cluster_->info()->configUpdateStats().update_failure_.value());[m
     }[m
   }[m
 }[m
[1mdiff --git a/test/extensions/filters/common/rbac/BUILD b/test/extensions/filters/common/rbac/BUILD[m
[1mindex dc8ec26940..57d88f3e74 100644[m
[1m--- a/test/extensions/filters/common/rbac/BUILD[m
[1m+++ b/test/extensions/filters/common/rbac/BUILD[m
[36m@@ -17,6 +17,7 @@[m [menvoy_extension_cc_test([m
     srcs = ["matchers_test.cc"],[m
     extension_names = ["envoy.filters.http.rbac"],[m
     deps = [[m
[32m+[m[32m        "//source/common/stream_info:filter_state_lib",[m
         "//source/extensions/filters/common/expr:evaluator_lib",[m
         "//source/extensions/filters/common/rbac:matchers_lib",[m
         "//test/mocks/network:network_mocks",[m
[1mdiff --git a/test/extensions/filters/common/rbac/matchers_test.cc b/test/extensions/filters/common/rbac/matchers_test.cc[m
[1mindex 7d96e355d9..6d9a582df7 100644[m
[1m--- a/test/extensions/filters/common/rbac/matchers_test.cc[m
[1m+++ b/test/extensions/filters/common/rbac/matchers_test.cc[m
[36m@@ -6,6 +6,7 @@[m
 [m
 #include "source/common/network/address_impl.h"[m
 #include "source/common/network/utility.h"[m
[32m+[m[32m#include "source/common/stream_info/filter_state_impl.h"[m
 #include "source/extensions/filters/common/expr/evaluator.h"[m
 #include "source/extensions/filters/common/rbac/matchers.h"[m
 [m
[36m@@ -517,6 +518,28 @@[m [mTEST(PathMatcher, ValidPathInHeader) {[m
   checkMatcher(PathMatcher(matcher), false, Envoy::Network::MockConnection(), headers);[m
 }[m
 [m
[32m+[m[32mclass TestObject : public StreamInfo::FilterState::Object {[m
[32m+[m[32mpublic:[m
[32m+[m[32m  absl::optional<std::string> serializeAsString() const override { return "test.value"; }[m
[32m+[m[32m};[m
[32m+[m
[32m+[m[32mTEST(FilterStateMatcher, FilterStateMatcher) {[m
[32m+[m[32m  Envoy::Network::MockConnection conn;[m
[32m+[m[32m  Envoy::Http::TestRequestHeaderMapImpl header;[m
[32m+[m[32m  NiceMock<StreamInfo::MockStreamInfo> info;[m
[32m+[m[32m  StreamInfo::FilterStateImpl filter_state(StreamInfo::FilterState::LifeSpan::Connection);[m
[32m+[m[32m  EXPECT_CALL(Const(info), filterState()).WillRepeatedly(ReturnRef(filter_state));[m
[32m+[m
[32m+[m[32m  envoy::type::matcher::v3::FilterStateMatcher matcher;[m
[32m+[m[32m  matcher.set_key("test.key");[m
[32m+[m[32m  matcher.mutable_string_match()->set_prefix("test");[m
[32m+[m
[32m+[m[32m  checkMatcher(FilterStateMatcher(matcher), false, conn, header, info);[m
[32m+[m[32m  filter_state.setData("test.key", std::make_shared<TestObject>(),[m
[32m+[m[32m                       StreamInfo::FilterState::StateType::ReadOnly);[m
[32m+[m[32m  checkMatcher(FilterStateMatcher(matcher), true, conn, header, info);[m
[32m+[m[32m}[m
[32m+[m
 } // namespace[m
 } // namespace RBAC[m
 } // namespace Common[m
[1mdiff --git a/test/extensions/filters/http/cache/cache_entry_utils_test.cc b/test/extensions/filters/http/cache/cache_entry_utils_test.cc[m
[1mindex d930d03d37..3d4d143313 100644[m
[1m--- a/test/extensions/filters/http/cache/cache_entry_utils_test.cc[m
[1m+++ b/test/extensions/filters/http/cache/cache_entry_utils_test.cc[m
[36m@@ -26,6 +26,81 @@[m [mTEST(Coverage, CacheEntryStatusStream) {[m
   EXPECT_EQ(stream.str(), "Ok");[m
 }[m
 [m
[32m+[m[32mTEST(CacheEntryUtils, ApplyHeaderUpdateReplacesMultiValues) {[m
[32m+[m[32m  Http::TestResponseHeaderMapImpl headers{[m
[32m+[m[32m      {"test_header", "test_value"},[m
[32m+[m[32m      {"second_header", "second_value"},[m
[32m+[m[32m      {"second_header", "additional_value"},[m
[32m+[m[32m  };[m
[32m+[m[32m  Http::TestResponseHeaderMapImpl new_headers{[m
[32m+[m[32m      {"second_header", "new_second_value"},[m
[32m+[m[32m  };[m
[32m+[m[32m  applyHeaderUpdate(new_headers, headers);[m
[32m+[m[32m  Http::TestResponseHeaderMapImpl expected{[m
[32m+[m[32m      {"test_header", "test_value"},[m
[32m+[m[32m      {"second_header", "new_second_value"},[m
[32m+[m[32m  };[m
[32m+[m[32m  EXPECT_THAT(&headers, HeaderMapEqualIgnoreOrder(&expected));[m
[32m+[m[32m}[m
[32m+[m
[32m+[m[32mTEST(CacheEntryUtils, ApplyHeaderUpdateAppliesMultiValues) {[m
[32m+[m[32m  Http::TestResponseHeaderMapImpl headers{[m
[32m+[m[32m      {"test_header", "test_value"},[m
[32m+[m[32m      {"second_header", "second_value"},[m
[32m+[m[32m  };[m
[32m+[m[32m  Http::TestResponseHeaderMapImpl new_headers{[m
[32m+[m[32m      {"second_header", "new_second_value"},[m
[32m+[m[32m      {"second_header", "another_new_second_value"},[m
[32m+[m[32m  };[m
[32m+[m[32m  applyHeaderUpdate(new_headers, headers);[m
[32m+[m[32m  Http::TestResponseHeaderMapImpl expected{[m
[32m+[m[32m      {"test_header", "test_value"},[m
[32m+[m[32m      {"second_header", "new_second_value"},[m
[32m+[m[32m      {"second_header", "another_new_second_value"},[m
[32m+[m[32m  };[m
[32m+[m[32m  EXPECT_THAT(&headers, HeaderMapEqualIgnoreOrder(&expected));[m
[32m+[m[32m}[m
[32m+[m
[32m+[m[32mTEST(CacheEntryUtils, ApplyHeaderUpdateIgnoresIgnoredValues) {[m
[32m+[m[32m  Http::TestResponseHeaderMapImpl headers{[m
[32m+[m[32m      {"test_header", "test_value"}, {"etag", "original_etag"}, {"content-length", "123456"},[m
[32m+[m[32m      {"content-range", "654321"},   {"vary", "original_vary"},[m
[32m+[m[32m  };[m
[32m+[m[32m  Http::TestResponseHeaderMapImpl new_headers{[m
[32m+[m[32m      {"etag", "updated_etag"},[m
[32m+[m[32m      {"content-length", "999999"},[m
[32m+[m[32m      {"content-range", "999999"},[m
[32m+[m[32m      {"vary", "updated_vary"},[m
[32m+[m[32m  };[m
[32m+[m[32m  applyHeaderUpdate(new_headers, headers);[m
[32m+[m[32m  Http::TestResponseHeaderMapImpl expected{[m
[32m+[m[32m      {"test_header", "test_value"}, {"etag", "original_etag"}, {"content-length", "123456"},[m
[32m+[m[32m      {"content-range", "654321"},   {"vary", "original_vary"},[m
[32m+[m[32m  };[m
[32m+[m[32m  EXPECT_THAT(&headers, HeaderMapEqualIgnoreOrder(&expected));[m
[32m+[m[32m}[m
[32m+[m
[32m+[m[32mTEST(CacheEntryUtils, ApplyHeaderUpdateCorrectlyMixesOverwriteIgnoreAddAndPersist) {[m
[32m+[m[32m  Http::TestResponseHeaderMapImpl headers{[m
[32m+[m[32m      {"persisted_header", "1"},[m
[32m+[m[32m      {"persisted_header", "2"},[m
[32m+[m[32m      {"overwritten_header", "old"},[m
[32m+[m[32m  };[m
[32m+[m[32m  Http::TestResponseHeaderMapImpl new_headers{[m
[32m+[m[32m      {"overwritten_header", "new"},[m
[32m+[m[32m      {"added_header", "also_new"},[m
[32m+[m[32m      {"etag", "ignored"},[m
[32m+[m[32m  };[m
[32m+[m[32m  applyHeaderUpdate(new_headers, headers);[m
[32m+[m[32m  Http::TestResponseHeaderMapImpl expected{[m
[32m+[m[32m      {"persisted_header", "1"},[m
[32m+[m[32m      {"persisted_header", "2"},[m
[32m+[m[32m      {"overwritten_header", "new"},[m
[32m+[m[32m      {"added_header", "also_new"},[m
[32m+[m[32m  };[m
[32m+[m[32m  EXPECT_THAT(&headers, HeaderMapEqualIgnoreOrder(&expected));[m
[32m+[m[32m}[m
[32m+[m
 } // namespace[m
 } // namespace Cache[m
 } // namespace HttpFilters[m
[1mdiff --git a/test/extensions/filters/http/health_check/health_check_test.cc b/test/extensions/filters/http/health_check/health_check_test.cc[m
[1mindex 7dcf99d512..5e9051c7bb 100644[m
[1m--- a/test/extensions/filters/http/health_check/health_check_test.cc[m
[1m+++ b/test/extensions/filters/http/health_check/health_check_test.cc[m
[36m@@ -71,9 +71,9 @@[m [mpublic:[m
   public:[m
     MockHealthCheckCluster(uint64_t membership_total, uint64_t membership_healthy,[m
                            uint64_t membership_degraded = 0) {[m
[31m-      info()->stats().membership_total_.set(membership_total);[m
[31m-      info()->stats().membership_healthy_.set(membership_healthy);[m
[31m-      info()->stats().membership_degraded_.set(membership_degraded);[m
[32m+[m[32m      info()->endpointStats().membership_total_.set(membership_total);[m
[32m+[m[32m      info()->endpointStats().membership_healthy_.set(membership_healthy);[m
[32m+[m[32m      info()->endpointStats().membership_degraded_.set(membership_degraded);[m
     }[m
   };[m
 };[m
[1mdiff --git a/test/extensions/matching/actions/format_string/BUILD b/test/extensions/matching/actions/format_string/BUILD[m
[1mnew file mode 100644[m
[1mindex 0000000000..a9a49de8c8[m
[1m--- /dev/null[m
[1m+++ b/test/extensions/matching/actions/format_string/BUILD[m
[36m@@ -0,0 +1,24 @@[m
[32m+[m[32mload([m
[32m+[m[32m    "//bazel:envoy_build_system.bzl",[m
[32m+[m[32m    "envoy_package",[m
[32m+[m[32m)[m
[32m+[m[32mload([m
[32m+[m[32m    "//test/extensions:extensions_build_system.bzl",[m
[32m+[m[32m    "envoy_extension_cc_test",[m
[32m+[m[32m)[m
[32m+[m
[32m+[m[32mlicenses(["notice"])  # Apache 2[m
[32m+[m
[32m+[m[32menvoy_package()[m
[32m+[m
[32m+[m[32menvoy_extension_cc_test([m
[32m+[m[32m    name = "config_test",[m
[32m+[m[32m    srcs = ["config_test.cc"],[m
[32m+[m[32m    extension_names = ["envoy.matching.actions.format_string"],[m
[32m+[m[32m    deps = [[m
[32m+[m[32m        "//source/extensions/matching/actions/format_string:config",[m
[32m+[m[32m        "//test/mocks/network:network_mocks",[m
[32m+[m[32m        "//test/mocks/server:instance_mocks",[m
[32m+[m[32m        "//test/mocks/stream_info:stream_info_mocks",[m
[32m+[m[32m    ],[m
[32m+[m[32m)[m
[1mdiff --git a/test/extensions/matching/actions/format_string/config_test.cc b/test/extensions/matching/actions/format_string/config_test.cc[m
[1mnew file mode 100644[m
[1mindex 0000000000..c52cc8f230[m
[1m--- /dev/null[m
[1m+++ b/test/extensions/matching/actions/format_string/config_test.cc[m
[36m@@ -0,0 +1,60 @@[m
[32m+[m[32m#include "source/extensions/matching/actions/format_string/config.h"[m
[32m+[m
[32m+[m[32m#include "test/mocks/network/mocks.h"[m
[32m+[m[32m#include "test/mocks/server/instance.h"[m
[32m+[m[32m#include "test/mocks/stream_info/mocks.h"[m
[32m+[m
[32m+[m[32m#include "gtest/gtest.h"[m
[32m+[m
[32m+[m[32mnamespace Envoy {[m
[32m+[m[32mnamespace Extensions {[m
[32m+[m[32mnamespace Matching {[m
[32m+[m[32mnamespace Actions {[m
[32m+[m[32mnamespace FormatString {[m
[32m+[m
[32m+[m[32mTEST(ConfigTest, TestConfig) {[m
[32m+[m[32m  const std::string yaml_string = R"EOF([m
[32m+[m[32m    text_format_source:[m
[32m+[m[32m      inline_string: "%DYNAMIC_METADATA(com.test_filter:test_key)%"[m
[32m+[m[32m)EOF";[m
[32m+[m
[32m+[m[32m  envoy::config::core::v3::SubstitutionFormatString config;[m
[32m+[m[32m  TestUtility::loadFromYaml(yaml_string, config);[m
[32m+[m
[32m+[m[32m  testing::NiceMock<Server::Configuration::MockServerFactoryContext> factory_context;[m
[32m+[m[32m  ActionFactory factory;[m
[32m+[m[32m  auto action_cb = factory.createActionFactoryCb(config, factory_context,[m
[32m+[m[32m                                                 ProtobufMessage::getStrictValidationVisitor());[m
[32m+[m[32m  ASSERT_NE(nullptr, action_cb);[m
[32m+[m[32m  auto action = action_cb();[m
[32m+[m[32m  ASSERT_NE(nullptr, action);[m
[32m+[m[32m  const auto& typed_action = action->getTyped<Server::FilterChainBaseAction>();[m
[32m+[m
[32m+[m[32m  Server::FilterChainsByName chains;[m
[32m+[m[32m  auto chain = std::make_shared<testing::NiceMock<Network::MockFilterChain>>();[m
[32m+[m[32m  chains.emplace("foo", chain);[m
[32m+[m
[32m+[m[32m  testing::NiceMock<StreamInfo::MockStreamInfo> info;[m
[32m+[m[32m  {[m
[32m+[m[32m    auto result = typed_action.get(chains, info);[m
[32m+[m[32m    EXPECT_EQ(nullptr, result);[m
[32m+[m[32m  }[m
[32m+[m
[32m+[m[32m  {[m
[32m+[m[32m    const std::string metadata_string = R"EOF([m
[32m+[m[32m  filter_metadata:[m
[32m+[m[32m    com.test_filter:[m
[32m+[m[32m      test_key: foo[m
[32m+[m[32m)EOF";[m
[32m+[m[32m    TestUtility::loadFromYaml(metadata_string, info.metadata_);[m
[32m+[m[32m    auto result = typed_action.get(chains, info);[m
[32m+[m[32m    ASSERT_NE(nullptr, result);[m
[32m+[m[32m    ASSERT_EQ(chain.get(), result);[m
[32m+[m[32m  }[m
[32m+[m[32m}[m
[32m+[m
[32m+[m[32m} // namespace FormatString[m
[32m+[m[32m} // namespace Actions[m
[32m+[m[32m} // namespace Matching[m
[32m+[m[32m} // namespace Extensions[m
[32m+[m[32m} // namespace Envoy[m
[1mdiff --git a/test/extensions/stats_sinks/common/statsd/statsd_test.cc b/test/extensions/stats_sinks/common/statsd/statsd_test.cc[m
[1mindex 3d20059442..8e036f008e 100644[m
[1m--- a/test/extensions/stats_sinks/common/statsd/statsd_test.cc[m
[1m+++ b/test/extensions/stats_sinks/common/statsd/statsd_test.cc[m
[36m@@ -225,13 +225,13 @@[m [mTEST_F(TcpStatsdSinkTest, Overflow) {[m
 [m
   // Synthetically set buffer above high watermark. Make sure we don't write anything.[m
   cluster_manager_.active_clusters_["fake_cluster"][m
[31m-      ->info_->stats()[m
[32m+[m[32m      ->info_->trafficStats()[m
       .upstream_cx_tx_bytes_buffered_.set(1024 * 1024 * 17);[m
   sink_->flush(snapshot_);[m
 [m
   // Lower and make sure we write.[m
   cluster_manager_.active_clusters_["fake_cluster"][m
[31m-      ->info_->stats()[m
[32m+[m[32m      ->info_->trafficStats()[m
       .upstream_cx_tx_bytes_buffered_.set(1024 * 1024 * 15);[m
   expectCreateConnection();[m
   EXPECT_CALL(*connection_, write(BufferStringEqual("envoy.test_counter:1|c\n"), _));[m
[36m@@ -239,7 +239,7 @@[m [mTEST_F(TcpStatsdSinkTest, Overflow) {[m
 [m
   // Raise and make sure we don't write and kill connection.[m
   cluster_manager_.active_clusters_["fake_cluster"][m
[31m-      ->info_->stats()[m
[32m+[m[32m      ->info_->trafficStats()[m
       .upstream_cx_tx_bytes_buffered_.set(1024 * 1024 * 17);[m
   EXPECT_CALL(*connection_, close(Network::ConnectionCloseType::NoFlush));[m
   sink_->flush(snapshot_);[m
[1mdiff --git a/test/extensions/stats_sinks/hystrix/hystrix_test.cc b/test/extensions/stats_sinks/hystrix/hystrix_test.cc[m
[1mindex 866cbc7df0..53dd5278ab 100644[m
[1m--- a/test/extensions/stats_sinks/hystrix/hystrix_test.cc[m
[1m+++ b/test/extensions/stats_sinks/hystrix/hystrix_test.cc[m
[36m@@ -82,9 +82,9 @@[m [mpublic:[m
     ON_CALL(error_4xx_counter_, value()).WillByDefault(Return((i + 1) * error_4xx_step));[m
     ON_CALL(retry_4xx_counter_, value()).WillByDefault(Return((i + 1) * error_4xx_retry_step));[m
     ON_CALL(success_counter_, value()).WillByDefault(Return((i + 1) * success_step));[m
[31m-    cluster_info_->stats().upstream_rq_timeout_.add(timeout_step);[m
[31m-    cluster_info_->stats().upstream_rq_per_try_timeout_.add(timeout_retry_step);[m
[31m-    cluster_info_->stats().upstream_rq_pending_overflow_.add(rejected_step);[m
[32m+[m[32m    cluster_info_->trafficStats().upstream_rq_timeout_.add(timeout_step);[m
[32m+[m[32m    cluster_info_->trafficStats().upstream_rq_per_try_timeout_.add(timeout_retry_step);[m
[32m+[m[32m    cluster_info_->trafficStats().upstream_rq_pending_overflow_.add(rejected_step);[m
   }[m
 [m
   NiceMock<Upstream::MockClusterMockPrioritySet> cluster_;[m
[1mdiff --git a/test/integration/eds_integration_test.cc b/test/integration/eds_integration_test.cc[m
[1mindex dbac39a9c7..d3be9d2ccb 100644[m
[1m--- a/test/integration/eds_integration_test.cc[m
[1m+++ b/test/integration/eds_integration_test.cc[m
[36m@@ -56,32 +56,45 @@[m [mpublic:[m
     }[m
   }[m
 [m
[32m+[m[32m  struct EndpointSettingOptions {[m
[32m+[m[32m    uint32_t total_endpoints = 1;[m
[32m+[m[32m    uint32_t healthy_endpoints = 0;[m
[32m+[m[32m    uint32_t degraded_endpoints = 0;[m
[32m+[m[32m    uint32_t disable_active_hc_endpoints = 0;[m
[32m+[m[32m    absl::optional<uint32_t> overprovisioning_factor = absl::nullopt;[m
[32m+[m[32m  };[m
[32m+[m
   // We need to supply the endpoints via EDS to provide health status. Use a[m
   // filesystem delivery to simplify test mechanics.[m
[31m-  void setEndpoints(uint32_t total_endpoints, uint32_t healthy_endpoints,[m
[31m-                    uint32_t degraded_endpoints, bool remaining_unhealthy = true,[m
[31m-                    absl::optional<uint32_t> overprovisioning_factor = absl::nullopt,[m
[32m+[m[32m  void setEndpoints(const EndpointSettingOptions& endpoint_setting, bool remaining_unhealthy = true,[m
                     bool await_update = true) {[m
[31m-    ASSERT(total_endpoints >= healthy_endpoints + degraded_endpoints);[m
[32m+[m[32m    ASSERT(endpoint_setting.total_endpoints >=[m
[32m+[m[32m           endpoint_setting.healthy_endpoints + endpoint_setting.degraded_endpoints);[m
[32m+[m[32m    ASSERT(endpoint_setting.total_endpoints >= endpoint_setting.disable_active_hc_endpoints);[m
     envoy::config::endpoint::v3::ClusterLoadAssignment cluster_load_assignment;[m
     cluster_load_assignment.set_cluster_name("cluster_0");[m
[31m-    if (overprovisioning_factor.has_value()) {[m
[32m+[m[32m    if (endpoint_setting.overprovisioning_factor.has_value()) {[m
       cluster_load_assignment.mutable_policy()->mutable_overprovisioning_factor()->set_value([m
[31m-          overprovisioning_factor.value());[m
[32m+[m[32m          endpoint_setting.overprovisioning_factor.value());[m
     }[m
     auto* locality_lb_endpoints = cluster_load_assignment.add_endpoints();[m
 [m
[31m-    for (uint32_t i = 0; i < total_endpoints; ++i) {[m
[32m+[m[32m    for (uint32_t i = 0; i < endpoint_setting.total_endpoints; ++i) {[m
       auto* endpoint = locality_lb_endpoints->add_lb_endpoints();[m
       setUpstreamAddress(i, *endpoint);[m
       // First N endpoints are degraded, next M are healthy and the remaining endpoints are[m
       // unhealthy or unknown depending on remaining_unhealthy.[m
[31m-      if (i < degraded_endpoints) {[m
[32m+[m[32m      if (i < endpoint_setting.degraded_endpoints) {[m
         endpoint->set_health_status(envoy::config::core::v3::DEGRADED);[m
[31m-      } else if (i >= healthy_endpoints + degraded_endpoints) {[m
[32m+[m[32m      } else if (i >= endpoint_setting.healthy_endpoints + endpoint_setting.degraded_endpoints) {[m
         endpoint->set_health_status(remaining_unhealthy ? envoy::config::core::v3::UNHEALTHY[m
                                                         : envoy::config::core::v3::UNKNOWN);[m
       }[m
[32m+[m[32m      if (i < endpoint_setting.disable_active_hc_endpoints) {[m
[32m+[m[32m        endpoint->mutable_endpoint()[m
[32m+[m[32m            ->mutable_health_check_config()[m
[32m+[m[32m            ->set_disable_active_health_check(true);[m
[32m+[m[32m      }[m
     }[m
 [m
     if (await_update) {[m
[36m@@ -136,7 +149,7 @@[m [mpublic:[m
       health_check->mutable_http_health_check()->set_path("/healthcheck");[m
       health_check->mutable_http_health_check()->set_codec_client_type(codec_client_type_);[m
     }[m
[31m-    setEndpoints(0, 0, 0, true, absl::nullopt, false);[m
[32m+[m[32m    setEndpoints({/*total_endpoints=*/0}, true, false);[m
 [m
     if (cluster_modifier != nullptr) {[m
       cluster_modifier(cluster_);[m
[36m@@ -175,7 +188,9 @@[m [mTEST_P(EdsIntegrationTest, Http2UpdatePriorities) {[m
 TEST_P(EdsIntegrationTest, Http2HcClusterRewarming) {[m
   codec_client_type_ = envoy::type::v3::HTTP2;[m
   initializeTest(true);[m
[31m-  setEndpoints(1, 0, 0, false);[m
[32m+[m
[32m+[m[32m  // There is 1 total endpoint.[m
[32m+[m[32m  setEndpoints(EndpointSettingOptions(), false);[m
   EXPECT_EQ(1, test_server_->gauge("cluster.cluster_0.membership_total")->value());[m
   EXPECT_EQ(0, test_server_->gauge("cluster.cluster_0.membership_healthy")->value());[m
 [m
[36m@@ -219,11 +234,51 @@[m [mTEST_P(EdsIntegrationTest, Http2HcClusterRewarming) {[m
   fake_upstream_connection.reset();[m
 }[m
 [m
[32m+[m[32mTEST_P(EdsIntegrationTest, EndpointDisableActiveHCFlag) {[m
[32m+[m[32m  initializeTest(true);[m
[32m+[m[32m  EndpointSettingOptions options;[m
[32m+[m
[32m+[m[32m  // Total 1 endpoints with all active health check enabled.[m
[32m+[m[32m  setEndpoints(options, false);[m
[32m+[m[32m  EXPECT_EQ(1, test_server_->gauge("cluster.cluster_0.membership_total")->value());[m
[32m+[m[32m  EXPECT_EQ(0, test_server_->gauge("cluster.cluster_0.membership_healthy")->value());[m
[32m+[m
[32m+[m[32m  // Wait for the first HC and verify the host is healthy.[m
[32m+[m[32m  waitForNextUpstreamRequest();[m
[32m+[m[32m  upstream_request_->encodeHeaders(Http::TestResponseHeaderMapImpl{{":status", "200"}}, true);[m
[32m+[m
[32m+[m[32m  test_server_->waitForGaugeEq("cluster.cluster_0.membership_healthy", 1);[m
[32m+[m[32m  EXPECT_EQ(1, test_server_->gauge("cluster.cluster_0.membership_total")->value());[m
[32m+[m[32m  EXPECT_EQ(1, test_server_->gauge("cluster.cluster_0.membership_healthy")->value());[m
[32m+[m
[32m+[m[32m  // Disable Active Healthy Check for the host. EDS Unknown is considered as healthy[m
[32m+[m[32m  options.disable_active_hc_endpoints = 1;[m
[32m+[m[32m  setEndpoints(options, false);[m
[32m+[m[32m  EXPECT_EQ(1, test_server_->gauge("cluster.cluster_0.membership_total")->value());[m
[32m+[m[32m  EXPECT_EQ(1, test_server_->gauge("cluster.cluster_0.membership_healthy")->value());[m
[32m+[m
[32m+[m[32m  // Set the host as unhealthy through EDS.[m
[32m+[m[32m  setEndpoints(options, true);[m
[32m+[m[32m  EXPECT_EQ(1, test_server_->gauge("cluster.cluster_0.membership_total")->value());[m
[32m+[m[32m  EXPECT_EQ(0, test_server_->gauge("cluster.cluster_0.membership_healthy")->value());[m
[32m+[m
[32m+[m[32m  // Set host as healthy through EDS.[m
[32m+[m[32m  options.healthy_endpoints = 1;[m
[32m+[m[32m  setEndpoints(options, false);[m
[32m+[m[32m  EXPECT_EQ(1, test_server_->gauge("cluster.cluster_0.membership_total")->value());[m
[32m+[m[32m  EXPECT_EQ(1, test_server_->gauge("cluster.cluster_0.membership_healthy")->value());[m
[32m+[m
[32m+[m[32m  // Clear out the host and verify the host is gone.[m
[32m+[m[32m  setEndpoints({/*total_endpoints=*/0});[m
[32m+[m[32m  EXPECT_EQ(0, test_server_->gauge("cluster.cluster_0.membership_total")->value());[m
[32m+[m[32m}[m
[32m+[m
 // Verify that a host stabilized via active health checking which is first removed from EDS and[m
 // then fails health checking is removed.[m
 TEST_P(EdsIntegrationTest, RemoveAfterHcFail) {[m
   initializeTest(true);[m
[31m-  setEndpoints(1, 0, 0, false);[m
[32m+[m[32m  EndpointSettingOptions options;[m
[32m+[m[32m  setEndpoints(options, false);[m
   EXPECT_EQ(1, test_server_->gauge("cluster.cluster_0.membership_total")->value());[m
   EXPECT_EQ(0, test_server_->gauge("cluster.cluster_0.membership_healthy")->value());[m
 [m
[36m@@ -234,7 +289,8 @@[m [mTEST_P(EdsIntegrationTest, RemoveAfterHcFail) {[m
   EXPECT_EQ(1, test_server_->gauge("cluster.cluster_0.membership_total")->value());[m
 [m
   // Clear out the host and verify the host is still healthy.[m
[31m-  setEndpoints(0, 0, 0);[m
[32m+[m[32m  options.total_endpoints = 0;[m
[32m+[m[32m  setEndpoints(options);[m
   EXPECT_EQ(1, test_server_->gauge("cluster.cluster_0.membership_total")->value());[m
   EXPECT_EQ(1, test_server_->gauge("cluster.cluster_0.membership_healthy")->value());[m
 [m
[36m@@ -253,7 +309,8 @@[m [mTEST_P(EdsIntegrationTest, FinishWarmingIgnoreHealthCheck) {[m
   initializeTest(true, [](envoy::config::cluster::v3::Cluster& cluster) {[m
     cluster.set_ignore_health_on_host_removal(true);[m
   });[m
[31m-  setEndpoints(1, 0, 0, false);[m
[32m+[m[32m  EndpointSettingOptions options;[m
[32m+[m[32m  setEndpoints(options, false);[m
   EXPECT_EQ(1, test_server_->gauge("cluster.cluster_0.membership_total")->value());[m
   EXPECT_EQ(0, test_server_->gauge("cluster.cluster_0.membership_healthy")->value());[m
   EXPECT_EQ(0, test_server_->gauge("cluster_manager.warming_clusters")->value());[m
[36m@@ -266,7 +323,8 @@[m [mTEST_P(EdsIntegrationTest, FinishWarmingIgnoreHealthCheck) {[m
 [m
   // Clear out the host before the health check finishes (regardless of success/error/timeout) and[m
   // ensure that warming_clusters goes to 0 to avoid a permanent warming state.[m
[31m-  setEndpoints(0, 0, 0, true, absl::nullopt, false);[m
[32m+[m[32m  options.total_endpoints = 0;[m
[32m+[m[32m  setEndpoints(options, true, false);[m
   test_server_->waitForGaugeEq("cluster_manager.warming_clusters", 0);[m
 }[m
 [m
[36m@@ -276,7 +334,7 @@[m [mTEST_P(EdsIntegrationTest, EndpointWarmingSuccessfulHc) {[m
 [m
   // Endpoints are initially excluded.[m
   initializeTest(true);[m
[31m-  setEndpoints(1, 0, 0, false);[m
[32m+[m[32m  setEndpoints(EndpointSettingOptions(), false);[m
 [m
   EXPECT_EQ(1, test_server_->gauge("cluster.cluster_0.membership_total")->value());[m
   EXPECT_EQ(1, test_server_->gauge("cluster.cluster_0.membership_excluded")->value());[m
[36m@@ -298,7 +356,7 @@[m [mTEST_P(EdsIntegrationTest, EndpointWarmingFailedHc) {[m
 [m
   // Endpoints are initially excluded.[m
   initializeTest(true);[m
[31m-  setEndpoints(1, 0, 0, false);[m
[32m+[m[32m  setEndpoints(EndpointSettingOptions(), false);[m
 [m
   EXPECT_EQ(1, test_server_->gauge("cluster.cluster_0.membership_total")->value());[m
   EXPECT_EQ(1, test_server_->gauge("cluster.cluster_0.membership_excluded")->value());[m
[36m@@ -316,32 +374,40 @@[m [mTEST_P(EdsIntegrationTest, EndpointWarmingFailedHc) {[m
 // Validate that health status updates are consumed from EDS.[m
 TEST_P(EdsIntegrationTest, HealthUpdate) {[m
   initializeTest(false);[m
[32m+[m[32m  EndpointSettingOptions options;[m
   // Initial state, no cluster members.[m
   EXPECT_EQ(0, test_server_->counter("cluster.cluster_0.membership_change")->value());[m
   EXPECT_EQ(0, test_server_->gauge("cluster.cluster_0.membership_total")->value());[m
   EXPECT_EQ(0, test_server_->gauge("cluster.cluster_0.membership_healthy")->value());[m
   // 2/2 healthy endpoints.[m
[31m-  setEndpoints(2, 2, 0);[m
[32m+[m[32m  options.total_endpoints = 2;[m
[32m+[m[32m  options.healthy_endpoints = 2;[m
[32m+[m[32m  setEndpoints(options);[m
   EXPECT_EQ(1, test_server_->counter("cluster.cluster_0.membership_change")->value());[m
   EXPECT_EQ(2, test_server_->gauge("cluster.cluster_0.membership_total")->value());[m
   EXPECT_EQ(2, test_server_->gauge("cluster.cluster_0.membership_healthy")->value());[m
   // Drop to 0/2 healthy endpoints.[m
[31m-  setEndpoints(2, 0, 0);[m
[32m+[m[32m  options.healthy_endpoints = 0;[m
[32m+[m[32m  setEndpoints(options);[m
   EXPECT_EQ(1, test_server_->counter("cluster.cluster_0.membership_change")->value());[m
   EXPECT_EQ(2, test_server_->gauge("cluster.cluster_0.membership_total")->value());[m
   EXPECT_EQ(0, test_server_->gauge("cluster.cluster_0.membership_healthy")->value());[m
   // Increase to 1/2 healthy endpoints.[m
[31m-  setEndpoints(2, 1, 0);[m
[32m+[m[32m  options.healthy_endpoints = 1;[m
[32m+[m[32m  setEndpoints(options);[m
   EXPECT_EQ(1, test_server_->counter("cluster.cluster_0.membership_change")->value());[m
   EXPECT_EQ(2, test_server_->gauge("cluster.cluster_0.membership_total")->value());[m
   EXPECT_EQ(1, test_server_->gauge("cluster.cluster_0.membership_healthy")->value());[m
   // Add host and modify health to 2/3 healthy endpoints.[m
[31m-  setEndpoints(3, 2, 0);[m
[32m+[m[32m  options.total_endpoints = 3;[m
[32m+[m[32m  options.healthy_endpoints = 2;[m
[32m+[m[32m  setEndpoints(options);[m
   EXPECT_EQ(2, test_server_->counter("cluster.cluster_0.membership_change")->value());[m
   EXPECT_EQ(3, test_server_->gauge("cluster.cluster_0.membership_total")->value());[m
   EXPECT_EQ(2, test_server_->gauge("cluster.cluster_0.membership_healthy")->value());[m
   // Modify health to 2/3 healthy and 1/3 degraded.[m
[31m-  setEndpoints(3, 2, 1);[m
[32m+[m[32m  options.degraded_endpoints = 1;[m
[32m+[m[32m  setEndpoints(options);[m
   EXPECT_EQ(2, test_server_->counter("cluster.cluster_0.membership_change")->value());[m
   EXPECT_EQ(3, test_server_->gauge("cluster.cluster_0.membership_total")->value());[m
   EXPECT_EQ(2, test_server_->gauge("cluster.cluster_0.membership_healthy")->value());[m
[36m@@ -352,7 +418,10 @@[m [mTEST_P(EdsIntegrationTest, HealthUpdate) {[m
 TEST_P(EdsIntegrationTest, OverprovisioningFactorUpdate) {[m
   initializeTest(false);[m
   // Default overprovisioning factor.[m
[31m-  setEndpoints(4, 4, 0);[m
[32m+[m[32m  EndpointSettingOptions options;[m
[32m+[m[32m  options.total_endpoints = 4;[m
[32m+[m[32m  options.healthy_endpoints = 4;[m
[32m+[m[32m  setEndpoints(options);[m
   auto get_and_compare = [this](const uint32_t expected_factor) {[m
     const auto& cluster_map = test_server_->server().clusterManager().clusters();[m
     EXPECT_EQ(1, cluster_map.active_clusters_.size());[m
[36m@@ -366,7 +435,8 @@[m [mTEST_P(EdsIntegrationTest, OverprovisioningFactorUpdate) {[m
   get_and_compare(Envoy::Upstream::kDefaultOverProvisioningFactor);[m
 [m
   // Use new overprovisioning factor 200.[m
[31m-  setEndpoints(4, 4, 0, true, 200);[m
[32m+[m[32m  options.overprovisioning_factor = 200;[m
[32m+[m[32m  setEndpoints(options);[m
   get_and_compare(200);[m
 }[m
 [m
[36m@@ -426,7 +496,7 @@[m [mTEST_P(EdsIntegrationTest, StatsReadyFilter) {[m
   cleanupUpstreamAndDownstream();[m
 [m
   // 2/2 healthy endpoints.[m
[31m-  setEndpoints(2, 2, 0);[m
[32m+[m[32m  setEndpoints({/*total_endpoint*/ 2, /*healthy_endpoints*/ 2});[m
   EXPECT_EQ(2, test_server_->gauge("cluster.cluster_0.membership_healthy")->value());[m
   response = IntegrationUtil::makeSingleRequest(lookupPort("http"), "GET", "/cluster1", "",[m
                                                 downstream_protocol_, version_, "foo.com");[m
[1mdiff --git a/test/integration/protocol_integration_test.cc b/test/integration/protocol_integration_test.cc[m
[1mindex 7201244e34..066820e6e5 100644[m
[1m--- a/test/integration/protocol_integration_test.cc[m
[1m+++ b/test/integration/protocol_integration_test.cc[m
[36m@@ -3915,4 +3915,44 @@[m [mTEST_P(DownstreamProtocolIntegrationTest, InvalidResponseHeaderName) {[m
 }[m
 #endif[m
 [m
[32m+[m[32mTEST_P(DownstreamProtocolIntegrationTest, InvalidSchemeHeaderWithWhitespace) {[m
[32m+[m[32m  useAccessLog("%RESPONSE_CODE_DETAILS%");[m
[32m+[m[32m  initialize();[m
[32m+[m[32m  codec_client_ = makeHttpConnection(lookupPort("http"));[m
[32m+[m
[32m+[m[32m  // Start the request.[m
[32m+[m[32m  auto response = codec_client_->makeHeaderOnlyRequest([m
[32m+[m[32m      Http::TestRequestHeaderMapImpl{{":method", "GET"},[m
[32m+[m[32m                                     {":path", "/test/long/url"},[m
[32m+[m[32m                                     {":scheme", "/admin http"},[m
[32m+[m[32m                                     {":authority", "sni.lyft.com"}});[m
[32m+[m
[32m+[m[32m#ifdef ENVOY_ENABLE_UHV[m
[32m+[m[32m  if (downstreamProtocol() != Http::CodecType::HTTP1) {[m
[32m+[m[32m    ASSERT_TRUE(response->waitForReset());[m
[32m+[m[32m    EXPECT_THAT(waitForAccessLog(access_log_name_), HasSubstr("invalid"));[m
[32m+[m[32m    return;[m
[32m+[m[32m  }[m
[32m+[m[32m#else[m
[32m+[m[32m  if (downstreamProtocol() == Http::CodecType::HTTP2 &&[m
[32m+[m[32m      GetParam().http2_implementation == Http2Impl::Nghttp2) {[m
[32m+[m[32m    ASSERT_TRUE(response->waitForReset());[m
[32m+[m[32m    EXPECT_THAT(waitForAccessLog(access_log_name_), HasSubstr("invalid"));[m
[32m+[m[32m    return;[m
[32m+[m[32m  }[m
[32m+[m[32m#endif[m
[32m+[m[32m  // Other HTTP codecs accept the bad scheme but the Envoy should replace it with a valid one.[m
[32m+[m[32m  waitForNextUpstreamRequest();[m
[32m+[m[32m  if (upstreamProtocol() == Http::CodecType::HTTP1) {[m
[32m+[m[32m    // The scheme header is not conveyed in HTTP/1.[m
[32m+[m[32m    EXPECT_EQ(nullptr, upstream_request_->headers().Scheme());[m
[32m+[m[32m  } else {[m
[32m+[m[32m    EXPECT_THAT(upstream_request_->headers(), HeaderValueOf(Http::Headers::get().Scheme, "http"));[m
[32m+[m[32m  }[m
[32m+[m[32m  upstream_request_->encodeHeaders(default_response_headers_, true);[m
[32m+[m[32m  ASSERT_TRUE(response->waitForEndStream());[m
[32m+[m[32m  ASSERT_TRUE(response->complete());[m
[32m+[m[32m  EXPECT_EQ("200", response->headers().getStatusValue());[m
[32m+[m[32m}[m
[32m+[m
 } // namespace Envoy[m
[1mdiff --git a/test/mocks/upstream/cluster_info.cc b/test/mocks/upstream/cluster_info.cc[m
[1mindex ba3a767c93..a210516d5e 100644[m
[1m--- a/test/mocks/upstream/cluster_info.cc[m
[1m+++ b/test/mocks/upstream/cluster_info.cc[m
[36m@@ -54,11 +54,15 @@[m [mMockClusterInfo::MockClusterInfo()[m
     : http2_options_(::Envoy::Http2::Utility::initializeAndValidateOptions([m
           envoy::config::core::v3::Http2ProtocolOptions())),[m
       stat_names_(stats_store_.symbolTable()),[m
[32m+[m[32m      config_update_stats_names_(stats_store_.symbolTable()),[m
[32m+[m[32m      lb_stat_names_(stats_store_.symbolTable()), endpoint_stat_names_(stats_store_.symbolTable()),[m
       cluster_load_report_stat_names_(stats_store_.symbolTable()),[m
       cluster_circuit_breakers_stat_names_(stats_store_.symbolTable()),[m
       cluster_request_response_size_stat_names_(stats_store_.symbolTable()),[m
       cluster_timeout_budget_stat_names_(stats_store_.symbolTable()),[m
       stats_(ClusterInfoImpl::generateStats(stats_store_, stat_names_)),[m
[32m+[m[32m      config_update_stats_(config_update_stats_names_, stats_store_),[m
[32m+[m[32m      lb_stats_(lb_stat_names_, stats_store_), endpoint_stats_(endpoint_stat_names_, stats_store_),[m
       transport_socket_matcher_(new NiceMock<Upstream::MockTransportSocketMatcher>()),[m
       load_report_stats_(ClusterInfoImpl::generateLoadReportStats(load_report_stats_store_,[m
                                                                   cluster_load_report_stat_names_)),[m
[36m@@ -94,7 +98,10 @@[m [mMockClusterInfo::MockClusterInfo()[m
       .WillByDefault(ReturnPointee(&max_response_headers_count_));[m
   ON_CALL(*this, maxRequestsPerConnection())[m
       .WillByDefault(ReturnPointee(&max_requests_per_connection_));[m
[31m-  ON_CALL(*this, stats()).WillByDefault(ReturnRef(stats_));[m
[32m+[m[32m  ON_CALL(*this, trafficStats()).WillByDefault(ReturnRef(stats_));[m
[32m+[m[32m  ON_CALL(*this, lbStats()).WillByDefault(ReturnRef(lb_stats_));[m
[32m+[m[32m  ON_CALL(*this, configUpdateStats()).WillByDefault(ReturnRef(config_update_stats_));[m
[32m+[m[32m  ON_CALL(*this, endpointStats()).WillByDefault(ReturnRef(endpoint_stats_));[m
   ON_CALL(*this, statsScope()).WillByDefault(ReturnRef(stats_store_));[m
   // TODO(incfly): The following is a hack because it's not possible to directly embed[m
   // a mock transport socket factory matcher due to circular dependencies. Fix this up in a follow[m
[1mdiff --git a/test/mocks/upstream/cluster_info.h b/test/mocks/upstream/cluster_info.h[m
[1mindex bb6fded4f2..a0a31e0d6c 100644[m
[1m--- a/test/mocks/upstream/cluster_info.h[m
[1m+++ b/test/mocks/upstream/cluster_info.h[m
[36m@@ -151,7 +151,10 @@[m [mpublic:[m
   MOCK_METHOD(const std::string&, observabilityName, (), (const));[m
   MOCK_METHOD(ResourceManager&, resourceManager, (ResourcePriority priority), (const));[m
   MOCK_METHOD(TransportSocketMatcher&, transportSocketMatcher, (), (const));[m
[31m-  MOCK_METHOD(ClusterStats&, stats, (), (const));[m
[32m+[m[32m  MOCK_METHOD(ClusterTrafficStats&, trafficStats, (), (const));[m
[32m+[m[32m  MOCK_METHOD(ClusterLbStats&, lbStats, (), (const));[m
[32m+[m[32m  MOCK_METHOD(ClusterEndpointStats&, endpointStats, (), (const));[m
[32m+[m[32m  MOCK_METHOD(ClusterConfigUpdateStats&, configUpdateStats, (), (const));[m
   MOCK_METHOD(Stats::Scope&, statsScope, (), (const));[m
   MOCK_METHOD(ClusterLoadReportStats&, loadReportStats, (), (const));[m
   MOCK_METHOD(ClusterRequestResponseSizeStatsOptRef, requestResponseSizeStats, (), (const));[m
[36m@@ -196,12 +199,18 @@[m [mpublic:[m
   uint64_t max_requests_per_connection_{};[m
   uint32_t max_response_headers_count_{Http::DEFAULT_MAX_HEADERS_COUNT};[m
   NiceMock<Stats::MockIsolatedStatsStore> stats_store_;[m
[31m-  ClusterStatNames stat_names_;[m
[32m+[m[32m  ClusterTrafficStatNames stat_names_;[m
[32m+[m[32m  ClusterConfigUpdateStatNames config_update_stats_names_;[m
[32m+[m[32m  ClusterLbStatNames lb_stat_names_;[m
[32m+[m[32m  ClusterEndpointStatNames endpoint_stat_names_;[m
   ClusterLoadReportStatNames cluster_load_report_stat_names_;[m
   ClusterCircuitBreakersStatNames cluster_circuit_breakers_stat_names_;[m
   ClusterRequestResponseSizeStatNames cluster_request_response_size_stat_names_;[m
   ClusterTimeoutBudgetStatNames cluster_timeout_budget_stat_names_;[m
[31m-  ClusterStats stats_;[m
[32m+[m[32m  ClusterTrafficStats stats_;[m
[32m+[m[32m  ClusterConfigUpdateStats config_update_stats_;[m
[32m+[m[32m  ClusterLbStats lb_stats_;[m
[32m+[m[32m  ClusterEndpointStats endpoint_stats_;[m
   Upstream::TransportSocketMatcherPtr transport_socket_matcher_;[m
   NiceMock<Stats::MockIsolatedStatsStore> load_report_stats_store_;[m
   ClusterLoadReportStats load_report_stats_;[m
[1mdiff --git a/test/mocks/upstream/cluster_manager.cc b/test/mocks/upstream/cluster_manager.cc[m
[1mindex 05c5afcd8a..3b588bf67d 100644[m
[1m--- a/test/mocks/upstream/cluster_manager.cc[m
[1m+++ b/test/mocks/upstream/cluster_manager.cc[m
[36m@@ -15,7 +15,9 @@[m [musing ::testing::ReturnRef;[m
 MockClusterManager::MockClusterManager(TimeSource&) : MockClusterManager() {}[m
 [m
 MockClusterManager::MockClusterManager()[m
[31m-    : cluster_stat_names_(*symbol_table_), cluster_load_report_stat_names_(*symbol_table_),[m
[32m+[m[32m    : cluster_stat_names_(*symbol_table_), cluster_config_update_stat_names_(*symbol_table_),[m
[32m+[m[32m      cluster_lb_stat_names_(*symbol_table_), cluster_endpoint_stat_names_(*symbol_table_),[m
[32m+[m[32m      cluster_load_report_stat_names_(*symbol_table_),[m
       cluster_circuit_breakers_stat_names_(*symbol_table_),[m
       cluster_request_response_size_stat_names_(*symbol_table_),[m
       cluster_timeout_budget_stat_names_(*symbol_table_) {[m
[1mdiff --git a/test/mocks/upstream/cluster_manager.h b/test/mocks/upstream/cluster_manager.h[m
[1mindex 59dfce2e01..6a068671ae 100644[m
[1m--- a/test/mocks/upstream/cluster_manager.h[m
[1m+++ b/test/mocks/upstream/cluster_manager.h[m
[36m@@ -56,7 +56,14 @@[m [mpublic:[m
   MOCK_METHOD(ClusterUpdateCallbacksHandle*, addThreadLocalClusterUpdateCallbacks_,[m
               (ClusterUpdateCallbacks & callbacks));[m
   MOCK_METHOD(Config::SubscriptionFactory&, subscriptionFactory, ());[m
[31m-  const ClusterStatNames& clusterStatNames() const override { return cluster_stat_names_; }[m
[32m+[m[32m  const ClusterTrafficStatNames& clusterStatNames() const override { return cluster_stat_names_; }[m
[32m+[m[32m  const ClusterConfigUpdateStatNames& clusterConfigUpdateStatNames() const override {[m
[32m+[m[32m    return cluster_config_update_stat_names_;[m
[32m+[m[32m  }[m
[32m+[m[32m  const ClusterEndpointStatNames& clusterEndpointStatNames() const override {[m
[32m+[m[32m    return cluster_endpoint_stat_names_;[m
[32m+[m[32m  }[m
[32m+[m[32m  const ClusterLbStatNames& clusterLbStatNames() const override { return cluster_lb_stat_names_; }[m
   const ClusterLoadReportStatNames& clusterLoadReportStatNames() const override {[m
     return cluster_load_report_stat_names_;[m
   }[m
[36m@@ -88,7 +95,10 @@[m [mpublic:[m
   absl::flat_hash_map<std::string, std::unique_ptr<MockCluster>> active_clusters_;[m
   absl::flat_hash_map<std::string, std::unique_ptr<MockCluster>> warming_clusters_;[m
   Stats::TestUtil::TestSymbolTable symbol_table_;[m
[31m-  ClusterStatNames cluster_stat_names_;[m
[32m+[m[32m  ClusterTrafficStatNames cluster_stat_names_;[m
[32m+[m[32m  ClusterConfigUpdateStatNames cluster_config_update_stat_names_;[m
[32m+[m[32m  ClusterLbStatNames cluster_lb_stat_names_;[m
[32m+[m[32m  ClusterEndpointStatNames cluster_endpoint_stat_names_;[m
   ClusterLoadReportStatNames cluster_load_report_stat_names_;[m
   ClusterCircuitBreakersStatNames cluster_circuit_breakers_stat_names_;[m
   ClusterRequestResponseSizeStatNames cluster_request_response_size_stat_names_;[m
[1mdiff --git a/test/mocks/upstream/host.h b/test/mocks/upstream/host.h[m
[1mindex 6e8e9e4168..eebd9099de 100644[m
[1m--- a/test/mocks/upstream/host.h[m
[1m+++ b/test/mocks/upstream/host.h[m
[36m@@ -163,6 +163,11 @@[m [mpublic:[m
     return locality_zone_stat_name_->statName();[m
   }[m
 [m
[32m+[m[32m  bool disableActiveHealthCheck() const override { return disable_active_health_check_; }[m
[32m+[m[32m  void setDisableActiveHealthCheck(bool disable_active_health_check) override {[m
[32m+[m[32m    disable_active_health_check_ = disable_active_health_check;[m
[32m+[m[32m  }[m
[32m+[m
   MOCK_METHOD(Network::Address::InstanceConstSharedPtr, address, (), (const));[m
   MOCK_METHOD(const std::vector<Network::Address::InstanceConstSharedPtr>&, addressList, (),[m
               (const));[m
[36m@@ -213,6 +218,7 @@[m [mpublic:[m
   LoadMetricStatsImpl load_metric_stats_;[m
   mutable Stats::TestUtil::TestSymbolTable symbol_table_;[m
   mutable std::unique_ptr<Stats::StatNameManagedStorage> locality_zone_stat_name_;[m
[32m+[m[32m  bool disable_active_health_check_ = false;[m
 };[m
 [m
 } // namespace Upstream[m
[1mdiff --git a/tools/extensions/extensions_schema.yaml b/tools/extensions/extensions_schema.yaml[m
[1mindex d85c9ac503..3d787139db 100644[m
[1m--- a/tools/extensions/extensions_schema.yaml[m
[1m+++ b/tools/extensions/extensions_schema.yaml[m
[36m@@ -106,6 +106,7 @@[m [mcategories:[m
 - envoy.rbac.matchers[m
 - envoy.access_loggers.extension_filters[m
 - envoy.http.stateful_session[m
[32m+[m[32m- envoy.matching.action[m
 - envoy.matching.http.input[m
 - envoy.matching.http.custom_matchers[m
 - envoy.matching.network.input[m
